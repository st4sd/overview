{"componentChunkName":"component---src-pages-running-workflows-on-openshift-mdx","path":"/running-workflows-on-openshift/","result":{"pageContext":{"frontmatter":{"title":"The ST4SD Python Client API"},"relativePagePath":"/running-workflows-on-openshift.mdx","titleType":"page","MdxNode":{"id":"d26ba59d-9f4b-560e-9b2d-f6f94e4c1718","children":[],"parent":"97279787-0e31-53a5-a2ee-d54e1506950d","internal":{"content":"---\ntitle: The ST4SD Python Client API\n---\n\nimport { Tools } from \"@carbon/pictograms-react\";\nimport { ArtTools_01 } from \"@carbon/pictograms-react\";\n\n<!--\n\n  Copyright IBM Inc. All Rights Reserved.\n  SPDX-License-Identifier: Apache-2.0\n\n-->\n\n<PageDescription>\n\nLearn how to use ST4SD python client API to run, query and interact with virtual\nexperiments\n\n</PageDescription>\n\n<AnchorLinks>\n\n<AnchorLink>Connecting to ST4SD</AnchorLink>\n <AnchorLink>Adding a virtual experiment package</AnchorLink>\n  <AnchorLink>Running a virtual experiment</AnchorLink>\n  <AnchorLink>Getting the status of a virtual experiment instance</AnchorLink>\n  <AnchorLink>Get the details of a virtual experiment instance</AnchorLink>\n  <AnchorLink>Retrieving the outputs of a virtual experiment instance</AnchorLink>\n  <AnchorLink>Stopping a virtual experiment instance</AnchorLink>\n</AnchorLinks>\n\n<InlineNotification>\n\n- **For**: Researchers\n- **Use When:**\n  - You want to run and get output of virtual experiments\n- **Skill Requirements:**\n  - Some knowledge of python\n\n</InlineNotification>\n\n<InlineNotification>\n\nThe code here can be tested using the\n[ST4SD Runtime API Example notebook](https://github.com/st4sd/st4sd-examples/blob/main/notebooks/ST4SD%20Runtime%20API%20Example.ipynb)\n\n</InlineNotification>\n\n## Overview\n\nWe provide a python API for interacting with the ST4SD Services. The API enables\nyou to:\n\n1. Launch virtual experiment instances\n2. Monitor virtual experiment instances\n3. Download outputs and measured properties of virtual experiment instances\n\n### Requirements\n\nThe basic requirements are access to an OpenShift instance with ST4SD installed\n(see the [ST4SD Cloud Installation instructions](/installation) for more\ninformation).\n\n#### Getting data into and out-of virtual experiments: Cloud Object Store\n\nAdditionally it can be useful to set up a Cloud Object Store bucket so you can\neasily get data into and out-of a virtual experiment.\n\n[See here](/UsingCloudObjectStore) for detailed instructions on how to do this\nwith IBM Cloud.\n\n### Examples\n\nWe maintain a [repository](https://github.com/st4sd/st4sd-examples/) containing\na set of IPython notebooks that illustrate interacting with virtual experiments\nvia these two methods.\n\nIf you are using a\n[local JupyterServer environment](/user-environments#local-jupyterserver-with-openshift-st4sd-instance)\nexecute this snippet to start-up the first notebook illustrating the RESTApi.\nMost of the examples discussed here are in this notebook\n\n```\ngit clone https://github.com/st4sd/st4sd-examples.git\ncd st4sd-examples\njupyter-notebook notebooks/ST4SD\\ Runtime\\ API\\ Example.ipynb\n```\n\n**_Note_: If you've installed `st4sd-runtime-core` into a virtualenv you will\nneed to activate it before executing above snippet**\n\nAfter reading this page have a browse around the `st4sd-examples` repository to\nsee what topics are covered in the other notebooks.\n\n## Connecting to ST4SD\n\nTo connect to an ST4SD instance you need to obtain an `auth-token` or use an\n`api-key`.\n\n<InlineNotification>\n\nTo learn what `auth-token`s and `api-key`s are in ST4SD and how to retrieve\nthem, read the\n[How do I connect to the ST4SD runtime service?](/runtime-service#how-do-i-connect-to-the-st4sd-runtime-service)\nsection.\n\n</InlineNotification>\n\nThe following code blocks show how to connect to an ST4SD instance.\n\n- Connect using an `auth-token`:\n\n    ```python\n      import experiment.service.db\n\n      # enter the https:// ST4SD url below\n      url=\"https://${your ST4SD url}\"\n\n      # enter your auth-token below\n      auth_token=\"put your authentication/token - do not share it with anyone\"\n\n      api=experiment.service.db.ExperimentRestAPI(url, cc_auth_token=auth_token)\n    ```\n\n    You can get an auth token by visiting `https://${your ST4SD url}/authorisation/token`.\n\n- Connect using an `api-key`:\n\n    ```python\n      import experiment.service.db\n\n      # enter the https:// ST4SD url below\n      url=\"https://${your ST4SD url}\"\n\n      # enter your auth-token below\n      bearer_key=\"put your api-key - do not share it with anyone\"\n\n      api=experiment.service.db.ExperimentRestAPI(url, cc_bearer_key=bearer_key)\n    ```\n\n    You can use the openshift token of service accounts for which there exists a Kubernetes RoleBinding to the `st4sd-authenticate-microservices` Role.\n    See our documentation on managing users with [ST4SD Cloud](/cloud-manage-users#give-a-group-of-users-access-to-just-the-st4sd-microservices) for more details.\n\n<InlineNotification>\n\nST4SD may print warnings to your python terminal e.g.\n`Unable to import tinydb module - tinydb interface not available`. These are not\nproblems. They are just informing you that select features are not supported on\nthe machine you are using. You can ignore them.\n\n</InlineNotification>\n\nThe `ExperimentRestAPI` initializer validates the authentication token you\nprovided and will raise an exception if it is invalid. If you do not get an\nException that means you can use `api` to interact with the\n`st4sd-runtime-service` and `st4sd-datastore` REST-APIs.\n\nThe above code needs to be executed once in a notebook session to get an `api`\ninstance to interact with. All the following examples assume this step has been\ndone.\n\n<InlineNotification>\n\nST4SD may print warnings to your python terminal e.g.\n`Unable to import pythonlsf - limited LSF functionality will be available`.\nThese are not problems. They are just informing you that select features are not\nsupported on the machine you are using. You can ignore them.\n\n</InlineNotification>\n\n## Adding a virtual experiment package\n\nBefore you can run a virtual experiment you need to add it to your ST4SD\nregistry. You do this using the `api_experiment_push()` method.\n\nTechnically you add a _parameterised_ virtual experiment package. A\nparameterised package can define particular, potentially complex values, for\nvariables in the virtual experiment to enable specific behaviour and make them\neasier to consume.\n\nYou can either type the definition of the\n[parameterised package](/creating-a-parameterised-package) manually or import it\nfrom another ST4SD registry\n[(e.g. the global ST4SD registry)](/using-the-virtual-experiments-registry-ui).\nIn the second-case the registry UI provides with the exact `api_experiment_push`\ncall you need to execute.\n\n## Running a virtual experiment\n\nThe API call `api_experiment_start` will start the virtual experiment that a\nparameterised virtual experiment package points to e.g.,\n\n```python\nrest_uid = api.api_experiment_start(experimentIdentifier, payload=...)\n```\n\n<InlineNotification kind=\"warning\">\n\nThe above example assumes `experimentIdentifier` is a valid [identifier](/creating-a-parameterised-package#the-parameterised-virtual-experiment-package-identifier-(pvep-identifier)) of a\nparameterised package (meaning the name of a Parameterised Virtual Experiment Package, with an optional\n[tag or SHA identifier](/creating-a-parameterised-package#package-tags)). The\nvalue of `payload` is a python dictionary that contains the run options to the\nexperiment.\n\n</InlineNotification>\n\nWhen you run a given virtual experiment you create a _virtual experiment\ninstance_. Each instance is assigned a unique identifier which is returned when\nyou start the virtual experiment. We use the term `ExperimentRunID` and\n`rest_uid` to refer to such identifiers.\n\nStarting an experiment involves:\n\n1. specifying experiment inputs\n2. (optional) specifying experiment variables\n3. (optional) selecting an experiment platform\n4. (optional) specifying experiment data\n\nThe following sections explain how to fill in the `payload`.\n\n### Specifying experiment inputs\n\n_inputs_ are files the experiment requires to run - they must be provided. Each\nexperiments documentation should explain what these files are.\n\nThe inputs are specified via the key `inputs` in the payload. The value of this\nkey is a list that has one item, a dictionary, for each required input file\ne.g.,\n\n```python\npayload = {\n    \"inputs\": [\n      {...},\n      ...\n    ]\n}\n```\n\nYou can provide the content of input files directly in the payload using the\n`content` key\n\n```python\ndata = pd.read_csv('mydata.csv')\npayload = {\n    \"inputs\": [{\n       \"content\": data.to_csv(index=False),\n       \"filename\": \"input_filename.csv\"\n   }]\n\n}\n```\n\nThis specifies that the content of the input file `input_filename.csv` comes\nfrom the pandas DataFrame `data`.\n\n### Specifying experiment variables\n\n_variables_ are optional parameters controlling the behaviour of the experiment,\ne.g. number of cpus. They are experiment specific i.e. the same variables don't\nexist in all experiment and variables controlling similar behaviour in two\nexperiments may not have the same name.\n\n<InlineNotification>\n\n_variables_ can only be overridden if the parameterised package allows it. If\nthis is the case the variables that can be set will be listed in the\n`executionOptions` section in the package description\n\n</InlineNotification>\n\n_variables_ are set using the `variables` key in the payload. The value of this\nkey is a dictionary of variable-name, variable-value pairs.\n\n```python\npayload = {\n    ... #Input/data options elided\n   \"variables\": {\n        \"startIndex\": 0,\n        \"numberMolecules\": 1,\n    }\n}\n```\n\n### Selecting an experiment platform\n\nThere are cases where variables are dependent on the values of other variables. ST4SD implements a feature called\nexperiment `platform` to help with these scenarios.\n\nYou can think of an experiment platform as a specialization of the experiment for a specific use case. Here are some\nof the ways developers use experiment platforms:\n\n1. maintain a single experiment definition which can execute on multiple different compute environments (OpenShift, HPC, ocal mode, local mode with containers, etc )\n2. support mass-configuration of steps to account for different scenarios (e.g. using less accurate methods which are faster to compute, etc)\n3. enable debug mode to assist debugging\n\nand many more!\n\nThe workflow developer can then use the [parameterised package section](/creating-a-parameterised-package#the-parameterisation-section) of experiments\nto encode the available options for running the experiment.\n\nIf the experiment has a `preset` platform (i.e. the field `parameterisation.presets.platform` of the Parameterised Virtual Experiment Package definition is set) then the user does not get the option to select which platform to use for the execution of the experiment. This option is hard-coded.\n\nIf the experiment has a configurable platform (i.e. the field `parameterisation.executionOptions.platforms` is set) then the user can select any of the options under `parameterisation.executionOptions.platforms`. If the user does not explicitly set a platform then the experiment instance will use the 1st platform in the `parameterisation.executionOptions.platforms` array.\n\n\n### Specifying experiment data\n\n_data_ refers to experiment configuration files that **may** be overridden by\nthe user.\n\n<InlineNotification>\n\n_data_ files can only be overridden if the parameterised package allows it. If\nthis is the case the files will be listed in the `executionOptions` section in\nthe package description\n\n</InlineNotification>\n\nThe data files are specified via the key `data` in the payload. The value of\nthis key is a list that has one item, a dictionary, for each `data` file you\nwant to override e.g.,\n\n```python\npayload = {\n    \"data\": [\n      {...},\n      ...\n    ]\n}\n```\n\nThe format of the `data` dictionary is identical to the `input` dictionary.\n\nProviding data files via S3/Datashim or providing their content directly follows\nsame process as described for inputs. See those sections for details.\n\n### Payload Details\n\nThe following is the complete structure of the `api_experiment_start()` payload\n(in `YAML`). This includes some advanced options not discussed here.\n\n```yaml\nplatform: name of platform (optional - see parameterisation notes)\ninputs: # optional (parameterised packages may have no inputs)\n  # see notes for interaction with Dataset/S3\n  - filename: str # required\n    content: str # optional - see S3 notes\ndata: # optional (parameterised packages may have no overiddable data files)\n  # see notes for interaction with Dataset/S3\n  - filename: str # required\n    content: str # optional - see S3 notes\nvolumes: # optional\n  - type:\n      # children fields are mutually exclusive - there must be exactly 1\n      persistentVolumeClaim: name of PVC object\n      dataset: name of Datashim dataset (see notes)\n      configMap: name of ConfigMap object\n      secret: name of Secret object\n    applicationDependency: name of appDep to bind to volume (optional)\n    # subPath applies only to pvc and dataset types\n    subPath: path inside volume to mount (optional, defaults to root of object)\nvariables: # optional - must adhere to parameterisation rules (see notes)\n  $name: $value\nadditionalOptions: # optional runtime arguments to elaunch.py\n  # they cannot override runtime arguments in the package parameterisation\n  - \"--someArgument=value\"\nenvironmentVariables: # optional\n  # creates environment variable in the pod that hosts elaunch.py\n  $name: $value\nmetadata: # optional\n  # custom metadata to insert into the \"user-metadata\" MongoDB document of run\n  $name: $value\ns3: # optional - used to read inputs/data with no \"content\" field\n  # dataset is mutually exclusive with any other sibling fields\n  dataset: name of Datashim dataset # optional\n  # can only provide fields below if dataset is unset\n  accessKeyID: S3 accessKeyID # optional\n  secretAccessKey: S3 secretAccessKey # optional\n  bucket: name of S3 bucket # optional\n  endpoint: the s3 endpoint # optional\n  region: the s3 region # optional\n# s3Store and datasetStoreURI are mutually exclusive and both optional\n# when one is provided, orchestrator uploads key-outputs to S3 after Run terminates\ns3Store: # optional\n  bucketPath:\n    path under bucket to store key-outputs files (default is\n    \"workflow_instances/\")\n  credentials: (same as s3 field)\ndatasetStoreURI: a \"dataset/$bucketPath\" URI (optional)\nruntimePolicy: # optional\n  name: the name of the runtime policy (required)\n  config: # optional\n    $key: Any # each runtime-policy has its own config-schema\n```\n\n<InlineNotification kind=\"info\">\n\nThe schema above is in YAML format. If you are using python (i.e. an instance of\n`experiment.service.db.ExperimentRestAPI`) you should either create an\nequivalent python dictionary OR a python string that contains the yaml\ndefinition and then convert the string into a dictionary using\n`yaml.loads(the_string)`.\n\n</InlineNotification>\n\n**Notes**\n\n1. `inputs` and `data` file-specifications have an optional `content` field. If\n   this field is missing then the contents of the files are expected to exist on\n   S3 or in a Dataset. When `s3` exists then the `filename` field acts as the\n   path inside the S3 bucket (or dataset) to use for reading the content of the\n   input/data file.\n1. Dataset objects are only available if a cluster-admin has installed\n   [Datashim](https://github.com/datashim-io/datashim) on the cluster.\n1. The fields `additionalOptions`, `data`, `inputs`, `platform`, and `variables`\n   must adhere to parameterisation rules. See the\n   [parameterised package documentation](/creating-a-parameterised-package#the-parameterisation-section)\n   for more information.\n\n## Getting the status of a virtual experiment instance\n\nA common user-task is to check the status of a virtual experiment instance. For\nexample, to see if it is still running, or, if finished, if there was any error.\n\nThe API method `api_rest_uid_status` returns the status of a given `rest_uid`:\n\n```python\n# put here the rest_uid of the virtual experiment instance\nrest_uid = \"toxicity-predictions-trol7a\"\n\nstatus = api.api_rest_uid_status(rest_uid)\n```\n\nThe variable `status` contains various information about the virtual experiment\ninstance in addition to its status. The _execution_ status is under the `status`\nkey, you can inspect by executing:\n\n```python\nimport json\nprint(json.dumps(status['status'], indent=2))\n```\n\nThe most important field is `exit-status`. ST4SD sets it after an experiment terminates.\n\nPossible values:\n\n- **Success**: The experiment existed successfully\n- **Failed**: The experiment failed (at least one component)\n- **Stopped**: The experiment was stopped/killed\n- **N/A**: The experiment is running and doesn't have an exit-status yet\n- **\"\"** (Empty): The experiment has not started.\n\nFor the full schema of the `status` dictionary see the section describing the [status of an experiment instance](#the-status-of-an-experiment-instance).\n\n## Get the details of a virtual experiment instance\n\nIn addition to the execution status information the `status` key also contains\nmetadata on the experiment. To see it:\n\n```python\nimport json\nprint(json.dumps(status['status']['meta'], indent=2))\n```\n\nThis will print a dictionary with the following keys\n\n- `arguments`: The command-line of the orchestrator\n- `data`: The list of files that override data files\n- `input`: The list of input files\n- `pid`: The process ID of the st4sd orchestrator\n- `platform`: The name of the platform that the virtual experiment instance\n  executes\n- `userVariables`: User provided variables, the schema is\n  `{'global':{name:value}, 'stages':{index:{name:value}}}`\n- `variables`: Global and stage variables active in the `platform`-scope that\n  the virtual experiment executes. The schema is\n  `{'global':{name:value}, 'stages':{index:{name:value}}}`\n- `hybridPlatform`: Name of hybrid-platform for communicating with LSF (can be\n  None),\n- `userMetadata`: A dictionary with `key(str): Any` value pairs that users can\n  provide\n- `instanceName`: The name of the directory containing the virtual experiment\n  instance.\n- `version`: The version of the st4sd orchestrator\n\n## Retrieving the outputs of a virtual experiment instance\n\nThere are multiple ways to retrieve outputs of virtual experiments: via a\nvirtual experiment interface, key-outputs, the Datastore APIs, and by leveraging\nST4SD's automated upload to S3.\n\n<AnchorLinks small>\n  <AnchorLink>Retrieving the properties measured by an experiment</AnchorLink>\n  <AnchorLink>Retrieving key-outputs</AnchorLink>\n  <AnchorLink>Automatically uploading key-outputs to S3</AnchorLink>\n  <AnchorLink>\n    Listing outputs produced by virtual experiment components\n  </AnchorLink>\n  <AnchorLink>Retrieving outputs via the ST4SD Datastore APIs</AnchorLink>\n</AnchorLinks>\n\n### Retrieving the properties measured by an experiment\n\nSome virtual experiments define interfaces which make it simple for users to\nretrieve the input systems and measured properties from runs of that virtual\nexperiment.\n\nLearn how to use virtual experiment interfaces [here](/using-a-virtual-experiment-interface/#getting-the-properties).\n\n### Retrieving key-outputs\n\nKey-Outputs are files produced by an experiment that the developer has flagged\nas being of special interest. Since the names of the file can be generic the\ndeveloper gives the key-output a descriptive label to better explain what it is.\n\nInformation on the key-outputs of a virtual experiment instance are stored in\nthe dictionary returned by `api_rest_uid_status`\n\n```python\nstatus = api.api_rest_uid_status(restUID)\npprint.pprint(instance_status['outputs'])\n```\n\nAn example of the output of this is\n\n```python\n{'OptimisationResults': {'creationtime': '1669584128.077387',\n                         'description': '',\n                         'filename': 'energies.csv',\n                         'filepath': 'stages/stage1/ExtractEnergies/energies.csv',\n                         'final': 'yes',\n                         'production': 'yes',\n                         'type': '',\n                         'version': '1'}}\n```\n\nIn the above example, the experiment has one key-output called **OptimisationResults**. This\nrefers to a file `energies.csv` produced by the component `ExtractEnergies` in\nthe given experiment.\n\nTo retrieve a key-output we can use `api_rest_uid_output`. This method returns\nthe contents as bytes so it needs to be converted to a string. Note: replace\n`OptimisationResults` with the name of a key-output relevant to an experiment\nyou have run.\n\n```python\nfilename, contents = api.api_rest_uid_output(rest_uid, 'OptimisationResults')\ncontents = contents.decode('utf-8', 'ignore')\n# read it into a pandas dataframe\ndf = pd.read_csv(io.StringIO(contents), sep=\",\", skipinitialspace=True)\n```\n\n### Automatically uploading key-outputs to S3\n\nST4SD supports automatically uploading key-outputs to S3. To enable this feature\ninclude the following values in the `payload` you provide to\n`api.api_experiment_start(experimentId, payload)` (see\n[Running a virtual experiment](#running-a-virtual-experiment) for more\ninformation):\n\n<InlineNotification>\n\nSee the [Cloud Object Store](/UsingCloudObjectStore) page for examples of\ngetting the values to populate the `credentials` section.\n\n</InlineNotification>\n\n<InlineNotification>\n\nNote that `bucketPath` is optional and it defaults to `/`.\n\n</InlineNotification>\n\nThe following example stores the key-outputs under `run1_output` in an bucket\ncalled`my-bucket`\n\n```json\n\"s3Store\":{\n      \"credentials\": {\n        \"accessKeyID\": \"$S3_AccessKeyID\",\n        \"secretAccessKey\": \"$S3_SecretAccessKey\",\n        \"endpoint\": \"$S3_ENDPOINT\",\n        \"region\": \"$S3_Region\",\n        \"bucket\": \"my-bucket\"\n      },\n      \"bucketPath\": \"/run1_output/\"\n    },\n```\n\n### Listing outputs produced by virtual experiment components\n\nBefore being able to retrieve the contents of the output files that the\nvirtual experiment components produce we need to know their paths. We start by\nretrieving the list of components that were part of our experiment through these\ntwo calls:\n\n```python\nmetadata = api.cdb_get_user_metadata_document_for_rest_uid(rest_uid)\ncomponents = api.cdb_get_document_component(instance=metadata['instance'])\n```\n\nThe list of full paths of the output files produced by each components will then\nbe available using the `files` key. We can access it for component `0` as such:\n\n```python\ncomponent_0_files=components[0]['files']\n```\n\n### Retrieving outputs via the ST4SD Datastore APIs\n\nTo retrieve outputs we use the `cdb_get_file_from_instance_uri` function.\n\n<InlineNotification>\n\nIn this example we reference variables that we retrieved in the previous section\n([Listing outputs produced by virtual experiment components](#listing-outputs-produced-by-virtual-experiment-components))\n\n</InlineNotification>\n\nThis function, however, expects to receive relative paths instead of full ones.\nWe can change our list of paths with this list comprehension:\n\n```python\ncomponent_0_files_relative = [component_file[component_file.index('/stages/'):] for component_file in component_0_files]\n```\n\nHere we show how to retrieve the first file produced by component `0` from the\nprevious example:\n\n```python\ndata = api.cdb_get_file_from_instance_uri(document[0]['instance'], component_0_files_relative[0])\n```\n\n## Stopping a virtual experiment instance\n\nThe API call to cancel and delete a virtual experiment instance is\n`api_rest_uid_delete`. Use this if you want to stop a run for any reason.\n\nThe `api_rest_uid_delete()` method _does not affect the files_ that the run has\nalready generated. However, it does cause the run, and any components that are\nstill running, to terminate. It also deletes any Kubernetes objects that have\nbeen created by the run.\n\nYou may also use this to delete Kubernetes objects created for running this\nvirtual experiment instance. Note that deleting the virtual experiment instance\n**does not** affect the files that it has produced.\n\n```python\n# put here the rest_uid of the virtual experiment instance\nrest_uid = \"toxicity-predictions-trol7a\"\n\napi.api_rest_uid_delete(rest_uid)\n```\n\n## More options when starting an experiment\n\n### Providing inputs via S3\n\nIf your input file is in an s3 bucket or a Datashim dataset you use the `s3`\ntop-level key of the payload dictionary to provides details for accessing the\nbucket/dataset.\n\n<InlineNotification>\n\n- If the filenames in the bucket do not match the names expected by the experiment you can ask ST4SD to map the S3 files to the input files by suffixing the S3 uri with the name of the input file: e.g. you can map the file `s3://my-bucket/path/my-file` to the input `my-input` with the following filename definition: `\"filename\": \"s3://my-bucket/path/my-file:my-input\"`\n- Only one bucket/dataset can be specified in the payload i.e. all files you\n  want to pass an experiment from s3 must be in the same bucket\n- The value of the `filename` key should be the path inside the bucket to find\n  the file.\n\n</InlineNotification>\n\nExample: Using s3. Fill the `s3` parameters with the required values. In this\ncase `input_filename.csv` is in the top-level of the bucket.\n\n```python\npayload = {\n    \"inputs\": [{\n       \"filename\": \"input_filename.csv\"\n   }],\n  \"s3\": {\n      \"accessKeyID\": \"$S3_AccessKeyID\",\n      \"secretAccessKey\": \"$S3_SecretAccessKey\",\n      \"bucket\": \"$S3_BUCKET_NAME\",\n      \"endpoint\": \"$S3_ENDPOINT\",\n      \"region\": \"$S3_Region\"\n  }\n}\n```\n\n### Providing inputs using Datashim\n\n\n[Datashim](https://github.com/datashim-io/datashim) is a Kubernetes Framework that presents S3 and NFS storage\nto pods in the form of PersistentVolumeClaim (PVC) resources. Datashims introduces a custom resource\ncalled Dataset which acts as a proxy to the S3 and NFS storage. The framework orchestrates the provisioning\nof Persistent Volume Claims and ConfigMaps needed for each Dataset. To enable support for using Datasets to\nprovide your input and data files to experiments there is a a one-time step for the\nentire cluster. A cluster-administrator must follow the installation instructions for Datashim in\nhttps://github.com/datashim-io/datashim#quickstart.\n\nFollowing that, users can create Datasets in a namespace that holds the ST4SD deployment following the\ninstructions in <https://github.com/datashim-io/datashim>.\n\n\n<InlineNotification>\n\n- If the filenames in the bucket do not match the names expected by the experiment you can ask ST4SD to map\n  the files in the Dataset to the input files by suffixing the S3 uri with the name of the input file:\n  e.g. you can map the file `path/my-file` to the input `my-input` with the following filename definition:\n  `\"filename\": \"path/my-file:my-input\"`\n- Only one dataset can be specified in the payload i.e. all files you\n  want to pass an experiment must be in the same Dataset\n- The value of the `filename` key should be the path inside the Dataset to find the file.\n\n</InlineNotification>\n\nIn the below example, the experiment will load the input `input_filename.csv` from the path\n`data/input_filename.csv` in the dataset `$MYDATASET_NAME`.\n\n```python\n\npayload = {\n    \"inputs\": [{\n       \"filename\": \"data/input_filename.csv\"\n   }],\n  \"s3\": {\n      \"dataset\": \"$MYDATASET_NAME\"\n  }\n}\n```\n\n## The status of an experiment instance\n\nUse the following python code to get the status of an experiment instance:\n\n```python\n# put here the rest_uid of the virtual experiment instance\nrest_uid = \"toxicity-predictions-trol7a\"\n\nstatus = api.api_rest_uid_status(rest_uid)\n```\n\n<InlineNotification kind=\"warning\">\n\nValues are case sensitive\n\n</InlineNotification>\n\n- `experiment-state`: Indicates the execution state of the experiment instance\n\n  - **Possible Values**:\n    - **unscheduled**: The experiment has not been scheduled to run yet. This\n      can be due to lack of resources, which may resolve, or be a critical issue\n      (unable to pull ST4SD images, unable to mount volumes)\n    - **unschedulable**: Required pre-tasks for the experiment execution failed\n      e.g. could not get workflow source, could not download s3 inputs.\n    - **Initialising**: The experiment is starting up\n    - **running**: The experiment has started running components\n    - **waiting_on_resource**: A component in the active stage is waiting on\n      resource\n    - **suspended**: The workflow execution has been suspended\n    - **finished**: The experiment is finished.\n    - **failed**: **Only** set if the experiment encountered an error during\n      initialisation (failed to run any steps of workflow after being started).\n      For example, fail to parse arguments, fail to create directory structures.\n\n- `exit-status`: Indicates how a **completed** experiment exited\n\n  - This receives its final value _after_ `experiment-state` transitions to\n    either `finished` or, in rare circumstances, `failed` (see above). Value\n    will be \"N/A\" or \"\" (empty) before this.\n  - **Possible Values**:\n    - **Success**: The experiment existed successfully\n    - **Failed**: The experiment failed (at least one component)\n    - **Stopped**: The experiment was stopped/killed\n    - **N/A**: The experiment is running and doesn't have an exit-status yet\n    - **\"\"** (Empty): The experiment has not started.\n\n- `error-description`: If the `exit-status` is **_Failed_** the value of this\n  key is a string which explains the failure cause.\n- `total-progress`: A number in [0.0, 1.0] indicating the progress of the\n  experiment. Note that workflow developers may decide to control this value.\n\n- `current-stage`: UID (e.g. `stage0`) of the active stage with the lowest stage\n  index\n\n- `stage-state`: Indicates the state of the active stage (a stage with a\n  component running) with with the lowest stage index. Value is one of\n  `[\"Initialising\", \"finished\", \"waiting_on_resource\",\"running\", \"component_shutdown\", \"failed\"]`\n\n- `stage-progress`: A number in [0.0, 1.0] indicating the progress of the active\n  stage with the lowest stage index. Note that workflow developers may decide to\n  control this value.\n\n<InlineNotification>\n\nAn experiment status can have `experiment-state=finished` with\n`exit-status=failed`. This means that the experiment completed with a failure.\n\n</InlineNotification>\n\n<InlineNotification kind=\"warning\">\n\nIn the case where there are no systems issues preventing an experiment to start\n`experiment-state` will always become `finished`. **You must check `exit-status`\nfor errors.**\n\n</InlineNotification>\n\nHere is an example of the status dictionary\n\n```json\n{\n  \"experiment-state\": \"finished\",\n  \"total-progress\": 1.0,\n  \"exit-status\": \"Success\",\n  \"stages\": [\n    \"Toxicity-prediction\"\n  ],\n  \"current-stage\": \"Toxicity-prediction\",\n  \"stage-state\": \"finished\",\n  \"stage-progress\": 1.0,\n  \"error-description\": \"\",\n  \"meta\": { ... omitted ... }\n}\n```\n\nIn the above example:\n\n1. The orchestrator observed that the virtual experiment instance terminated\n   (`experiment-state` = `finished`)\n2. The virtual experiment instance has produced all its outputs\n   (`total-progress` = `1.0`)\n3. The virtual experiment instance completed successfully (`exit-status` =\n   `Success`)\n4. The experiment had 1 stage (`stages` = `[\"Toxicity-prediction\"]`)\n5. `Toxicity-prediction` was the most recently executed stage with the lowest\n   stage index (`current-stage` = `Toxicity-prediction`).\n   1. All its tasks terminated and they were all successful (`stage-state` =\n      `finished`)\n   2. It reached its max progress (`stage-progress` = `1.0`)\n6. The virtual experiment instance did not raise any errors (`error-description`\n   = `\"\"`)\n\n\n## Learn more\n\n<div className=\"expressive-content-list-group\">\n\n  <ExpressiveList \n    title=\"Mastering stp\"\n    background=\"true\"\n    pictogram={<Tools />}>\n\nLearn how to master [our command line tool](/stp), `stp`, for creating and\nmanaging virtual experiment packages.\n\n  </ExpressiveList>\n\n<ExpressiveList\n    title=\"Take a look under the hood\"\n    background=\"true\"\n    pictogram={<ArtTools_01 />}>\n\nWant to learn more about how experiments are run? Learn about the\n[command line tool](/direct-run) that the API uses to launch experiments.\n\n  </ExpressiveList>\n  \n</div>\n","type":"Mdx","contentDigest":"915bae1c498bbd8d3988817c03e73f3b","owner":"gatsby-plugin-mdx","counter":266},"frontmatter":{"title":"The ST4SD Python Client API"},"exports":{},"rawBody":"---\ntitle: The ST4SD Python Client API\n---\n\nimport { Tools } from \"@carbon/pictograms-react\";\nimport { ArtTools_01 } from \"@carbon/pictograms-react\";\n\n<!--\n\n  Copyright IBM Inc. All Rights Reserved.\n  SPDX-License-Identifier: Apache-2.0\n\n-->\n\n<PageDescription>\n\nLearn how to use ST4SD python client API to run, query and interact with virtual\nexperiments\n\n</PageDescription>\n\n<AnchorLinks>\n\n<AnchorLink>Connecting to ST4SD</AnchorLink>\n <AnchorLink>Adding a virtual experiment package</AnchorLink>\n  <AnchorLink>Running a virtual experiment</AnchorLink>\n  <AnchorLink>Getting the status of a virtual experiment instance</AnchorLink>\n  <AnchorLink>Get the details of a virtual experiment instance</AnchorLink>\n  <AnchorLink>Retrieving the outputs of a virtual experiment instance</AnchorLink>\n  <AnchorLink>Stopping a virtual experiment instance</AnchorLink>\n</AnchorLinks>\n\n<InlineNotification>\n\n- **For**: Researchers\n- **Use When:**\n  - You want to run and get output of virtual experiments\n- **Skill Requirements:**\n  - Some knowledge of python\n\n</InlineNotification>\n\n<InlineNotification>\n\nThe code here can be tested using the\n[ST4SD Runtime API Example notebook](https://github.com/st4sd/st4sd-examples/blob/main/notebooks/ST4SD%20Runtime%20API%20Example.ipynb)\n\n</InlineNotification>\n\n## Overview\n\nWe provide a python API for interacting with the ST4SD Services. The API enables\nyou to:\n\n1. Launch virtual experiment instances\n2. Monitor virtual experiment instances\n3. Download outputs and measured properties of virtual experiment instances\n\n### Requirements\n\nThe basic requirements are access to an OpenShift instance with ST4SD installed\n(see the [ST4SD Cloud Installation instructions](/installation) for more\ninformation).\n\n#### Getting data into and out-of virtual experiments: Cloud Object Store\n\nAdditionally it can be useful to set up a Cloud Object Store bucket so you can\neasily get data into and out-of a virtual experiment.\n\n[See here](/UsingCloudObjectStore) for detailed instructions on how to do this\nwith IBM Cloud.\n\n### Examples\n\nWe maintain a [repository](https://github.com/st4sd/st4sd-examples/) containing\na set of IPython notebooks that illustrate interacting with virtual experiments\nvia these two methods.\n\nIf you are using a\n[local JupyterServer environment](/user-environments#local-jupyterserver-with-openshift-st4sd-instance)\nexecute this snippet to start-up the first notebook illustrating the RESTApi.\nMost of the examples discussed here are in this notebook\n\n```\ngit clone https://github.com/st4sd/st4sd-examples.git\ncd st4sd-examples\njupyter-notebook notebooks/ST4SD\\ Runtime\\ API\\ Example.ipynb\n```\n\n**_Note_: If you've installed `st4sd-runtime-core` into a virtualenv you will\nneed to activate it before executing above snippet**\n\nAfter reading this page have a browse around the `st4sd-examples` repository to\nsee what topics are covered in the other notebooks.\n\n## Connecting to ST4SD\n\nTo connect to an ST4SD instance you need to obtain an `auth-token` or use an\n`api-key`.\n\n<InlineNotification>\n\nTo learn what `auth-token`s and `api-key`s are in ST4SD and how to retrieve\nthem, read the\n[How do I connect to the ST4SD runtime service?](/runtime-service#how-do-i-connect-to-the-st4sd-runtime-service)\nsection.\n\n</InlineNotification>\n\nThe following code blocks show how to connect to an ST4SD instance.\n\n- Connect using an `auth-token`:\n\n    ```python\n      import experiment.service.db\n\n      # enter the https:// ST4SD url below\n      url=\"https://${your ST4SD url}\"\n\n      # enter your auth-token below\n      auth_token=\"put your authentication/token - do not share it with anyone\"\n\n      api=experiment.service.db.ExperimentRestAPI(url, cc_auth_token=auth_token)\n    ```\n\n    You can get an auth token by visiting `https://${your ST4SD url}/authorisation/token`.\n\n- Connect using an `api-key`:\n\n    ```python\n      import experiment.service.db\n\n      # enter the https:// ST4SD url below\n      url=\"https://${your ST4SD url}\"\n\n      # enter your auth-token below\n      bearer_key=\"put your api-key - do not share it with anyone\"\n\n      api=experiment.service.db.ExperimentRestAPI(url, cc_bearer_key=bearer_key)\n    ```\n\n    You can use the openshift token of service accounts for which there exists a Kubernetes RoleBinding to the `st4sd-authenticate-microservices` Role.\n    See our documentation on managing users with [ST4SD Cloud](/cloud-manage-users#give-a-group-of-users-access-to-just-the-st4sd-microservices) for more details.\n\n<InlineNotification>\n\nST4SD may print warnings to your python terminal e.g.\n`Unable to import tinydb module - tinydb interface not available`. These are not\nproblems. They are just informing you that select features are not supported on\nthe machine you are using. You can ignore them.\n\n</InlineNotification>\n\nThe `ExperimentRestAPI` initializer validates the authentication token you\nprovided and will raise an exception if it is invalid. If you do not get an\nException that means you can use `api` to interact with the\n`st4sd-runtime-service` and `st4sd-datastore` REST-APIs.\n\nThe above code needs to be executed once in a notebook session to get an `api`\ninstance to interact with. All the following examples assume this step has been\ndone.\n\n<InlineNotification>\n\nST4SD may print warnings to your python terminal e.g.\n`Unable to import pythonlsf - limited LSF functionality will be available`.\nThese are not problems. They are just informing you that select features are not\nsupported on the machine you are using. You can ignore them.\n\n</InlineNotification>\n\n## Adding a virtual experiment package\n\nBefore you can run a virtual experiment you need to add it to your ST4SD\nregistry. You do this using the `api_experiment_push()` method.\n\nTechnically you add a _parameterised_ virtual experiment package. A\nparameterised package can define particular, potentially complex values, for\nvariables in the virtual experiment to enable specific behaviour and make them\neasier to consume.\n\nYou can either type the definition of the\n[parameterised package](/creating-a-parameterised-package) manually or import it\nfrom another ST4SD registry\n[(e.g. the global ST4SD registry)](/using-the-virtual-experiments-registry-ui).\nIn the second-case the registry UI provides with the exact `api_experiment_push`\ncall you need to execute.\n\n## Running a virtual experiment\n\nThe API call `api_experiment_start` will start the virtual experiment that a\nparameterised virtual experiment package points to e.g.,\n\n```python\nrest_uid = api.api_experiment_start(experimentIdentifier, payload=...)\n```\n\n<InlineNotification kind=\"warning\">\n\nThe above example assumes `experimentIdentifier` is a valid [identifier](/creating-a-parameterised-package#the-parameterised-virtual-experiment-package-identifier-(pvep-identifier)) of a\nparameterised package (meaning the name of a Parameterised Virtual Experiment Package, with an optional\n[tag or SHA identifier](/creating-a-parameterised-package#package-tags)). The\nvalue of `payload` is a python dictionary that contains the run options to the\nexperiment.\n\n</InlineNotification>\n\nWhen you run a given virtual experiment you create a _virtual experiment\ninstance_. Each instance is assigned a unique identifier which is returned when\nyou start the virtual experiment. We use the term `ExperimentRunID` and\n`rest_uid` to refer to such identifiers.\n\nStarting an experiment involves:\n\n1. specifying experiment inputs\n2. (optional) specifying experiment variables\n3. (optional) selecting an experiment platform\n4. (optional) specifying experiment data\n\nThe following sections explain how to fill in the `payload`.\n\n### Specifying experiment inputs\n\n_inputs_ are files the experiment requires to run - they must be provided. Each\nexperiments documentation should explain what these files are.\n\nThe inputs are specified via the key `inputs` in the payload. The value of this\nkey is a list that has one item, a dictionary, for each required input file\ne.g.,\n\n```python\npayload = {\n    \"inputs\": [\n      {...},\n      ...\n    ]\n}\n```\n\nYou can provide the content of input files directly in the payload using the\n`content` key\n\n```python\ndata = pd.read_csv('mydata.csv')\npayload = {\n    \"inputs\": [{\n       \"content\": data.to_csv(index=False),\n       \"filename\": \"input_filename.csv\"\n   }]\n\n}\n```\n\nThis specifies that the content of the input file `input_filename.csv` comes\nfrom the pandas DataFrame `data`.\n\n### Specifying experiment variables\n\n_variables_ are optional parameters controlling the behaviour of the experiment,\ne.g. number of cpus. They are experiment specific i.e. the same variables don't\nexist in all experiment and variables controlling similar behaviour in two\nexperiments may not have the same name.\n\n<InlineNotification>\n\n_variables_ can only be overridden if the parameterised package allows it. If\nthis is the case the variables that can be set will be listed in the\n`executionOptions` section in the package description\n\n</InlineNotification>\n\n_variables_ are set using the `variables` key in the payload. The value of this\nkey is a dictionary of variable-name, variable-value pairs.\n\n```python\npayload = {\n    ... #Input/data options elided\n   \"variables\": {\n        \"startIndex\": 0,\n        \"numberMolecules\": 1,\n    }\n}\n```\n\n### Selecting an experiment platform\n\nThere are cases where variables are dependent on the values of other variables. ST4SD implements a feature called\nexperiment `platform` to help with these scenarios.\n\nYou can think of an experiment platform as a specialization of the experiment for a specific use case. Here are some\nof the ways developers use experiment platforms:\n\n1. maintain a single experiment definition which can execute on multiple different compute environments (OpenShift, HPC, ocal mode, local mode with containers, etc )\n2. support mass-configuration of steps to account for different scenarios (e.g. using less accurate methods which are faster to compute, etc)\n3. enable debug mode to assist debugging\n\nand many more!\n\nThe workflow developer can then use the [parameterised package section](/creating-a-parameterised-package#the-parameterisation-section) of experiments\nto encode the available options for running the experiment.\n\nIf the experiment has a `preset` platform (i.e. the field `parameterisation.presets.platform` of the Parameterised Virtual Experiment Package definition is set) then the user does not get the option to select which platform to use for the execution of the experiment. This option is hard-coded.\n\nIf the experiment has a configurable platform (i.e. the field `parameterisation.executionOptions.platforms` is set) then the user can select any of the options under `parameterisation.executionOptions.platforms`. If the user does not explicitly set a platform then the experiment instance will use the 1st platform in the `parameterisation.executionOptions.platforms` array.\n\n\n### Specifying experiment data\n\n_data_ refers to experiment configuration files that **may** be overridden by\nthe user.\n\n<InlineNotification>\n\n_data_ files can only be overridden if the parameterised package allows it. If\nthis is the case the files will be listed in the `executionOptions` section in\nthe package description\n\n</InlineNotification>\n\nThe data files are specified via the key `data` in the payload. The value of\nthis key is a list that has one item, a dictionary, for each `data` file you\nwant to override e.g.,\n\n```python\npayload = {\n    \"data\": [\n      {...},\n      ...\n    ]\n}\n```\n\nThe format of the `data` dictionary is identical to the `input` dictionary.\n\nProviding data files via S3/Datashim or providing their content directly follows\nsame process as described for inputs. See those sections for details.\n\n### Payload Details\n\nThe following is the complete structure of the `api_experiment_start()` payload\n(in `YAML`). This includes some advanced options not discussed here.\n\n```yaml\nplatform: name of platform (optional - see parameterisation notes)\ninputs: # optional (parameterised packages may have no inputs)\n  # see notes for interaction with Dataset/S3\n  - filename: str # required\n    content: str # optional - see S3 notes\ndata: # optional (parameterised packages may have no overiddable data files)\n  # see notes for interaction with Dataset/S3\n  - filename: str # required\n    content: str # optional - see S3 notes\nvolumes: # optional\n  - type:\n      # children fields are mutually exclusive - there must be exactly 1\n      persistentVolumeClaim: name of PVC object\n      dataset: name of Datashim dataset (see notes)\n      configMap: name of ConfigMap object\n      secret: name of Secret object\n    applicationDependency: name of appDep to bind to volume (optional)\n    # subPath applies only to pvc and dataset types\n    subPath: path inside volume to mount (optional, defaults to root of object)\nvariables: # optional - must adhere to parameterisation rules (see notes)\n  $name: $value\nadditionalOptions: # optional runtime arguments to elaunch.py\n  # they cannot override runtime arguments in the package parameterisation\n  - \"--someArgument=value\"\nenvironmentVariables: # optional\n  # creates environment variable in the pod that hosts elaunch.py\n  $name: $value\nmetadata: # optional\n  # custom metadata to insert into the \"user-metadata\" MongoDB document of run\n  $name: $value\ns3: # optional - used to read inputs/data with no \"content\" field\n  # dataset is mutually exclusive with any other sibling fields\n  dataset: name of Datashim dataset # optional\n  # can only provide fields below if dataset is unset\n  accessKeyID: S3 accessKeyID # optional\n  secretAccessKey: S3 secretAccessKey # optional\n  bucket: name of S3 bucket # optional\n  endpoint: the s3 endpoint # optional\n  region: the s3 region # optional\n# s3Store and datasetStoreURI are mutually exclusive and both optional\n# when one is provided, orchestrator uploads key-outputs to S3 after Run terminates\ns3Store: # optional\n  bucketPath:\n    path under bucket to store key-outputs files (default is\n    \"workflow_instances/\")\n  credentials: (same as s3 field)\ndatasetStoreURI: a \"dataset/$bucketPath\" URI (optional)\nruntimePolicy: # optional\n  name: the name of the runtime policy (required)\n  config: # optional\n    $key: Any # each runtime-policy has its own config-schema\n```\n\n<InlineNotification kind=\"info\">\n\nThe schema above is in YAML format. If you are using python (i.e. an instance of\n`experiment.service.db.ExperimentRestAPI`) you should either create an\nequivalent python dictionary OR a python string that contains the yaml\ndefinition and then convert the string into a dictionary using\n`yaml.loads(the_string)`.\n\n</InlineNotification>\n\n**Notes**\n\n1. `inputs` and `data` file-specifications have an optional `content` field. If\n   this field is missing then the contents of the files are expected to exist on\n   S3 or in a Dataset. When `s3` exists then the `filename` field acts as the\n   path inside the S3 bucket (or dataset) to use for reading the content of the\n   input/data file.\n1. Dataset objects are only available if a cluster-admin has installed\n   [Datashim](https://github.com/datashim-io/datashim) on the cluster.\n1. The fields `additionalOptions`, `data`, `inputs`, `platform`, and `variables`\n   must adhere to parameterisation rules. See the\n   [parameterised package documentation](/creating-a-parameterised-package#the-parameterisation-section)\n   for more information.\n\n## Getting the status of a virtual experiment instance\n\nA common user-task is to check the status of a virtual experiment instance. For\nexample, to see if it is still running, or, if finished, if there was any error.\n\nThe API method `api_rest_uid_status` returns the status of a given `rest_uid`:\n\n```python\n# put here the rest_uid of the virtual experiment instance\nrest_uid = \"toxicity-predictions-trol7a\"\n\nstatus = api.api_rest_uid_status(rest_uid)\n```\n\nThe variable `status` contains various information about the virtual experiment\ninstance in addition to its status. The _execution_ status is under the `status`\nkey, you can inspect by executing:\n\n```python\nimport json\nprint(json.dumps(status['status'], indent=2))\n```\n\nThe most important field is `exit-status`. ST4SD sets it after an experiment terminates.\n\nPossible values:\n\n- **Success**: The experiment existed successfully\n- **Failed**: The experiment failed (at least one component)\n- **Stopped**: The experiment was stopped/killed\n- **N/A**: The experiment is running and doesn't have an exit-status yet\n- **\"\"** (Empty): The experiment has not started.\n\nFor the full schema of the `status` dictionary see the section describing the [status of an experiment instance](#the-status-of-an-experiment-instance).\n\n## Get the details of a virtual experiment instance\n\nIn addition to the execution status information the `status` key also contains\nmetadata on the experiment. To see it:\n\n```python\nimport json\nprint(json.dumps(status['status']['meta'], indent=2))\n```\n\nThis will print a dictionary with the following keys\n\n- `arguments`: The command-line of the orchestrator\n- `data`: The list of files that override data files\n- `input`: The list of input files\n- `pid`: The process ID of the st4sd orchestrator\n- `platform`: The name of the platform that the virtual experiment instance\n  executes\n- `userVariables`: User provided variables, the schema is\n  `{'global':{name:value}, 'stages':{index:{name:value}}}`\n- `variables`: Global and stage variables active in the `platform`-scope that\n  the virtual experiment executes. The schema is\n  `{'global':{name:value}, 'stages':{index:{name:value}}}`\n- `hybridPlatform`: Name of hybrid-platform for communicating with LSF (can be\n  None),\n- `userMetadata`: A dictionary with `key(str): Any` value pairs that users can\n  provide\n- `instanceName`: The name of the directory containing the virtual experiment\n  instance.\n- `version`: The version of the st4sd orchestrator\n\n## Retrieving the outputs of a virtual experiment instance\n\nThere are multiple ways to retrieve outputs of virtual experiments: via a\nvirtual experiment interface, key-outputs, the Datastore APIs, and by leveraging\nST4SD's automated upload to S3.\n\n<AnchorLinks small>\n  <AnchorLink>Retrieving the properties measured by an experiment</AnchorLink>\n  <AnchorLink>Retrieving key-outputs</AnchorLink>\n  <AnchorLink>Automatically uploading key-outputs to S3</AnchorLink>\n  <AnchorLink>\n    Listing outputs produced by virtual experiment components\n  </AnchorLink>\n  <AnchorLink>Retrieving outputs via the ST4SD Datastore APIs</AnchorLink>\n</AnchorLinks>\n\n### Retrieving the properties measured by an experiment\n\nSome virtual experiments define interfaces which make it simple for users to\nretrieve the input systems and measured properties from runs of that virtual\nexperiment.\n\nLearn how to use virtual experiment interfaces [here](/using-a-virtual-experiment-interface/#getting-the-properties).\n\n### Retrieving key-outputs\n\nKey-Outputs are files produced by an experiment that the developer has flagged\nas being of special interest. Since the names of the file can be generic the\ndeveloper gives the key-output a descriptive label to better explain what it is.\n\nInformation on the key-outputs of a virtual experiment instance are stored in\nthe dictionary returned by `api_rest_uid_status`\n\n```python\nstatus = api.api_rest_uid_status(restUID)\npprint.pprint(instance_status['outputs'])\n```\n\nAn example of the output of this is\n\n```python\n{'OptimisationResults': {'creationtime': '1669584128.077387',\n                         'description': '',\n                         'filename': 'energies.csv',\n                         'filepath': 'stages/stage1/ExtractEnergies/energies.csv',\n                         'final': 'yes',\n                         'production': 'yes',\n                         'type': '',\n                         'version': '1'}}\n```\n\nIn the above example, the experiment has one key-output called **OptimisationResults**. This\nrefers to a file `energies.csv` produced by the component `ExtractEnergies` in\nthe given experiment.\n\nTo retrieve a key-output we can use `api_rest_uid_output`. This method returns\nthe contents as bytes so it needs to be converted to a string. Note: replace\n`OptimisationResults` with the name of a key-output relevant to an experiment\nyou have run.\n\n```python\nfilename, contents = api.api_rest_uid_output(rest_uid, 'OptimisationResults')\ncontents = contents.decode('utf-8', 'ignore')\n# read it into a pandas dataframe\ndf = pd.read_csv(io.StringIO(contents), sep=\",\", skipinitialspace=True)\n```\n\n### Automatically uploading key-outputs to S3\n\nST4SD supports automatically uploading key-outputs to S3. To enable this feature\ninclude the following values in the `payload` you provide to\n`api.api_experiment_start(experimentId, payload)` (see\n[Running a virtual experiment](#running-a-virtual-experiment) for more\ninformation):\n\n<InlineNotification>\n\nSee the [Cloud Object Store](/UsingCloudObjectStore) page for examples of\ngetting the values to populate the `credentials` section.\n\n</InlineNotification>\n\n<InlineNotification>\n\nNote that `bucketPath` is optional and it defaults to `/`.\n\n</InlineNotification>\n\nThe following example stores the key-outputs under `run1_output` in an bucket\ncalled`my-bucket`\n\n```json\n\"s3Store\":{\n      \"credentials\": {\n        \"accessKeyID\": \"$S3_AccessKeyID\",\n        \"secretAccessKey\": \"$S3_SecretAccessKey\",\n        \"endpoint\": \"$S3_ENDPOINT\",\n        \"region\": \"$S3_Region\",\n        \"bucket\": \"my-bucket\"\n      },\n      \"bucketPath\": \"/run1_output/\"\n    },\n```\n\n### Listing outputs produced by virtual experiment components\n\nBefore being able to retrieve the contents of the output files that the\nvirtual experiment components produce we need to know their paths. We start by\nretrieving the list of components that were part of our experiment through these\ntwo calls:\n\n```python\nmetadata = api.cdb_get_user_metadata_document_for_rest_uid(rest_uid)\ncomponents = api.cdb_get_document_component(instance=metadata['instance'])\n```\n\nThe list of full paths of the output files produced by each components will then\nbe available using the `files` key. We can access it for component `0` as such:\n\n```python\ncomponent_0_files=components[0]['files']\n```\n\n### Retrieving outputs via the ST4SD Datastore APIs\n\nTo retrieve outputs we use the `cdb_get_file_from_instance_uri` function.\n\n<InlineNotification>\n\nIn this example we reference variables that we retrieved in the previous section\n([Listing outputs produced by virtual experiment components](#listing-outputs-produced-by-virtual-experiment-components))\n\n</InlineNotification>\n\nThis function, however, expects to receive relative paths instead of full ones.\nWe can change our list of paths with this list comprehension:\n\n```python\ncomponent_0_files_relative = [component_file[component_file.index('/stages/'):] for component_file in component_0_files]\n```\n\nHere we show how to retrieve the first file produced by component `0` from the\nprevious example:\n\n```python\ndata = api.cdb_get_file_from_instance_uri(document[0]['instance'], component_0_files_relative[0])\n```\n\n## Stopping a virtual experiment instance\n\nThe API call to cancel and delete a virtual experiment instance is\n`api_rest_uid_delete`. Use this if you want to stop a run for any reason.\n\nThe `api_rest_uid_delete()` method _does not affect the files_ that the run has\nalready generated. However, it does cause the run, and any components that are\nstill running, to terminate. It also deletes any Kubernetes objects that have\nbeen created by the run.\n\nYou may also use this to delete Kubernetes objects created for running this\nvirtual experiment instance. Note that deleting the virtual experiment instance\n**does not** affect the files that it has produced.\n\n```python\n# put here the rest_uid of the virtual experiment instance\nrest_uid = \"toxicity-predictions-trol7a\"\n\napi.api_rest_uid_delete(rest_uid)\n```\n\n## More options when starting an experiment\n\n### Providing inputs via S3\n\nIf your input file is in an s3 bucket or a Datashim dataset you use the `s3`\ntop-level key of the payload dictionary to provides details for accessing the\nbucket/dataset.\n\n<InlineNotification>\n\n- If the filenames in the bucket do not match the names expected by the experiment you can ask ST4SD to map the S3 files to the input files by suffixing the S3 uri with the name of the input file: e.g. you can map the file `s3://my-bucket/path/my-file` to the input `my-input` with the following filename definition: `\"filename\": \"s3://my-bucket/path/my-file:my-input\"`\n- Only one bucket/dataset can be specified in the payload i.e. all files you\n  want to pass an experiment from s3 must be in the same bucket\n- The value of the `filename` key should be the path inside the bucket to find\n  the file.\n\n</InlineNotification>\n\nExample: Using s3. Fill the `s3` parameters with the required values. In this\ncase `input_filename.csv` is in the top-level of the bucket.\n\n```python\npayload = {\n    \"inputs\": [{\n       \"filename\": \"input_filename.csv\"\n   }],\n  \"s3\": {\n      \"accessKeyID\": \"$S3_AccessKeyID\",\n      \"secretAccessKey\": \"$S3_SecretAccessKey\",\n      \"bucket\": \"$S3_BUCKET_NAME\",\n      \"endpoint\": \"$S3_ENDPOINT\",\n      \"region\": \"$S3_Region\"\n  }\n}\n```\n\n### Providing inputs using Datashim\n\n\n[Datashim](https://github.com/datashim-io/datashim) is a Kubernetes Framework that presents S3 and NFS storage\nto pods in the form of PersistentVolumeClaim (PVC) resources. Datashims introduces a custom resource\ncalled Dataset which acts as a proxy to the S3 and NFS storage. The framework orchestrates the provisioning\nof Persistent Volume Claims and ConfigMaps needed for each Dataset. To enable support for using Datasets to\nprovide your input and data files to experiments there is a a one-time step for the\nentire cluster. A cluster-administrator must follow the installation instructions for Datashim in\nhttps://github.com/datashim-io/datashim#quickstart.\n\nFollowing that, users can create Datasets in a namespace that holds the ST4SD deployment following the\ninstructions in <https://github.com/datashim-io/datashim>.\n\n\n<InlineNotification>\n\n- If the filenames in the bucket do not match the names expected by the experiment you can ask ST4SD to map\n  the files in the Dataset to the input files by suffixing the S3 uri with the name of the input file:\n  e.g. you can map the file `path/my-file` to the input `my-input` with the following filename definition:\n  `\"filename\": \"path/my-file:my-input\"`\n- Only one dataset can be specified in the payload i.e. all files you\n  want to pass an experiment must be in the same Dataset\n- The value of the `filename` key should be the path inside the Dataset to find the file.\n\n</InlineNotification>\n\nIn the below example, the experiment will load the input `input_filename.csv` from the path\n`data/input_filename.csv` in the dataset `$MYDATASET_NAME`.\n\n```python\n\npayload = {\n    \"inputs\": [{\n       \"filename\": \"data/input_filename.csv\"\n   }],\n  \"s3\": {\n      \"dataset\": \"$MYDATASET_NAME\"\n  }\n}\n```\n\n## The status of an experiment instance\n\nUse the following python code to get the status of an experiment instance:\n\n```python\n# put here the rest_uid of the virtual experiment instance\nrest_uid = \"toxicity-predictions-trol7a\"\n\nstatus = api.api_rest_uid_status(rest_uid)\n```\n\n<InlineNotification kind=\"warning\">\n\nValues are case sensitive\n\n</InlineNotification>\n\n- `experiment-state`: Indicates the execution state of the experiment instance\n\n  - **Possible Values**:\n    - **unscheduled**: The experiment has not been scheduled to run yet. This\n      can be due to lack of resources, which may resolve, or be a critical issue\n      (unable to pull ST4SD images, unable to mount volumes)\n    - **unschedulable**: Required pre-tasks for the experiment execution failed\n      e.g. could not get workflow source, could not download s3 inputs.\n    - **Initialising**: The experiment is starting up\n    - **running**: The experiment has started running components\n    - **waiting_on_resource**: A component in the active stage is waiting on\n      resource\n    - **suspended**: The workflow execution has been suspended\n    - **finished**: The experiment is finished.\n    - **failed**: **Only** set if the experiment encountered an error during\n      initialisation (failed to run any steps of workflow after being started).\n      For example, fail to parse arguments, fail to create directory structures.\n\n- `exit-status`: Indicates how a **completed** experiment exited\n\n  - This receives its final value _after_ `experiment-state` transitions to\n    either `finished` or, in rare circumstances, `failed` (see above). Value\n    will be \"N/A\" or \"\" (empty) before this.\n  - **Possible Values**:\n    - **Success**: The experiment existed successfully\n    - **Failed**: The experiment failed (at least one component)\n    - **Stopped**: The experiment was stopped/killed\n    - **N/A**: The experiment is running and doesn't have an exit-status yet\n    - **\"\"** (Empty): The experiment has not started.\n\n- `error-description`: If the `exit-status` is **_Failed_** the value of this\n  key is a string which explains the failure cause.\n- `total-progress`: A number in [0.0, 1.0] indicating the progress of the\n  experiment. Note that workflow developers may decide to control this value.\n\n- `current-stage`: UID (e.g. `stage0`) of the active stage with the lowest stage\n  index\n\n- `stage-state`: Indicates the state of the active stage (a stage with a\n  component running) with with the lowest stage index. Value is one of\n  `[\"Initialising\", \"finished\", \"waiting_on_resource\",\"running\", \"component_shutdown\", \"failed\"]`\n\n- `stage-progress`: A number in [0.0, 1.0] indicating the progress of the active\n  stage with the lowest stage index. Note that workflow developers may decide to\n  control this value.\n\n<InlineNotification>\n\nAn experiment status can have `experiment-state=finished` with\n`exit-status=failed`. This means that the experiment completed with a failure.\n\n</InlineNotification>\n\n<InlineNotification kind=\"warning\">\n\nIn the case where there are no systems issues preventing an experiment to start\n`experiment-state` will always become `finished`. **You must check `exit-status`\nfor errors.**\n\n</InlineNotification>\n\nHere is an example of the status dictionary\n\n```json\n{\n  \"experiment-state\": \"finished\",\n  \"total-progress\": 1.0,\n  \"exit-status\": \"Success\",\n  \"stages\": [\n    \"Toxicity-prediction\"\n  ],\n  \"current-stage\": \"Toxicity-prediction\",\n  \"stage-state\": \"finished\",\n  \"stage-progress\": 1.0,\n  \"error-description\": \"\",\n  \"meta\": { ... omitted ... }\n}\n```\n\nIn the above example:\n\n1. The orchestrator observed that the virtual experiment instance terminated\n   (`experiment-state` = `finished`)\n2. The virtual experiment instance has produced all its outputs\n   (`total-progress` = `1.0`)\n3. The virtual experiment instance completed successfully (`exit-status` =\n   `Success`)\n4. The experiment had 1 stage (`stages` = `[\"Toxicity-prediction\"]`)\n5. `Toxicity-prediction` was the most recently executed stage with the lowest\n   stage index (`current-stage` = `Toxicity-prediction`).\n   1. All its tasks terminated and they were all successful (`stage-state` =\n      `finished`)\n   2. It reached its max progress (`stage-progress` = `1.0`)\n6. The virtual experiment instance did not raise any errors (`error-description`\n   = `\"\"`)\n\n\n## Learn more\n\n<div className=\"expressive-content-list-group\">\n\n  <ExpressiveList \n    title=\"Mastering stp\"\n    background=\"true\"\n    pictogram={<Tools />}>\n\nLearn how to master [our command line tool](/stp), `stp`, for creating and\nmanaging virtual experiment packages.\n\n  </ExpressiveList>\n\n<ExpressiveList\n    title=\"Take a look under the hood\"\n    background=\"true\"\n    pictogram={<ArtTools_01 />}>\n\nWant to learn more about how experiments are run? Learn about the\n[command line tool](/direct-run) that the API uses to launch experiments.\n\n  </ExpressiveList>\n  \n</div>\n","fileAbsolutePath":"/home/travis/build/st4sd/overview/src/pages/running-workflows-on-openshift.mdx"}}},"staticQueryHashes":["1364590287","137577622","2102389209","2456312558","2746626797","3018647132","3037994772","768070550"]}