{"componentChunkName":"component---src-pages-using-cloud-object-store-md","path":"/UsingCloudObjectStore/","result":{"pageContext":{"frontmatter":{"title":"Using Cloud Object Store"},"relativePagePath":"/UsingCloudObjectStore.md","titleType":"page","MdxNode":{"id":"718c45fb-38c9-588e-b727-5e14c5e6805a","children":[],"parent":"20a700a3-ddd9-5753-abe0-c81ac2612058","internal":{"content":"---\ntitle: Using Cloud Object Store\n---\n\n<!--\n\n  Copyright IBM Inc. All Rights Reserved.\n  SPDX-License-Identifier: Apache-2.0\n\n-->\n\n<PageDescription>\n\nUse this page to learn how to create a Cloud Object Storage bucket on IBM Cloud, use it in ST4SD and accessing it on your laptop.\n\n</PageDescription>\n\n<AnchorLinks>\n  <AnchorLink>Creating a bucket</AnchorLink>\n  <AnchorLink>Getting the bucket access details</AnchorLink>\n  <AnchorLink>Using the bucket with st4sd-runtime</AnchorLink>\n  <AnchorLink>Accessing COS Buckets on your laptop</AnchorLink>\n</AnchorLinks>\n\nCloud Object Store (COS) buckets are places you can store files in the cloud. This guide describes how to create a COS bucket you can use to provide data to, and get data from, workflows running on Kubernetes/OpenShift.\n\nThe advantage of this is that you can have the bucket appear as a folder on your local computer, allowing you to easily place input files for workflows in it. Further workflows can be configured to copy their output files to this bucket so they will appear in this folder too - even if they executed in some remote cluster.\n\n## Creating a bucket\n\n<AnchorLinks small>\n  <AnchorLink>Login to IBM Cloud</AnchorLink>\n  <AnchorLink>Create a COS bucket</AnchorLink>\n</AnchorLinks>\n\n### Login to IBM Cloud\n\nVisit <https://cloud.ibm.com/>. There's a drop down menu at the top right corner where you can select your active account. Here we pick the option that reads `289600 - RIS`.\n\n![image.png](../assets/images/using-cloud-object-store/363df92f-bda6-4c0f-b0a0-bd463d4a2b5a.png)\n\nClick on the `Resource List` in the left hand menu:\n\n![image.png](../assets/images/using-cloud-object-store/49fa34a7-5be8-4106-a888-f910d963aafb.png)\n\nSelect `Storage` then click on the `Cloud Object Storage` service instance you want to host the bucket\n\n![image.png](../assets/images/using-cloud-object-store/0d861fd4-f80e-4289-8b9d-56a008528010.png)\n\nIf there is no `Cloud Object Storage` instance under `Storage` you will have to [create one](https://cloud.ibm.com/docs/cloud-object-storage?topic=cloud-object-storage-provision).\n\n### Create a COS bucket\n\nWhen you click on the cloud storage instance you should see a screen like the following: Click `Create bucket`\n\n![image-20210202081906903](../assets/images/using-cloud-object-store/image-20210202081906903.png)\n\nand then `Quickly get started`(click the arrow in the tile)\n\n![image-20210202081927265](../assets/images/using-cloud-object-store/image-20210202081927265.png)\n\nOn the following screen (see below) enter a name for your bucket. You will use the name this name in the next steps. Also make a note of the bucket region, which is listed in the panel on the right - in this case it is `eu-de`.\n\n![Screenshot 2021-02-04 at 21.17.08](../assets/images/using-cloud-object-store/Screenshot-bucket-create.png)\n\nClick the `Next`button, then you'll be greeted with a webpage like this:\n\n![image.png](../assets/images/using-cloud-object-store/07217461-3ba2-4cd8-a989-0e1dc724c91c.png)\n\nYour bucket is now created and you can add files through this page. The next sections details easier ways to interact with the bucket as a folder on your laptop and also how to use it with the `st4sd-runtime`.\n\n## Getting the bucket access details\n\n<AnchorLinks small>\n  <AnchorLink>Getting the S3 credentials</AnchorLink>\n  <AnchorLink>Get an endpoint</AnchorLink>\n</AnchorLinks>\n\nThe next step is to get the details that allow external programs to access the bucket\n\n### Getting the S3 credentials\n\nFrom the page shown in the last image click on the grey button `View bucket configuration` and then click on `Service credentials` on the left-most panel on the webpage. You will be offered a list of Service credentials to pick.\n\nThere is normally an entry `Service credentials-1`. Click on the dropdown button on its left and copy the values for the fields `cos_hmac_keys.access_key_id` and `cos_hmac_keys.secret_access_key`.\n\nIf you encounter either of the following\n\n- There is no `hmac` entry!\n- There is no `Service credentials-1`!\n\ngo to the next section, else continue to section `Get an endpoint`.\n\n#### Creating a credential\n\nThere are a couple of reasons `Service credentials-1` is not present, for example\n\n- Someone may have renamed it\n- The first person to create a bucket did not use a `smart tier storage class bucket` and forgot to toggle the `Include HMAC Credential Option`\n\nHowever, both are is easily corrected by creating a new credential\n\n1. Click `New Credential`\n2. Give it a name\n3. Chose `Writer` for `role`\n4. Click on `Advanced options`\n5. Toggle on that `Include HMAC Credential` option\n\n![image-20210202083358811](../assets/images/using-cloud-object-store/image-20210202083358811.png)\n\nNote that, even though it is best practice to periodically rotate the service credentials you do not need to create new credentials each time you execute a virtual experiment. We suggest that each person generates their own service credentials and periodically rotates them.\n\n### Get an endpoint\n\n**Note**: Do not use the `endpoint` URL that you see listed in your HMAC `Service credentials` (section above). Please follow the instructions below to locate your S3 endpoint.\n\nClick on the `Buckets` link in the left panel, find your bucket (use the search box if you like) and click on it.\n\nClick on `Details` button in top right\n\n![image-20210204164241892](../assets/images/using-cloud-object-store/image-20210204164241892.png)\n\nA side panel will appear on the right. Click the `View configuration` button. This will bring up a page whiich has a section called `Endpoints` e.g.\n\n![image.png](../assets/images/using-cloud-object-store/29996139-b0b1-406e-8c25-1fb010ee793b.png)\n\nThe `public` endpoint is usually the easiest to go with. However, if you are using a Cloud Object Storage instance that is located in the same IBM Cloud organization we suggest using the `private` endpoint as it is faster and cheaper.\n\n## Using the bucket with `st4sd-runtime`\n\n<AnchorLinks small>\n  <AnchorLink>S3 method</AnchorLink>\n  <AnchorLink>Datashim method</AnchorLink>\n</AnchorLinks>\n\nThere are two ways to use a COS bucket, directly (`s3`) and via [**Datashim**](https://datashim.io/). You can find more examples that you can use as a starting point for your notebooks in our [ST4SD example notebooks](https://github.com/st4sd/st4sd-examples/blob/main/notebooks/ST4SD%20Runtime%20API%20Example.ipynb).\n\nDatashim simplifies access to buckets allowing one person to create a `dataset` (a label for the bucket) and others to use it without having to know the access keys or endpoint. It also means the bucket can be moved and users of the bucket don't have to worry as long as the dataset is updated.\n\nWe recommend using Datashim in the long run for managing datasets with workflows. However using Datashim requires it to be installed in the target cluster - check this with the cluster admin.\n\n### S3 method\n\nTo use the bucket directly when submitting a workflow/virtual experiment from a notebook add the following to the submission configuration (see the [ST4SD Runtime API Example](https://github.com/st4sd/st4sd-examples/blob/main/notebooks/ST4SD%20Runtime%20API%20Example.ipynb) notebook for examples of submitting workflows)\n\n```json\n\n\"s3\": {\n  \"accessKeyID\": \"the  contents of `cos_hmac_keys.access_key_id`\",\n  \"secretAccessKey\": \"the contents of  `cos_hmac_keys.secret_access_key`\",\n  \"endpoint\":  \"your endpoint prefixed with https:// (e.g. https://s3.eu-de.cloud-object-storage.appdomain.cloud)\",\n  \"bucket\": \"the name of your bucket here\"\n},\n\n```\n\nYou can then specify files in the bucket as values to the `input` and `data` fields in the experiment configuration. For example, if you add a bucket and specify the following\n\n```json\n   \"data\": [{\n       \"filename\": \"get-smiles/input_smiles.csv\"\n   }]\n```\n\nThen this file, `input_smiles.csv`, will be retrieved from the folder `get-smiles/` in the bucket.\n\nYou may also store key-outputs to `S3`. For example, [`sum-numbers`](https://github.com/st4sd/sum-numbers/) has a `TotalSum` key-output. You could use the payload below to upload that output to an S3 bucket:\n\n```python\n\"s3Store\":{\n  \"credentials\": {\n    \"accessKeyID\": \"the  contents of `cos_hmac_keys.access_key_id`\",\n    \"secretAccessKey\": \"the contents of  `cos_hmac_keys.secret_access_key`\",\n    \"endpoint\": \"your endpoint prefixed with https:// (e.g. https://s3.eu-de.cloud-object-storage.appdomain.cloud)\",\n    \"bucket\": \"my-bucket\", # the name of your bucket\n  },\n  \"bucketPath\": \"/run1_output/\"\n}\n```\n\nWhen sum-numbers finishes, it will store the file `out.stdout` on the bucket `my-bucket` under the path `/run1_output/out.stdout`.\n\n### Datashim method\n\nThe Datashim method is similar to the previous except you provide the `s3` access informattion separately to creating a workflow and give it a name. You, and others, can then use this name to access the bucket instead of having to provide the keys etc.\n\nFor example if you wanted to create a dataset called `my-dataset` for a bucket, first create a dictionary containing (assuming you are in a iPython notebook)\n\n```python\npayload = {\n  \"access_key_id\": \"the  contents of `cos_hmac_keys.access_key_id`\",\n  \"secret_access_key\": \"the contents of  `cos_hmac_keys.secret_access_key`\",\n  \"endpoint\": \"your endpoint prefixed with https:// (e.g. https://s3.eu-de.cloud-object-storage.appdomain.cloud)\",\n  \"bucket\": \"my-bucket\", # the name of your bucket\n}\n# notice that the payload has snake-case fields (these are arguments to a python method)\n```\n\nThen execute\n\n```python\napi.api_dataset_create(dataset_name=\"my-dataset\", **payload)\n```\n\nYou can list the available datasets using\n\n```python\nr = api.api_datasets_list()\nprint(r)\n```\n\nWhen you submit the experiment you no longer need a full `s3` entry like the previous method. Instead, you use the dataset label\n\n```json\n\"s3\": {\n  \"dataset\": \"my-dataset\"\n},\n```\n\nYou may also store key-ouputs of your virtual experiment on buckets for which you have created a Dataset. For example:\n\n```python\n\"s3Store\": {\n  \"datasetStoreURI\": \"dataset://my-dataset/run1_output\"\n}\n```\n\nThe above snippet stores the workflow outputs into the bucket of Dataset `my-dataset` using `datasetStoreURI`. In this case it puts it into a sub-folder called `run1_output`. Notice that by using `s3Store.datasetStoreURI` you get the same behavior with specifying `s3Store.credentials` and `s3Store.bucketPath`.\n\n#### Validate that Dataset has been successfully created\n\nFor this step you need access to the OpenShift cluster that is hosting the associated ST4SD instance.\n\nTo check if the dataset `my-dataset` was properly created, `oc login` to the cluster and use `oc project` to switch to the project that holds the ST4SD instance. Execute:\n\n```\noc describe dataset my-dataset\n```\n\nThe above should print a description of the `my-dataset` object. Focus on the end of the printout, the part that contains the `Status` of the object. After the Dataet object has been successfully created it should look similar to:\n\n```python\n...\nStatus:\n  Caching:\n    Info:    No DLF caching plugins are installed\n    Status:  Disabled\n  Provision:\n    Status:  OK\n```\n\nIt normally takes about 30 seconds for a Dataset object to finish provisioning. The status will display warning messages if there is something wrong with the configuration of your Dataset. The most common mistake is using the wrong `endpoint`, `bucket`, or `credentials`. The next section discusses addressing such problems.\n\n#### I used wrong the bucket/credentials/endpoint to create my dataset, what do I do?\n\nFor this step you need access to the OpenShift cluster that is hosting the associated ST4SD instance.\n\nFirst, `oc login` to the cluster, use `oc project` to switch to the project that holds the ST4SD instance, and then delete the `Dataset` object with the invalid configuration. Then you should wait for a few seconds until the `PersistentVolumeClaim` with the same name as your dataset is garbage collected and then create the dataset again using the correct configuration.\n\nFor example, to delete the `my-dataset` Dataset in the `st4sd-production` namespace after logging in to OpenShift on your terminal, run\n\n```python\noc project st4sd-production\noc delete dataset my-dataset\n# wait for the PersistentVolumeClaim (PVC) `my-dataset` to be deleted, run the command below every few seconds\n# until it returns that there is no such object\noc get pvc my-dataset\n```\n\nAt this point you can go back to your python notebook, correct the `Dataset` payload and execute the code:\n\n```python\napi.api_dataset_create(dataset_name=\"my-dataset\", **payload)\n```\n\n## Accessing COS Buckets on your laptop\n\nYou can mount a COS bucket on your laptop so it appears as any other folder using a variety of tools. This means you can provide workflow input by copying into this folder, and also easily browse workflow output.\n\nHere will demonstrate using `s3fs` (https://github.com/s3fs-fuse/s3fs-fuse). The general command is:\n\n```\ns3fs  $BUCKETNAME -o url=$ENDPOINT $MOUNTPOINT -o passwd_file=$PASSWORDFIILE\n```\n\nHere's a concrete example for a bucket called `cloud-object-storage-drl-cos-standard-ecx-focintegration`\n\n```\ns3fs cloud-object-storage-drl-cos-standard-ecx-focintegration -o url=https://s3.eu-de.cloud-object-storage.appdomain.cloud /Users/michaelj/git-working/st4sd-examples/mvp2bucket/ -o passwd_file=${HOME}/.passwd-s3fs\n```\n\nTo create the password file do e.g.\n\n```\necho ACCESS_KEY_ID:SECRET_ACCESS_KEY > ${HOME}/.passwd-s3fs\nchmod 600 ${HOME}/.passwd-s3fs\n```\n\nWhere\n\n- `ACCESS_KEY_ID` - the contents of `cos_hmac_keys.access_key_id`\n- `SECRET_ACCESS_KEY ` - the contents of `cos_hmac_keys.secret_access_key`\n","type":"Mdx","contentDigest":"4976e95325de41ed9e6d38df53ab9d3d","owner":"gatsby-plugin-mdx","counter":264},"frontmatter":{"title":"Using Cloud Object Store"},"exports":{},"rawBody":"---\ntitle: Using Cloud Object Store\n---\n\n<!--\n\n  Copyright IBM Inc. All Rights Reserved.\n  SPDX-License-Identifier: Apache-2.0\n\n-->\n\n<PageDescription>\n\nUse this page to learn how to create a Cloud Object Storage bucket on IBM Cloud, use it in ST4SD and accessing it on your laptop.\n\n</PageDescription>\n\n<AnchorLinks>\n  <AnchorLink>Creating a bucket</AnchorLink>\n  <AnchorLink>Getting the bucket access details</AnchorLink>\n  <AnchorLink>Using the bucket with st4sd-runtime</AnchorLink>\n  <AnchorLink>Accessing COS Buckets on your laptop</AnchorLink>\n</AnchorLinks>\n\nCloud Object Store (COS) buckets are places you can store files in the cloud. This guide describes how to create a COS bucket you can use to provide data to, and get data from, workflows running on Kubernetes/OpenShift.\n\nThe advantage of this is that you can have the bucket appear as a folder on your local computer, allowing you to easily place input files for workflows in it. Further workflows can be configured to copy their output files to this bucket so they will appear in this folder too - even if they executed in some remote cluster.\n\n## Creating a bucket\n\n<AnchorLinks small>\n  <AnchorLink>Login to IBM Cloud</AnchorLink>\n  <AnchorLink>Create a COS bucket</AnchorLink>\n</AnchorLinks>\n\n### Login to IBM Cloud\n\nVisit <https://cloud.ibm.com/>. There's a drop down menu at the top right corner where you can select your active account. Here we pick the option that reads `289600 - RIS`.\n\n![image.png](../assets/images/using-cloud-object-store/363df92f-bda6-4c0f-b0a0-bd463d4a2b5a.png)\n\nClick on the `Resource List` in the left hand menu:\n\n![image.png](../assets/images/using-cloud-object-store/49fa34a7-5be8-4106-a888-f910d963aafb.png)\n\nSelect `Storage` then click on the `Cloud Object Storage` service instance you want to host the bucket\n\n![image.png](../assets/images/using-cloud-object-store/0d861fd4-f80e-4289-8b9d-56a008528010.png)\n\nIf there is no `Cloud Object Storage` instance under `Storage` you will have to [create one](https://cloud.ibm.com/docs/cloud-object-storage?topic=cloud-object-storage-provision).\n\n### Create a COS bucket\n\nWhen you click on the cloud storage instance you should see a screen like the following: Click `Create bucket`\n\n![image-20210202081906903](../assets/images/using-cloud-object-store/image-20210202081906903.png)\n\nand then `Quickly get started`(click the arrow in the tile)\n\n![image-20210202081927265](../assets/images/using-cloud-object-store/image-20210202081927265.png)\n\nOn the following screen (see below) enter a name for your bucket. You will use the name this name in the next steps. Also make a note of the bucket region, which is listed in the panel on the right - in this case it is `eu-de`.\n\n![Screenshot 2021-02-04 at 21.17.08](../assets/images/using-cloud-object-store/Screenshot-bucket-create.png)\n\nClick the `Next`button, then you'll be greeted with a webpage like this:\n\n![image.png](../assets/images/using-cloud-object-store/07217461-3ba2-4cd8-a989-0e1dc724c91c.png)\n\nYour bucket is now created and you can add files through this page. The next sections details easier ways to interact with the bucket as a folder on your laptop and also how to use it with the `st4sd-runtime`.\n\n## Getting the bucket access details\n\n<AnchorLinks small>\n  <AnchorLink>Getting the S3 credentials</AnchorLink>\n  <AnchorLink>Get an endpoint</AnchorLink>\n</AnchorLinks>\n\nThe next step is to get the details that allow external programs to access the bucket\n\n### Getting the S3 credentials\n\nFrom the page shown in the last image click on the grey button `View bucket configuration` and then click on `Service credentials` on the left-most panel on the webpage. You will be offered a list of Service credentials to pick.\n\nThere is normally an entry `Service credentials-1`. Click on the dropdown button on its left and copy the values for the fields `cos_hmac_keys.access_key_id` and `cos_hmac_keys.secret_access_key`.\n\nIf you encounter either of the following\n\n- There is no `hmac` entry!\n- There is no `Service credentials-1`!\n\ngo to the next section, else continue to section `Get an endpoint`.\n\n#### Creating a credential\n\nThere are a couple of reasons `Service credentials-1` is not present, for example\n\n- Someone may have renamed it\n- The first person to create a bucket did not use a `smart tier storage class bucket` and forgot to toggle the `Include HMAC Credential Option`\n\nHowever, both are is easily corrected by creating a new credential\n\n1. Click `New Credential`\n2. Give it a name\n3. Chose `Writer` for `role`\n4. Click on `Advanced options`\n5. Toggle on that `Include HMAC Credential` option\n\n![image-20210202083358811](../assets/images/using-cloud-object-store/image-20210202083358811.png)\n\nNote that, even though it is best practice to periodically rotate the service credentials you do not need to create new credentials each time you execute a virtual experiment. We suggest that each person generates their own service credentials and periodically rotates them.\n\n### Get an endpoint\n\n**Note**: Do not use the `endpoint` URL that you see listed in your HMAC `Service credentials` (section above). Please follow the instructions below to locate your S3 endpoint.\n\nClick on the `Buckets` link in the left panel, find your bucket (use the search box if you like) and click on it.\n\nClick on `Details` button in top right\n\n![image-20210204164241892](../assets/images/using-cloud-object-store/image-20210204164241892.png)\n\nA side panel will appear on the right. Click the `View configuration` button. This will bring up a page whiich has a section called `Endpoints` e.g.\n\n![image.png](../assets/images/using-cloud-object-store/29996139-b0b1-406e-8c25-1fb010ee793b.png)\n\nThe `public` endpoint is usually the easiest to go with. However, if you are using a Cloud Object Storage instance that is located in the same IBM Cloud organization we suggest using the `private` endpoint as it is faster and cheaper.\n\n## Using the bucket with `st4sd-runtime`\n\n<AnchorLinks small>\n  <AnchorLink>S3 method</AnchorLink>\n  <AnchorLink>Datashim method</AnchorLink>\n</AnchorLinks>\n\nThere are two ways to use a COS bucket, directly (`s3`) and via [**Datashim**](https://datashim.io/). You can find more examples that you can use as a starting point for your notebooks in our [ST4SD example notebooks](https://github.com/st4sd/st4sd-examples/blob/main/notebooks/ST4SD%20Runtime%20API%20Example.ipynb).\n\nDatashim simplifies access to buckets allowing one person to create a `dataset` (a label for the bucket) and others to use it without having to know the access keys or endpoint. It also means the bucket can be moved and users of the bucket don't have to worry as long as the dataset is updated.\n\nWe recommend using Datashim in the long run for managing datasets with workflows. However using Datashim requires it to be installed in the target cluster - check this with the cluster admin.\n\n### S3 method\n\nTo use the bucket directly when submitting a workflow/virtual experiment from a notebook add the following to the submission configuration (see the [ST4SD Runtime API Example](https://github.com/st4sd/st4sd-examples/blob/main/notebooks/ST4SD%20Runtime%20API%20Example.ipynb) notebook for examples of submitting workflows)\n\n```json\n\n\"s3\": {\n  \"accessKeyID\": \"the  contents of `cos_hmac_keys.access_key_id`\",\n  \"secretAccessKey\": \"the contents of  `cos_hmac_keys.secret_access_key`\",\n  \"endpoint\":  \"your endpoint prefixed with https:// (e.g. https://s3.eu-de.cloud-object-storage.appdomain.cloud)\",\n  \"bucket\": \"the name of your bucket here\"\n},\n\n```\n\nYou can then specify files in the bucket as values to the `input` and `data` fields in the experiment configuration. For example, if you add a bucket and specify the following\n\n```json\n   \"data\": [{\n       \"filename\": \"get-smiles/input_smiles.csv\"\n   }]\n```\n\nThen this file, `input_smiles.csv`, will be retrieved from the folder `get-smiles/` in the bucket.\n\nYou may also store key-outputs to `S3`. For example, [`sum-numbers`](https://github.com/st4sd/sum-numbers/) has a `TotalSum` key-output. You could use the payload below to upload that output to an S3 bucket:\n\n```python\n\"s3Store\":{\n  \"credentials\": {\n    \"accessKeyID\": \"the  contents of `cos_hmac_keys.access_key_id`\",\n    \"secretAccessKey\": \"the contents of  `cos_hmac_keys.secret_access_key`\",\n    \"endpoint\": \"your endpoint prefixed with https:// (e.g. https://s3.eu-de.cloud-object-storage.appdomain.cloud)\",\n    \"bucket\": \"my-bucket\", # the name of your bucket\n  },\n  \"bucketPath\": \"/run1_output/\"\n}\n```\n\nWhen sum-numbers finishes, it will store the file `out.stdout` on the bucket `my-bucket` under the path `/run1_output/out.stdout`.\n\n### Datashim method\n\nThe Datashim method is similar to the previous except you provide the `s3` access informattion separately to creating a workflow and give it a name. You, and others, can then use this name to access the bucket instead of having to provide the keys etc.\n\nFor example if you wanted to create a dataset called `my-dataset` for a bucket, first create a dictionary containing (assuming you are in a iPython notebook)\n\n```python\npayload = {\n  \"access_key_id\": \"the  contents of `cos_hmac_keys.access_key_id`\",\n  \"secret_access_key\": \"the contents of  `cos_hmac_keys.secret_access_key`\",\n  \"endpoint\": \"your endpoint prefixed with https:// (e.g. https://s3.eu-de.cloud-object-storage.appdomain.cloud)\",\n  \"bucket\": \"my-bucket\", # the name of your bucket\n}\n# notice that the payload has snake-case fields (these are arguments to a python method)\n```\n\nThen execute\n\n```python\napi.api_dataset_create(dataset_name=\"my-dataset\", **payload)\n```\n\nYou can list the available datasets using\n\n```python\nr = api.api_datasets_list()\nprint(r)\n```\n\nWhen you submit the experiment you no longer need a full `s3` entry like the previous method. Instead, you use the dataset label\n\n```json\n\"s3\": {\n  \"dataset\": \"my-dataset\"\n},\n```\n\nYou may also store key-ouputs of your virtual experiment on buckets for which you have created a Dataset. For example:\n\n```python\n\"s3Store\": {\n  \"datasetStoreURI\": \"dataset://my-dataset/run1_output\"\n}\n```\n\nThe above snippet stores the workflow outputs into the bucket of Dataset `my-dataset` using `datasetStoreURI`. In this case it puts it into a sub-folder called `run1_output`. Notice that by using `s3Store.datasetStoreURI` you get the same behavior with specifying `s3Store.credentials` and `s3Store.bucketPath`.\n\n#### Validate that Dataset has been successfully created\n\nFor this step you need access to the OpenShift cluster that is hosting the associated ST4SD instance.\n\nTo check if the dataset `my-dataset` was properly created, `oc login` to the cluster and use `oc project` to switch to the project that holds the ST4SD instance. Execute:\n\n```\noc describe dataset my-dataset\n```\n\nThe above should print a description of the `my-dataset` object. Focus on the end of the printout, the part that contains the `Status` of the object. After the Dataet object has been successfully created it should look similar to:\n\n```python\n...\nStatus:\n  Caching:\n    Info:    No DLF caching plugins are installed\n    Status:  Disabled\n  Provision:\n    Status:  OK\n```\n\nIt normally takes about 30 seconds for a Dataset object to finish provisioning. The status will display warning messages if there is something wrong with the configuration of your Dataset. The most common mistake is using the wrong `endpoint`, `bucket`, or `credentials`. The next section discusses addressing such problems.\n\n#### I used wrong the bucket/credentials/endpoint to create my dataset, what do I do?\n\nFor this step you need access to the OpenShift cluster that is hosting the associated ST4SD instance.\n\nFirst, `oc login` to the cluster, use `oc project` to switch to the project that holds the ST4SD instance, and then delete the `Dataset` object with the invalid configuration. Then you should wait for a few seconds until the `PersistentVolumeClaim` with the same name as your dataset is garbage collected and then create the dataset again using the correct configuration.\n\nFor example, to delete the `my-dataset` Dataset in the `st4sd-production` namespace after logging in to OpenShift on your terminal, run\n\n```python\noc project st4sd-production\noc delete dataset my-dataset\n# wait for the PersistentVolumeClaim (PVC) `my-dataset` to be deleted, run the command below every few seconds\n# until it returns that there is no such object\noc get pvc my-dataset\n```\n\nAt this point you can go back to your python notebook, correct the `Dataset` payload and execute the code:\n\n```python\napi.api_dataset_create(dataset_name=\"my-dataset\", **payload)\n```\n\n## Accessing COS Buckets on your laptop\n\nYou can mount a COS bucket on your laptop so it appears as any other folder using a variety of tools. This means you can provide workflow input by copying into this folder, and also easily browse workflow output.\n\nHere will demonstrate using `s3fs` (https://github.com/s3fs-fuse/s3fs-fuse). The general command is:\n\n```\ns3fs  $BUCKETNAME -o url=$ENDPOINT $MOUNTPOINT -o passwd_file=$PASSWORDFIILE\n```\n\nHere's a concrete example for a bucket called `cloud-object-storage-drl-cos-standard-ecx-focintegration`\n\n```\ns3fs cloud-object-storage-drl-cos-standard-ecx-focintegration -o url=https://s3.eu-de.cloud-object-storage.appdomain.cloud /Users/michaelj/git-working/st4sd-examples/mvp2bucket/ -o passwd_file=${HOME}/.passwd-s3fs\n```\n\nTo create the password file do e.g.\n\n```\necho ACCESS_KEY_ID:SECRET_ACCESS_KEY > ${HOME}/.passwd-s3fs\nchmod 600 ${HOME}/.passwd-s3fs\n```\n\nWhere\n\n- `ACCESS_KEY_ID` - the contents of `cos_hmac_keys.access_key_id`\n- `SECRET_ACCESS_KEY ` - the contents of `cos_hmac_keys.secret_access_key`\n","fileAbsolutePath":"/Users/vassiliad/projects/st4sd/overview/src/pages/UsingCloudObjectStore.md"}}},"staticQueryHashes":["1364590287","137577622","2102389209","2456312558","2746626797","3018647132","3037994772","768070550"]}