{"componentChunkName":"component---src-pages-running-ve-via-terminal-mdx","path":"/running-ve-via-terminal/","result":{"pageContext":{"frontmatter":{"title":"Using the OpenShift CLI"},"relativePagePath":"/running-ve-via-terminal.mdx","titleType":"page","MdxNode":{"id":"e6022da1-dfd4-5420-b794-b200b7b13093","children":[],"parent":"2b168de1-c659-530d-a750-72352339757e","internal":{"content":"---\ntitle: Using the OpenShift CLI\n---\n\n<!--\n\n  Copyright IBM Inc. All Rights Reserved.\n  SPDX-License-Identifier: Apache-2.0\n\n-->\n\n<PageDescription>\n\nThis page shows how to use the OpenShift CLI to interact ST4SD. \n\n</PageDescription>\n\n<AnchorLinks>\n  <AnchorLink>Software Requirements</AnchorLink>\n  <AnchorLink>Getting Started</AnchorLink>\n  <AnchorLink>Running a Workflow</AnchorLink>\n  <AnchorLink>Names and Labels</AnchorLink>\n  <AnchorLink>Examining Virtual Experiments</AnchorLink>\n  <AnchorLink>Debugging</AnchorLink>\n  <AnchorLink>Switching from Notebook to Commandline</AnchorLink>\n  <AnchorLink>Managing Workflows</AnchorLink>\n</AnchorLinks>\n\niPython notebooks  make it easy to run and work with virtual experiments. However there is often a need to go deeper, especially for developers. In this case using the OpenShift CLI via a terminal is invaluable. \n\n<InlineNotification>\n\n- **For**: Researchers, Developers and Administrators\n- **Use-When:**\n    - Researchers: You want to go deeper into the outputs of a particular workflow using standard terminal tools\n    - Developers: Debugging virtual experiments. Inspecting and deleting workflows.\n    - Administrators: Inspecting and deleting workflows.\n- **Skill Requirements:**\n    - Being comfortable with working with command-line technologies\n    - Some knowledge of OpenShift/K8s CLI tools\n\n   \n\n</InlineNotification>\n\n\n\n## Software Requirements\n\nThe basic requirements are:\n* Access to an OpenShift instance with the ST4SD stack installed (see [first steps](/) for more information)\n* OpenShift command line tools installed\n\n## Getting Started\n\n### Terminology \n\nIn the following we use \n\n- `component` to refer to a task/step in a workflow. \n- `workflow` to mean a particular encoding of a set of steps\n- `workflow instance` to mean a particular execution of a `workflow`\n\n### Logging-In to the Cluster\n\nTo run from a terminal you first need to login to the OpenShift cluster you want to run on. You can use user/password e.g.\n\n```\noc login -u $USERNAME $CLUSTER\n```\n\nor copy the login command from the OpenShift console of the cluster you want to run in: \n\n1. Navigate to the console, login,\n2. Click on your name in upper right corner  \n3. Click `Copy Login Command`\n4. Paste the copied command into the terminal and run it\n\n## Running a Workflow\n\nRunning from a terminal involves two steps\n\n1. Writing a short file in `YAML` that describes the workflow you want to run  \n   - We call it ***`workflow.yaml`*** here but you can call it anything\n2. Executing `oc apply -f workflow.yaml`\n\n[Here](https://github.com/st4sd/st4sd-runtime-k8s/blob/main/examples/sum-numbers.yaml) is an example `yaml` for launching the `sum-numbers` toy workflow.  You can use this as a template for your writing your own workflow `yaml`. [This document](https://github.com/st4sd/st4sd-runtime-k8s/blob/main/docs/schema.md) gives a detailed description of all the available fields. \n\nYou can also try running the `sum-numbers` workflow via this `yaml` \n\n```\noc apply -f docs/examples/sum-numbers.yaml\n```\n\nThis currently assumes that various objects in the target ST4SD instance you want to run on have been named following the conventions outlined in the [ST4SD installation guide](/installation).  We hope to remove this requirement shortly.  \n\nIf the above works it will create a `workflow object` called `sn` - this is determined by the value of the `name` field in the workflow `yaml`. \n\n### I get st4sd.ibm.com/sn unchanged\n\nThis means someone already has run a workflow with the same name in the same instance of ST4SD. Execute\n\n```\noc delete workflow sn\n```\n\nto remove the old version and try the `apply` command again. \n\n<InlineNotification>\n\nThe <b>oc delete</b> command <b>does not affect the files</b> that the workflow instance has already generated under the persistent volume storage. However, it does trigger the workflow instance, and any components that are still running, to terminate. It also deletes any Kubernetes objects that have been created by the workflow instance. See <a href=\"#querying-a-workflow-kubernetes-object\">Querying a workflow kubernetes object</a> for more information on retrieving the status of a workflow via <b>oc</b>.\n\n</InlineNotification>\n\n## Names and Labels\n\nEvery kubernetes object has a name given by its `name` field. This name must be unique as it identifies a specific object. Here will refer to the name of a workflow as `WORKFLOW_NAME`\n\nA kubernetes object can also have arbitrary `labels` associated with it. These are given as key-values under the `metadata.labels` section in a workflow's `yaml`. For example in the `sum-numbers.yaml` file we have:\n\n```\nmetadata:\n  labels:\n    rest-uid: sn\n    workflow: sn\n  name: sn\n```\n\n`labels` are useful for **grouping** of objects as they allow you to list all objects with particular label key-value pair.\n\nAll labels defined in the workflows `yaml` are propagated to the pods the workflow creates. This allows us to find all pods created by a workflow instance, for example,  by asking for all pods who also have a label `workflow:sn`. In this document we call the value of the `workflow` field `WORKFLOW_ID`\n\nFrom the example `yaml` above `WORKFLOW_NAME` and `WORKFLOW_ID` can be set independently. This is why we differentiate them. However **by convention** we assume they will be the same, as this makes life much easier!\n\n## Examining Virtual Experiments\n\n<AnchorLinks small>\n  <AnchorLink>Querying a Workflow Kubernetes Object</AnchorLink>\n  <AnchorLink>Examining a Components Logs</AnchorLink>\n  <AnchorLink>Examining a Components Output Directory</AnchorLink>\n</AnchorLinks>\n\n### Querying a Workflow Kubernetes Object\n\nOnce you've started a workflow try the following commands\n\n* `oc get workflows`\n  *  Lists all workflows and displays their status (running, finished, failed, or an empty string for workflows which have not started running yet).\n* `oc get workflow $WORKFLOW_NAME -o yaml` \n  * Returns the `yaml` of the workflow along with current status etc.\n  * Example: `oc get workflow sn -o yaml`\n* `oc describe workflow $WORKFLOW_NAME` \n  * This gives a similar, but shorter, output to `get`\n  * **Importantly** it also returns the **EVENTS** associated with the created object - this is a primary way to [debug issues](#debugging-a-workflow-component).\n* `oc get pods -l workflow=$WORKFLOW_ID `\n  * Lists all the pods in the workflow\n  * Example: `oc get pods -l workflow=sn`\n  * The argument `-l` means `has this label key-value pair`\n\nWhen you list the pods of a workflow instance you will see one that is also called `WORKFLOW_NAME` - this is the primary pod of the instance. Pods for other workflow components that use the Kubernetes backend are called `flow-$STAGE-$COMPONENT_NAME-$UNIQUEID`\n\n![Pods of the sn workflow instance](../assets/images/running-ve-via-terminal/sn-pods-screenshot.png)\n\n\n### Examining a Components Logs\n\nGetting the logs of a task is straight-forward:\n\n* `oc logs POD_NAME` \n  * This outputs the logs of the container running in POD_NAME e.g. `oc logs flow-stage1-partialsum2-12290339-dc5vw`\n\n### Examining a Components Output Directory\n\nAll components in a workflow write their output under a directory tree on the target cluster which you can login to and browse around with a terminal.  The key piece of information you need here is the `INSTANCE_NAME` of the executing instance of the workflow, which is also the name of the root of the directory tree. To get this run: \n\n```\noc describe workflow sn | grep instanceName\n```\n\nThis will give output like:\n\n```\ninstanceName: sum-numbers-2021-02-09T133047.732342.instance\n```\n\n`sum-numbers-2021-02-09T133047.732342.instance` is the `INSTANCE_DIR_NAME`\n\nThen you can do:\n\n* `oc exec $WORKFLOW_NAME -c elaunch-primary /bin/bash`\n  *  Start a shell session in the primary pod of the workflow. From here you can `cd` to the directory at `$INSTANCE_DIR_NAME`. \n     * e.g you can run `cd /tmp/workdir/$INSTANCE_DIR_NAME`\n  * The output of a workflow component called `COMPONENT_NAME` in stage X will be at `stages/stageX/COMPONENT_NAME`\n  *  **This only works while the pod is executing** \n* `oc debug $WORKFLOW_NAME -c elaunch-primary` \n  *  Start a shell session in a copy of the primary pod of the workflow. From here you can browse to the workflow directory at `INSTANCE_DIR_NAME` like above\n  * e.g you can run `cd /tmp/workdir/$INSTANCE_DIR_NAME`\n  *  Tip: The default shell will be `sh`, however the primary pod also has `bash`\n  *  **Use this if the pod has finished**\n* `oc exec $WORKFLOW_NAME -c elaunch-primary -- einspect.py $INSTANCE_DIR_NAME`: \n  * Returns a detailed list of the state of the workflow components\n\n## Debugging\n\n<AnchorLinks small>\n  <AnchorLink>Debugging a Workflow Component</AnchorLink>\n  <AnchorLink>Debug Tools Available in the primary Pod</AnchorLink>\n</AnchorLinks>\n\n### Debugging a Workflow Component\n\nYour first step in checking if there is an issue is to run `oc describe` e.g. `oc describe workflow WORKFLOW_NAME` or `oc describe pod WORKFLOW_NAME`. This allows you to check for example:\n\n- If the pod was scheduled\n- If the pods image was pulled\n- Any issues with mounts\n- If the pod went out-of-memory\n\nIf you want to debug deeper you can use\n\n* `oc debug $PODNAME`\n  * This starts a shell in a copy of `$PODNAME`.  Use this if you want to debug the image the pod was using. \n* `oc debug $WORKFLOW_NAME -c elaunch-primary` \n  * Start a shell in a copy of the primary pod of the workflow (same environment, mounts etc.)\n  * As noted above you can browse to the workflow directory at `INSTANCE_NAME` however you also get access to three useful debug tools\n\n### Debug Tools Available in the primary Pod\n\nIf you start a shell in the primary pod, via `exec` or `debug` you can run the following tools. First `cd` to the workflow instance directory at `INSTANCE_NAME`\n\n- `ccommand.py`\n  - This prints the command line for a component. \n  - Useful to check what was actually executed is what you thought it was\n  - Example: `ccommand.py -s0 MyComponent`\n- `einspect.py`\n  - The prints the state of all the workflow components e.g. who failed, restarted etc. \n  - By default, it only prints components with issues\n  - Example: `einspect -f all`\n    - Prints status info on all components\n- `cexecute.py`\n  - Executes a component. In case of kubernetes use to reexecute any pod of any component. The same workdir will be used\n  - Example: `cexecute.py -f -s0 MyComponent`\n    - Note the `-f` causes a new pod to be spawned. If you don't specify it, ST4SD will try to execute `MyComponent` in the primary pod, which will usually fail due to necessary programs not being present.\n\n## Switching from Notebook to Commandline\n\nWhen you use the RESTApi via notebook to start a workflow it automates the creation of a workflow `yaml` as described above. Hence, you can still use the command line to work with it. \n\nHowever note the reverse is not fully true. If you launch from the terminal:\n\n* you cannot use the start/stop/status functions in the notebook - the workflow is not registered with the RESTApi\n* you *can* use the `st4sd-datastore` commands\n\nTo work with the command line all you need is to `REST UID` returned when you submitted the workflow from the notebook. This will be the `WORKFLOW_NAME` and the `WORKFLOW_ID` of the workflow instance.\n\n## Managing Workflows\n\n<AnchorLinks small>\n  <AnchorLink>List and Sort</AnchorLink>\n  <AnchorLink>Deleting</AnchorLink>\n</AnchorLinks>\n\nThe number of Workflows and associated Job and Pod objects in a namespace can easily reach O(1000) and higher. \nThis section describes how to examine, sort and delete these objects.\n\n### List and Sort\n\nBy default `oc get wf` only lists workflows name and experiment state in alphabetical order of Name. However this can easily be customized\n\n<InlineNotification>\nThis document assumes that there is only one kind of \"Workflow\" object on your cluster. If there are multiple CustomResourceDefinitions that introduce the Workflow object then you must use the full qualifier of the Workflow kind. Instead of \"oc get wf\" you should run \"oc get wf.st4sd.ibm.com\" (or the shorter form \"oc get wc.st4sd\").\n</InlineNotification>\n\n#### List workflows with custom-columns showing creation date, exit status and state\n\n```\noc get wf --output custom-columns=\"NAME:metadata.name,AGE:metadata.creationTimestamp,EXIT-STATUS:status.exitstatus,STATE:status.experimentstate\"\n```\n\nYou can create a column for any data in the workflow objects YAML using the appropriate key-path.\n\n#### Sort workflows by creation date\n\n```\noc --sort-by=\".metadata.creationTimestamp\" get wf\n```\n\nSimilarly you can sort workflow by any data in the workflow objects YAML using the appropriate key-path.\n\nCombining the above you can list workflows showing creation data, exist-status and state sorted by date\n\n```\noc --sort-by=\".metadata.creationTimestamp\" get wf --output custom-columns=\"NAME:metadata.name,AGE:metadata.creationTimestamp,EXIT-STATUS:status.exitstatus,STATE:status.experimentstate\"\n```\n\n### Deleting\n\nThe following commands illustrate how you can delete workflow objects\nThis also deletes all Job and Pod objects associated with the workflow\n\n<InlineNotification>\n\nDeleting a Workflow object does not delete the data created by the workflow.\nThe output folder will still exist and the output is still accessible via the st4sd-datastore python API.\n\n</InlineNotification>\n\n<InlineNotification kind=\"warning\">\n\nDeleting the workflow object will delete it from st4sd-runtime-service: st4sd-runtime-service API\ncalls using the deleted workflows rest-uid will not work.\n\n</InlineNotification>\n\n<InlineNotification>\n\nWhen deleting Workflows it's worth reviewing the meaning of experiment-state and exit-status [here](/running-workflows-on-openshift#getting-the-status-of-a-virtual-experiment-instance)\n\n</InlineNotification>\n\n\n#### Delete all workflows whose experiment-state is `finished`\n\n```\noc get wf | awk '/finished/{print $1}' | xargs oc delete wf\n```\n\n#### Delete all workflows whose experiment-state is `failed`\n\n```\noc get wf | awk '/failed/{print $1}' | xargs oc delete wf\n```\n\n#### Delete all workflows whose exist-status is failed\n\n```\noc get wf --output custom-columns=\"NAME:metadata.name,EXIT-STATUS:status.exitstatus\" | awk '/Failed/{print $1}' | xargs oc delete wf\n```\n\n#### Delete component Job and Pod objects associated with workflows whose experiment-state is `finished`\n\nThe oneliner below will not delete the Pod objects that contain the `elaunch-primary` container (i.e. the entrypoint Pod of the virtual experiment instance).\nThe code just deletes all Job and Pod objects that `elaunch-primary` created to run the tasks of components.\nAfter running it, you will still be able to run `oc debug $WORKFLOW_NAME`.\n\n```\noc get wf | awk '/finished/{print $1}' | xargs -n1 -I {} oc delete job -lworkflow={}\n```\n\n#### Delete all workflows whose name starts with `homolumo`\n\n```\noc get wf | awk '/homolumo/{print $1}' | xargs oc delete wf\n```\n\n<InlineNotification kind=\"info\">\nFuture versions of ST4SD will contain utilities to automate garbage collection of Kubernetes objects.\n</InlineNotification>\n","type":"Mdx","contentDigest":"8bbed5126d9da92947dff1d4190185ff","owner":"gatsby-plugin-mdx","counter":262},"frontmatter":{"title":"Using the OpenShift CLI"},"exports":{},"rawBody":"---\ntitle: Using the OpenShift CLI\n---\n\n<!--\n\n  Copyright IBM Inc. All Rights Reserved.\n  SPDX-License-Identifier: Apache-2.0\n\n-->\n\n<PageDescription>\n\nThis page shows how to use the OpenShift CLI to interact ST4SD. \n\n</PageDescription>\n\n<AnchorLinks>\n  <AnchorLink>Software Requirements</AnchorLink>\n  <AnchorLink>Getting Started</AnchorLink>\n  <AnchorLink>Running a Workflow</AnchorLink>\n  <AnchorLink>Names and Labels</AnchorLink>\n  <AnchorLink>Examining Virtual Experiments</AnchorLink>\n  <AnchorLink>Debugging</AnchorLink>\n  <AnchorLink>Switching from Notebook to Commandline</AnchorLink>\n  <AnchorLink>Managing Workflows</AnchorLink>\n</AnchorLinks>\n\niPython notebooks  make it easy to run and work with virtual experiments. However there is often a need to go deeper, especially for developers. In this case using the OpenShift CLI via a terminal is invaluable. \n\n<InlineNotification>\n\n- **For**: Researchers, Developers and Administrators\n- **Use-When:**\n    - Researchers: You want to go deeper into the outputs of a particular workflow using standard terminal tools\n    - Developers: Debugging virtual experiments. Inspecting and deleting workflows.\n    - Administrators: Inspecting and deleting workflows.\n- **Skill Requirements:**\n    - Being comfortable with working with command-line technologies\n    - Some knowledge of OpenShift/K8s CLI tools\n\n   \n\n</InlineNotification>\n\n\n\n## Software Requirements\n\nThe basic requirements are:\n* Access to an OpenShift instance with the ST4SD stack installed (see [first steps](/) for more information)\n* OpenShift command line tools installed\n\n## Getting Started\n\n### Terminology \n\nIn the following we use \n\n- `component` to refer to a task/step in a workflow. \n- `workflow` to mean a particular encoding of a set of steps\n- `workflow instance` to mean a particular execution of a `workflow`\n\n### Logging-In to the Cluster\n\nTo run from a terminal you first need to login to the OpenShift cluster you want to run on. You can use user/password e.g.\n\n```\noc login -u $USERNAME $CLUSTER\n```\n\nor copy the login command from the OpenShift console of the cluster you want to run in: \n\n1. Navigate to the console, login,\n2. Click on your name in upper right corner  \n3. Click `Copy Login Command`\n4. Paste the copied command into the terminal and run it\n\n## Running a Workflow\n\nRunning from a terminal involves two steps\n\n1. Writing a short file in `YAML` that describes the workflow you want to run  \n   - We call it ***`workflow.yaml`*** here but you can call it anything\n2. Executing `oc apply -f workflow.yaml`\n\n[Here](https://github.com/st4sd/st4sd-runtime-k8s/blob/main/examples/sum-numbers.yaml) is an example `yaml` for launching the `sum-numbers` toy workflow.  You can use this as a template for your writing your own workflow `yaml`. [This document](https://github.com/st4sd/st4sd-runtime-k8s/blob/main/docs/schema.md) gives a detailed description of all the available fields. \n\nYou can also try running the `sum-numbers` workflow via this `yaml` \n\n```\noc apply -f docs/examples/sum-numbers.yaml\n```\n\nThis currently assumes that various objects in the target ST4SD instance you want to run on have been named following the conventions outlined in the [ST4SD installation guide](/installation).  We hope to remove this requirement shortly.  \n\nIf the above works it will create a `workflow object` called `sn` - this is determined by the value of the `name` field in the workflow `yaml`. \n\n### I get st4sd.ibm.com/sn unchanged\n\nThis means someone already has run a workflow with the same name in the same instance of ST4SD. Execute\n\n```\noc delete workflow sn\n```\n\nto remove the old version and try the `apply` command again. \n\n<InlineNotification>\n\nThe <b>oc delete</b> command <b>does not affect the files</b> that the workflow instance has already generated under the persistent volume storage. However, it does trigger the workflow instance, and any components that are still running, to terminate. It also deletes any Kubernetes objects that have been created by the workflow instance. See <a href=\"#querying-a-workflow-kubernetes-object\">Querying a workflow kubernetes object</a> for more information on retrieving the status of a workflow via <b>oc</b>.\n\n</InlineNotification>\n\n## Names and Labels\n\nEvery kubernetes object has a name given by its `name` field. This name must be unique as it identifies a specific object. Here will refer to the name of a workflow as `WORKFLOW_NAME`\n\nA kubernetes object can also have arbitrary `labels` associated with it. These are given as key-values under the `metadata.labels` section in a workflow's `yaml`. For example in the `sum-numbers.yaml` file we have:\n\n```\nmetadata:\n  labels:\n    rest-uid: sn\n    workflow: sn\n  name: sn\n```\n\n`labels` are useful for **grouping** of objects as they allow you to list all objects with particular label key-value pair.\n\nAll labels defined in the workflows `yaml` are propagated to the pods the workflow creates. This allows us to find all pods created by a workflow instance, for example,  by asking for all pods who also have a label `workflow:sn`. In this document we call the value of the `workflow` field `WORKFLOW_ID`\n\nFrom the example `yaml` above `WORKFLOW_NAME` and `WORKFLOW_ID` can be set independently. This is why we differentiate them. However **by convention** we assume they will be the same, as this makes life much easier!\n\n## Examining Virtual Experiments\n\n<AnchorLinks small>\n  <AnchorLink>Querying a Workflow Kubernetes Object</AnchorLink>\n  <AnchorLink>Examining a Components Logs</AnchorLink>\n  <AnchorLink>Examining a Components Output Directory</AnchorLink>\n</AnchorLinks>\n\n### Querying a Workflow Kubernetes Object\n\nOnce you've started a workflow try the following commands\n\n* `oc get workflows`\n  *  Lists all workflows and displays their status (running, finished, failed, or an empty string for workflows which have not started running yet).\n* `oc get workflow $WORKFLOW_NAME -o yaml` \n  * Returns the `yaml` of the workflow along with current status etc.\n  * Example: `oc get workflow sn -o yaml`\n* `oc describe workflow $WORKFLOW_NAME` \n  * This gives a similar, but shorter, output to `get`\n  * **Importantly** it also returns the **EVENTS** associated with the created object - this is a primary way to [debug issues](#debugging-a-workflow-component).\n* `oc get pods -l workflow=$WORKFLOW_ID `\n  * Lists all the pods in the workflow\n  * Example: `oc get pods -l workflow=sn`\n  * The argument `-l` means `has this label key-value pair`\n\nWhen you list the pods of a workflow instance you will see one that is also called `WORKFLOW_NAME` - this is the primary pod of the instance. Pods for other workflow components that use the Kubernetes backend are called `flow-$STAGE-$COMPONENT_NAME-$UNIQUEID`\n\n![Pods of the sn workflow instance](../assets/images/running-ve-via-terminal/sn-pods-screenshot.png)\n\n\n### Examining a Components Logs\n\nGetting the logs of a task is straight-forward:\n\n* `oc logs POD_NAME` \n  * This outputs the logs of the container running in POD_NAME e.g. `oc logs flow-stage1-partialsum2-12290339-dc5vw`\n\n### Examining a Components Output Directory\n\nAll components in a workflow write their output under a directory tree on the target cluster which you can login to and browse around with a terminal.  The key piece of information you need here is the `INSTANCE_NAME` of the executing instance of the workflow, which is also the name of the root of the directory tree. To get this run: \n\n```\noc describe workflow sn | grep instanceName\n```\n\nThis will give output like:\n\n```\ninstanceName: sum-numbers-2021-02-09T133047.732342.instance\n```\n\n`sum-numbers-2021-02-09T133047.732342.instance` is the `INSTANCE_DIR_NAME`\n\nThen you can do:\n\n* `oc exec $WORKFLOW_NAME -c elaunch-primary /bin/bash`\n  *  Start a shell session in the primary pod of the workflow. From here you can `cd` to the directory at `$INSTANCE_DIR_NAME`. \n     * e.g you can run `cd /tmp/workdir/$INSTANCE_DIR_NAME`\n  * The output of a workflow component called `COMPONENT_NAME` in stage X will be at `stages/stageX/COMPONENT_NAME`\n  *  **This only works while the pod is executing** \n* `oc debug $WORKFLOW_NAME -c elaunch-primary` \n  *  Start a shell session in a copy of the primary pod of the workflow. From here you can browse to the workflow directory at `INSTANCE_DIR_NAME` like above\n  * e.g you can run `cd /tmp/workdir/$INSTANCE_DIR_NAME`\n  *  Tip: The default shell will be `sh`, however the primary pod also has `bash`\n  *  **Use this if the pod has finished**\n* `oc exec $WORKFLOW_NAME -c elaunch-primary -- einspect.py $INSTANCE_DIR_NAME`: \n  * Returns a detailed list of the state of the workflow components\n\n## Debugging\n\n<AnchorLinks small>\n  <AnchorLink>Debugging a Workflow Component</AnchorLink>\n  <AnchorLink>Debug Tools Available in the primary Pod</AnchorLink>\n</AnchorLinks>\n\n### Debugging a Workflow Component\n\nYour first step in checking if there is an issue is to run `oc describe` e.g. `oc describe workflow WORKFLOW_NAME` or `oc describe pod WORKFLOW_NAME`. This allows you to check for example:\n\n- If the pod was scheduled\n- If the pods image was pulled\n- Any issues with mounts\n- If the pod went out-of-memory\n\nIf you want to debug deeper you can use\n\n* `oc debug $PODNAME`\n  * This starts a shell in a copy of `$PODNAME`.  Use this if you want to debug the image the pod was using. \n* `oc debug $WORKFLOW_NAME -c elaunch-primary` \n  * Start a shell in a copy of the primary pod of the workflow (same environment, mounts etc.)\n  * As noted above you can browse to the workflow directory at `INSTANCE_NAME` however you also get access to three useful debug tools\n\n### Debug Tools Available in the primary Pod\n\nIf you start a shell in the primary pod, via `exec` or `debug` you can run the following tools. First `cd` to the workflow instance directory at `INSTANCE_NAME`\n\n- `ccommand.py`\n  - This prints the command line for a component. \n  - Useful to check what was actually executed is what you thought it was\n  - Example: `ccommand.py -s0 MyComponent`\n- `einspect.py`\n  - The prints the state of all the workflow components e.g. who failed, restarted etc. \n  - By default, it only prints components with issues\n  - Example: `einspect -f all`\n    - Prints status info on all components\n- `cexecute.py`\n  - Executes a component. In case of kubernetes use to reexecute any pod of any component. The same workdir will be used\n  - Example: `cexecute.py -f -s0 MyComponent`\n    - Note the `-f` causes a new pod to be spawned. If you don't specify it, ST4SD will try to execute `MyComponent` in the primary pod, which will usually fail due to necessary programs not being present.\n\n## Switching from Notebook to Commandline\n\nWhen you use the RESTApi via notebook to start a workflow it automates the creation of a workflow `yaml` as described above. Hence, you can still use the command line to work with it. \n\nHowever note the reverse is not fully true. If you launch from the terminal:\n\n* you cannot use the start/stop/status functions in the notebook - the workflow is not registered with the RESTApi\n* you *can* use the `st4sd-datastore` commands\n\nTo work with the command line all you need is to `REST UID` returned when you submitted the workflow from the notebook. This will be the `WORKFLOW_NAME` and the `WORKFLOW_ID` of the workflow instance.\n\n## Managing Workflows\n\n<AnchorLinks small>\n  <AnchorLink>List and Sort</AnchorLink>\n  <AnchorLink>Deleting</AnchorLink>\n</AnchorLinks>\n\nThe number of Workflows and associated Job and Pod objects in a namespace can easily reach O(1000) and higher. \nThis section describes how to examine, sort and delete these objects.\n\n### List and Sort\n\nBy default `oc get wf` only lists workflows name and experiment state in alphabetical order of Name. However this can easily be customized\n\n<InlineNotification>\nThis document assumes that there is only one kind of \"Workflow\" object on your cluster. If there are multiple CustomResourceDefinitions that introduce the Workflow object then you must use the full qualifier of the Workflow kind. Instead of \"oc get wf\" you should run \"oc get wf.st4sd.ibm.com\" (or the shorter form \"oc get wc.st4sd\").\n</InlineNotification>\n\n#### List workflows with custom-columns showing creation date, exit status and state\n\n```\noc get wf --output custom-columns=\"NAME:metadata.name,AGE:metadata.creationTimestamp,EXIT-STATUS:status.exitstatus,STATE:status.experimentstate\"\n```\n\nYou can create a column for any data in the workflow objects YAML using the appropriate key-path.\n\n#### Sort workflows by creation date\n\n```\noc --sort-by=\".metadata.creationTimestamp\" get wf\n```\n\nSimilarly you can sort workflow by any data in the workflow objects YAML using the appropriate key-path.\n\nCombining the above you can list workflows showing creation data, exist-status and state sorted by date\n\n```\noc --sort-by=\".metadata.creationTimestamp\" get wf --output custom-columns=\"NAME:metadata.name,AGE:metadata.creationTimestamp,EXIT-STATUS:status.exitstatus,STATE:status.experimentstate\"\n```\n\n### Deleting\n\nThe following commands illustrate how you can delete workflow objects\nThis also deletes all Job and Pod objects associated with the workflow\n\n<InlineNotification>\n\nDeleting a Workflow object does not delete the data created by the workflow.\nThe output folder will still exist and the output is still accessible via the st4sd-datastore python API.\n\n</InlineNotification>\n\n<InlineNotification kind=\"warning\">\n\nDeleting the workflow object will delete it from st4sd-runtime-service: st4sd-runtime-service API\ncalls using the deleted workflows rest-uid will not work.\n\n</InlineNotification>\n\n<InlineNotification>\n\nWhen deleting Workflows it's worth reviewing the meaning of experiment-state and exit-status [here](/running-workflows-on-openshift#getting-the-status-of-a-virtual-experiment-instance)\n\n</InlineNotification>\n\n\n#### Delete all workflows whose experiment-state is `finished`\n\n```\noc get wf | awk '/finished/{print $1}' | xargs oc delete wf\n```\n\n#### Delete all workflows whose experiment-state is `failed`\n\n```\noc get wf | awk '/failed/{print $1}' | xargs oc delete wf\n```\n\n#### Delete all workflows whose exist-status is failed\n\n```\noc get wf --output custom-columns=\"NAME:metadata.name,EXIT-STATUS:status.exitstatus\" | awk '/Failed/{print $1}' | xargs oc delete wf\n```\n\n#### Delete component Job and Pod objects associated with workflows whose experiment-state is `finished`\n\nThe oneliner below will not delete the Pod objects that contain the `elaunch-primary` container (i.e. the entrypoint Pod of the virtual experiment instance).\nThe code just deletes all Job and Pod objects that `elaunch-primary` created to run the tasks of components.\nAfter running it, you will still be able to run `oc debug $WORKFLOW_NAME`.\n\n```\noc get wf | awk '/finished/{print $1}' | xargs -n1 -I {} oc delete job -lworkflow={}\n```\n\n#### Delete all workflows whose name starts with `homolumo`\n\n```\noc get wf | awk '/homolumo/{print $1}' | xargs oc delete wf\n```\n\n<InlineNotification kind=\"info\">\nFuture versions of ST4SD will contain utilities to automate garbage collection of Kubernetes objects.\n</InlineNotification>\n","fileAbsolutePath":"/home/travis/build/st4sd/overview/src/pages/running-ve-via-terminal.mdx"}}},"staticQueryHashes":["1364590287","137577622","2102389209","2456312558","2746626797","3018647132","3037994772","768070550"]}