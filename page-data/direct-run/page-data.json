{"componentChunkName":"component---src-pages-direct-run-mdx","path":"/direct-run/","result":{"pageContext":{"frontmatter":{"title":"The elaunch.py command line tool"},"relativePagePath":"/direct-run.mdx","titleType":"page","MdxNode":{"id":"83a810a0-74ca-5f78-854d-5eb9f141ab72","children":[],"parent":"ae465a95-0524-5bba-9857-8ffc2c229dfd","internal":{"content":"---\ntitle: The elaunch.py command line tool\n---\n\nimport { CarbonForIbmDotcom } from \"@carbon/pictograms-react\";\nimport { ArtTools_01 } from \"@carbon/pictograms-react\";\n\n<!--\n\n  Copyright IBM Inc. All Rights Reserved.\n  SPDX-License-Identifier: Apache-2.0\n\n-->\n\n<PageDescription>\n\nThe `elaunch.py` command line tool executes and monitors virtual experiments. You can use it to run experiments on your laptop or on a High Performance Computing Cluster. When you submit a virtual experiment via ST4SD API it is executed by `elaunch.py`.\n\n</PageDescription>\n\n\n<AnchorLinks>\n\n  <AnchorLink>Running experiments with elaunch.py</AnchorLink>\n  <AnchorLink>Checking if your experiment worked</AnchorLink>\n  <AnchorLink>What is the output of my experiment ?</AnchorLink>\n  <AnchorLink>What is the status of my experiment ?</AnchorLink>\n  <AnchorLink>Troubleshooting</AnchorLink>\n  <AnchorLink>How do I select an execution platform ?</AnchorLink>\n  <AnchorLink>How to restart an experiment ?</AnchorLink>\n</AnchorLinks>\n\n\n## Install elaunch.py\n\nIf you haven't already, install the `st4sd-runtime-core` python package:\n\n```\npip install \"st4sd-runtime-core[develop]\"\n```\n\n## Running experiments with elaunch.py\n\nWith elaunch.py you can run experiments - sets of files describing computational workflows - given their path: simply run `elaunch.py <path to experiment>`.\n\nFor example, you can run the workflow [`nanopore-geometry-experiment`](https://github.com/st4sd/nanopore-geometry-experiment) like so:\n\n```bash\n: # Get the directory that provides input to the next command\ngit clone https://github.com/st4sd/nanopore-geometry-experiment.git\n\ncd nanopore-geometry-experiment\n\n: # Run elaunch.py specifying certain files in the directory created above\nelaunch.py --nostamp -l 40 --input docker-example/cif_files.dat \\\n      --applicationDependencySource=\"nanopore-database=cif:copy\" \\\n      nanopore-geometry-experiment.package\n```\n\nThe experiment should take about 5 minutes to complete. The `-l 40` option keeps the log printouts to the bare minimum so don't worry if the command is silent for a few minutes. When the experiment completes expect to see something similar to the following text on your terminal:\n\n```\ncompleted-on=2024-03-15 14:39:29.909105\ncost=0\ncreated-on=2024-03-15 14:38:04.402969\ncurrent-stage=stage3\nexit-status=Success\nexperiment-state=finished\nstage-progress=1.0\nstage-state=finished\nstages=['stage0', 'stage1', 'stage2', 'stage3']\ntotal-progress=1.0\nupdated=2024-03-15 14:39:33.804727\nupdated-on=2024-03-15 14:39:33.804727\n```\n\nRunning an experiment creates a directory which contains the outputs. In this example we set the argument `--nostamp` which instructs ST4SD to not include a timestamp in the directory it creates for the experiment. As such it creates the directory `nanopore-geometry-experiment.instance`. If you run the same command a second time elaunch.py will complain that the experiment instance already exists. Either remove the `--nostamp` argument or delete the directory and retry `elaunch.py`. See Section [What is the output of my experiment?](#what-is-the-output-of-my-experiment) for more information.\n\n### Experiment project types\n\nExperiments can be packaged in two different ways. One way is the `standalone` project which is the example we show above. This type of experiments only support a single virtual experiment and are best suited for workflows with many artifacts or resources that are actively changing (i.e., they have multiple commits).\n\nStandalone projects contain:\n\n* a `conf` directory with the experiment definition files\n* (optional) a `data` directory with data files that the workflow steps can reference and the users may override at execution time\n* (optional) additional custom directories that the workflow developers include for the workflow steps to reference\n\nAnother type is the `standard` project. These are flexible, allowing for multiple virtual experiment definitions to be bundled together and share files, like scripts and restart hooks. They consist of\n\n* a YAML file that contains the experiment definition\n* (optional) manifest YAML file listing the directories that the virtual experiment needs and where they will be accessible from when it is running.\n\n### Providing input files\n\nExperiments typically require inputs to function properly. To view them, you can use the command `einputs.py <path to experiment>`. Refer to the documentation of the experiment you're trying to run to find out more about the necessary inputs. \n\nTo pass inputs to your experiment, you can use the `-i ${path to input file}` option in `elaunch.py`. In the above example we provide the input file `cif_files.dat` which is located in the directory `docker-example`.\n\nIf you want to use an input file whose name **is not the same** as the one the experiment expects, you must map them explicitly with `--input $local_path:$input_name`. For example, to use the contents of the file `/tmp/my-file.dat` as the input file `cif_files.dat` above you would specify `--input /tmp/my-file.dat:cif_files.dat`.\n\n### Setting configuration options\n\nExperiments may also come with configuration options that you can optionally override. We call these options `variables` and you can use `einputs.py` to get the list of variables (and their default) values for an experiment.\n\nFor example, here is the relevant section from the output of `einputs.py nanopore-geometry-experiment.package`:\n\n\n```yaml\noptional:\n  variables:\n    global:\n      numberOfNanopores: 1\n      probeRadius_A: 1.4\n      zeo_memory: 2Gi\n```\n\nTypically the experiment documentation explains what these variables control. To configure their values, put together a `variables` file using the format:\n\n```yaml\nglobal:\n  parameterName: value\n```\n\nUse this variables file with your experiment by specifying the `elaunch.py` argument `-a ${path to variables file}`. Take care when formatting the `variables.yaml` file, it should follow the indentation and syntax of YAML files.\n\n## Checking if your experiment worked\n\nIf the experiment works `elaunch.py` prints `exit-status=Success` before it terminates and then exits with return code 0. You can find more information about the status of your experiment under the file `${package_name}-${timestamp}.instance/output/status.txt`. For example, here's a `status.txt` for a successful run of an experiment:\n\n```\ncompleted-on=2024-03-15 14:39:29.909105\ncost=0\ncreated-on=2024-03-15 14:38:04.402969\ncurrent-stage=stage3\nexit-status=Success\nexperiment-state=finished\nstage-progress=1.0\nstage-state=finished\nstages=['stage0', 'stage1', 'stage2', 'stage3']\ntotal-progress=1.0\nupdated=2024-03-15 14:39:33.804727\nupdated-on=2024-03-15 14:39:33.804727\n```\n\nIf the experiment fails you will see the line `exit-status=Failed` in the logs of `elaunch.py` and it will exit with a return code other than `0`. If the experiment failed after the instance directory was created you will see this information in the `output/status.txt` file too. Common reasons for failures are invalid syntax, missing input files, or requesting a compute resource that is not available. For more information and dealing with these errors see our [Troubleshooting](#troubleshooting) section.\n\n## What is the output of my experiment ?\n\nAll outputs of the experiment are placed in the experiment instance directory. By default, this directory is`${package-name}-${timestamp}.instance` and you will find it under the directory you were in when you ran `elaunch.py`. If you specify the `--nostamp` argument then elaunch.py will not omit the `-${timestamp}` part.\n\nThe experiment instance directory contains several nested directories, of which the most noteworthy are `output` and `stages`. Here is the full list of directories and their description:\n\n* `stages`: contains one directory per stage of your experiment. Each stage directory contains one directory for each of the working directories of the components in that stage. Components store any files their produce, as well as text they print to the terminal under their working directory\n* `output`: contains the runtime logs and files with metadata about the outputs and status of your experiment\n* `inputs`: contains the input files you provided, including any variable files\n* `data`: (optional) contains files that the workflow definition bundles and the workflow steps can reference. Users may optionally override those files when they launch an experiment\n* `conf`: contains the experiment definition\n\n### The output directory\n\nIt contains the following files:\n\n* experiment.log: the logs of the `elaunch.py` process\n* status.txt: the final status of the experiment (see the status printout above for an example)\n* status_details.json: Similar to above but easier to consume programmatically\n* output.txt: contains metadata about key files that your experiments produce i.e. key outputs. This file gets updated when when the key named files that one of your tasks produced. It contains information such as their path relative to the root of the instance directory, modification time, etc.\n* output.json: Similar to above but easier to consume programmatically\n* properties.csv: (optional) If your experiment defines its [interface](/using-a-virtual-experiment-interface), then this file contains the measured properties of your experiment,\n* input-ids.json: (optional) If your experiment defines its [interface](/using-a-virtual-experiment-interface), then this file contains an array with the input ids that your experiment processed\n* additional_input_data.json: (optional) If your experiment defines its interface, then this file contains dictionary whose keys are input ids and values are additional input data (e.g. absolute paths) associated with the corresponding input id\n\n### The stages directory\n\nA virtual experiment is a computational workflow that executes tasks. Task outputs are organized under the `stages` directory like so: `stages/stage{$index}/${task-name}`. To find out the tasks that are in your experiment read the  experiment definition or look at the file structure of the `stages` directory.\n\nComponents specify which stage they belong to and by default they are all part of stage `0`. Generally, stages help you create logical groups of components. They do not really play a role in scheduling decisions, except for some special cases which are outside the scope of the information in this document\n\n### Understanding an experiment's execution requirements\n\nThe experiment documentation should explain what is required to execute it. For example, an experiment contains a set of tasks and elaunch.py submits those tasks to the backends that the tasks select. This means that if the machine on which you run elaunch.py does not support the backend that a task selects then elaunch.py cannot run that task.\n\n### How to run elaunch with LSF ?\n\nSome experiments can launch tasks on using the batch scheduler LSF (IBM Spectrum). If an experiment supports execution on LSF it should say so in its documentation and explain how to launch using it.\n\n<!--NOTE maybe something you can run that tells you which backend tasks need (and whether they're available???)-->\n\nTo launch an experiment that supports LSF you need to also install the official [`lsf-python-api`](https://github.com/IBMSpectrumComputing/lsf-python-api) python module:\n\n```bash\n. /path/to/profile.lsf\ngit clone https://github.com/IBMSpectrumComputing/lsf-python-api.git\ncd lsf-python-api\npython3 setup.py build\npython3 setup.py install\n```\n\nCheck the homepage of [`lsf-python-api`](https://github.com/IBMSpectrumComputing/lsf-python-api) for more information.\n\n### How to override experiment configuration data files\n\nExperiments may optionally bundle data files which you may override. The experiment documentation should explain what these files are and what your options are for overriding. Additionally `einputs.py` displays the names of the data files that an experiment references.\n\n\n### Store outputs to S3\n\nExperiments may optionally upload their [`key-outputs`](/tutorial#key-outputs) to S3 after termination. You can instruct `elaunch.py` to upload these files to S3 using the `--s3StoreToURI` parameter. When using this parameter, you must also specify exactly one of the parameters `--s3AuthWithEnvVars` or `--s3AuthBearer64`.\n\n### Example:\n\n\n```bash\nexport bucket=\"a-bucket\"\nexport path_in_bucket=\"optional/path\"\n\nexport S3_ACCESS_KEY_ID=\"s3 access key id\"\nexport S3_SECRET_ACCESS_KEY=\"s3 secret access key\"\nexport S3_END_POINT=\"s3 end point\"\n\nelaunch.py --s3StoreToURI s3://${bucket}/${path_in_bucket} \\\n  --s3AuthWithEnvVars path/to/experiment\n```\n\nWhen `--s3StoreToURI` is set, after the experiment terminates, `elaunch.py` will start uploading the `key-outputs` to the S3 bucket you provided under the specified `${path_in_bucket}`. `elaunch.py` replaces occurrences of the `%(instanceDir)s` literal in `--s3StoreToURI` with the name of the experiment instance. For example, you can use this to store the `key-outputs` of multiple workflow instances in the same bucket.\n\nAlternatively, you can base64-encode the JSON representation of the dictionary `{\"S3_ACCESS_KEY_ID\": \"val\", \"S3_SECRET_ACCESS_KEY\": \"val\", \"S3_END_POINT\": \"val\"}` and use the `--s3AuthBearer64` parameter instead:\n\n```bash\nexport bucket=\"a-bucket\"\nexport path_in_bucket=\"optional/path\"\nexport json=\"{\\\"S3_ACCESS_KEY_ID\\\": \\\"val\\\", \\\"S3_SECRET_ACCESS_KEY\\\": \\\"val\\\", \\\"S3_END_POINT\\\": \\\"val\\\"}\"\nexport s3_auth=`echo \"${json}\" | base64`\n\nelaunch.py --s3StoreToURI s3://${bucket}/${path_in_bucket} \\\n  --s3AuthBearer64 path/to/experiment\n```\n\n\n## What is the status of my experiment ?\n\nThe `elaunch.py` script will periodically store information about the status of your experiment instance under its `$instanceDir` directory. You can use `einspect.py` to see the current status of tasks in your experiment instance.\n\nHere is an example output of running `einspect.py` after a [`sum-numbers`](https://github.com/st4sd/sum-numbers) experiment terminates.\n\n\n```\ncd sum-numbers-2024-03-15T143804.402969.instance\neinspect.py -f all\n\n\nWARNING   MainThread                     root                          : <module>             2024-03-15 14:39:50,782: No instance given - checking if inside one\n\n========== STAGE 0 ==========\n\nComponents using engine-type: engine\nreference, state, backend, isWaitingOnOutput, engineExitReason, lastTaskRunTime, lastTaskRunState\nstage0.GenerateInput, finished, local, True, Success, 0:00:00.241827, finished\n\n========== STAGE 1 ==========\n\nComponents using engine-type: engine\nreference, state, backend, isWaitingOnOutput, engineExitReason, lastTaskRunTime, lastTaskRunState\nstage1.ExtractRow0, finished, local, True, Success, 0:01:00.197737, finished\nstage1.ExtractRow1, finished, local, True, Success, 0:01:00.227356, finished\nstage1.ExtractRow2, finished, local, True, Success, 0:01:00.208578, finished\nstage1.PartialSum0, finished, local, True, Success, 0:00:00.368524, finished\nstage1.PartialSum1, finished, local, True, Success, 0:00:00.385971, finished\nstage1.PartialSum2, finished, local, True, Success, 0:00:00.410082, finished\n\n========== STAGE 2 ==========\n\nComponents using engine-type: engine\nreference, state, backend, isWaitingOnOutput, engineExitReason, lastTaskRunTime, lastTaskRunState\nstage2.Sum, finished, local, True, Success, 0:00:00.061634, finished\n\n========== STAGE 3 ==========\n\nComponents using engine-type: engine\nreference, state, backend, isWaitingOnOutput, engineExitReason, lastTaskRunTime, lastTaskRunState\nstage3.Cat, finished, local, True, Success, 0:00:00.013691, finished\n```\n\nYou may also see a summary of your status in the `$instanceDir/output/status.txt` file:\n\n```\ncompleted-on=2024-03-15 14:39:29.909105\ncost=0\ncreated-on=2024-03-15 14:38:04.402969\ncurrent-stage=stage3\nexit-status=Success\nexperiment-state=finished\nstage-progress=1.0\nstage-state=finished\nstages=['stage0', 'stage1', 'stage2', 'stage3']\ntotal-progress=1.0\nupdated=2024-03-15 14:39:33.804727\nupdated-on=2024-03-15 14:39:33.804727\n```\n\nThe current status of your experiment is the value of `exit-status`.\n\n## Troubleshooting\n\nIf the `exit-status` of your experiment instance is `Failed` then this means that at least one of your components was unable to terminate successfully. You can find the name of the component that caused the experiment to fail in the `status` file and printout.\n\nHere is an example:\n\n```\ncompleted-on=2024-05-23 09:44:03.757679\ncost=0\ncreated-on=2024-05-23 09:42:19.223491\ncurrent-stage=stage1\nerror-description=Stage 1 failed. Reason:\\\\\\\\n3 jobs failed unexpectedly.\\\\\\\\nJob: stage1.PartialSum0. Returncode 1. Reason KnownIssue\\\\\\\\nJob: stage1.PartialSum2. Returncode 1. Reason KnownIssue\\\\\\\\nJob: stage1.PartialSum1. Returncode 1. Reason KnownIssue\\\\\\\\n\nexit-status=Failed\nexperiment-state=finished\nstage-progress=0.5\nstage-state=failed\nstages=['stage0', 'stage1', 'stage2', 'stage3']\ntotal-progress=0.875\nupdated=2024-05-23 09:44:08.793316\nupdated-on=2024-05-23 09:44:08.793316\n```\n\nThe error reports that multiple components failed: `stage1.PartialSum0`, `stage1.PartialSum1`, `stage1.PartialSum2`.\n\nYou may also get a full view of the state of the experiment by using the `einspect.py -f all` tool.\n\n```\n========== STAGE 0 ==========\n\nComponents using engine-type: engine\nreference, state, backend, isWaitingOnOutput, engineExitReason, lastTaskRunTime, lastTaskRunState\nstage0.GenerateInput, finished, local, True, Success, 0:00:00.358677, finished\n\n========== STAGE 1 ==========\n\nComponents using engine-type: engine\nreference, state, backend, isWaitingOnOutput, engineExitReason, lastTaskRunTime, lastTaskRunState\nstage1.ExtractRow0, finished, local, True, Success, 0:01:00.246955, finished\nstage1.ExtractRow1, finished, local, True, Success, 0:01:00.209115, finished\nstage1.ExtractRow2, finished, local, True, Success, 0:01:00.226946, finished\nstage1.PartialSum0, failed, local, True, KnownIssue, 0:00:00.323739, failed\nstage1.PartialSum1, failed, local, True, KnownIssue, 0:00:00.336899, failed\nstage1.PartialSum2, failed, local, True, KnownIssue, 0:00:00.360914, failed\n\n========== STAGE 2 ==========\n\nComponents using engine-type: engine\nreference, state, backend, isWaitingOnOutput, engineExitReason, lastTaskRunTime, lastTaskRunState\nstage2.Sum, component_shutdown, local, False, Killed, N/A, N/A\n\n========== STAGE 3 ==========\n\nComponents using engine-type: engine\nreference, state, backend, isWaitingOnOutput, engineExitReason, lastTaskRunTime, lastTaskRunState\nstage3.Cat, finished, local, True, Success, 0:00:00.011466, finished\n```\n\nAfter you spot a `Failed` component, try looking at the files it produced, including its stdout and stderr (for some backends both streams get fed into stdout). Recall that you can find these files under `$INSTANCE_DIR/stages/stage<stage index>/<component name>/`. Look for the `out.stdout` and `out.stderr` files.\n\nSometimes, a component fails because one of its predecessors (direct, or indirect) produced unexpected output. To find the predecessors of a component, look at the `$INSTANCE_DIR/conf/flowir_instance.yaml`, locate the component you are investigating and then follow its predecessors by looking at the `references` of the component. You can then investigate the output files and stdout/stderr of those components to see if you can spot why the downstream component failed.\n\n## How do I select an execution platform ?\n\nOften, workflows have support for multiple execution environments such as Cloud (e.g. Kubernetes/OpenShift), HPC, or even personal devices like laptops.\nST4SD uses the concept of execution platform to help workflow developers define how their workflows should execute under different execution environments.\nPlatforms are designed to assist in implementing generic components which are specialized for different purposes when specifying different platforms. This is particularly useful when working with packages that can utilize various kinds of HPC resources (e.g. a cluster fitted with LSF, a kubernetes installation, etc). For example, a component can be configured to utilize a certain amount of GPUs when it targets platform A but exclusively use CPUs on platform B. You can find more information about platforms [in our docs](/workflow-specification#platforms).\n\nUse `einputs.py <path to package>` to find a list of the platforms that an experiment supports. The experiment documentation should explain the requirements for executing the experiment with any of the platforms. To select a platform use the `--platform` commandline argument of `elaunch.py`. If you don't provide the commandline parameter `--platform` then `elaunch.py` will select the platform called `default` which is the default platform of experiments.\n\n## How to restart an experiment ?\n\nSometimes it's useful to restart a previously completed experiment instance instead of starting a brand new instance. For example, you can modify a script that a component in the instance used and then restart all components starting from a specific stage index and onwards.\n\nTo restart an existing instance from a given stage index use `elaunch.py --restart <stageindex> ... path/to/dir.instance`. All components from stage `<stageindex>` and onwards will be restarted. This means that `elaunch.py` will run the logic of their `restart hook` and may re-run a component depending on the output of the restart method. Use the `--noRestartHooks` option with `--restart` to skip the restart hook logic and re-run the components. Find out more information about `restart` hooks in [our docs](/restart).\n\n\n\n## Learn more\n\n<ExpressiveList\n    title=\"Write experiments\"\n    background=\"true\"\n    pictogram={<FileBackup />}>\n\nGet an introduction to [writing virtual experiment](/write-experiments) with ST4SD Core.\n\n  </ExpressiveList>\n\n<div className=\"expressive-content-list-group\">\n\n  <ExpressiveList\n    title=\"Exploring the Registry UI\"\n    background=\"true\"\n    pictogram={<CarbonForIbmDotcom />}>\n\nLearn about all the features of\n[our web interface](/using-the-virtual-experiments-registry-ui) for browsing and\nexamining virtual experiments packages and runs. You can visit the\n[ST4SD Global Registry](https://registry.st4sd.res.ibm.com/) for a first look.\n\n  </ExpressiveList>\n\n  <ExpressiveList\n    title=\"No Code, No Fuss creation of Experiments\"\n    background=\"true\"\n    pictogram={<ArtTools_01 />}>\n\nUse\n[an interactive Build Canvas and a Graph Library](/build-experiments-registry-ui)\nto create and modify experiments straight from your browser.\n\n  </ExpressiveList>\n\n</div>\n","type":"Mdx","contentDigest":"0d92f52165ba50826032a7462aca224c","owner":"gatsby-plugin-mdx","counter":255},"frontmatter":{"title":"The elaunch.py command line tool"},"exports":{},"rawBody":"---\ntitle: The elaunch.py command line tool\n---\n\nimport { CarbonForIbmDotcom } from \"@carbon/pictograms-react\";\nimport { ArtTools_01 } from \"@carbon/pictograms-react\";\n\n<!--\n\n  Copyright IBM Inc. All Rights Reserved.\n  SPDX-License-Identifier: Apache-2.0\n\n-->\n\n<PageDescription>\n\nThe `elaunch.py` command line tool executes and monitors virtual experiments. You can use it to run experiments on your laptop or on a High Performance Computing Cluster. When you submit a virtual experiment via ST4SD API it is executed by `elaunch.py`.\n\n</PageDescription>\n\n\n<AnchorLinks>\n\n  <AnchorLink>Running experiments with elaunch.py</AnchorLink>\n  <AnchorLink>Checking if your experiment worked</AnchorLink>\n  <AnchorLink>What is the output of my experiment ?</AnchorLink>\n  <AnchorLink>What is the status of my experiment ?</AnchorLink>\n  <AnchorLink>Troubleshooting</AnchorLink>\n  <AnchorLink>How do I select an execution platform ?</AnchorLink>\n  <AnchorLink>How to restart an experiment ?</AnchorLink>\n</AnchorLinks>\n\n\n## Install elaunch.py\n\nIf you haven't already, install the `st4sd-runtime-core` python package:\n\n```\npip install \"st4sd-runtime-core[develop]\"\n```\n\n## Running experiments with elaunch.py\n\nWith elaunch.py you can run experiments - sets of files describing computational workflows - given their path: simply run `elaunch.py <path to experiment>`.\n\nFor example, you can run the workflow [`nanopore-geometry-experiment`](https://github.com/st4sd/nanopore-geometry-experiment) like so:\n\n```bash\n: # Get the directory that provides input to the next command\ngit clone https://github.com/st4sd/nanopore-geometry-experiment.git\n\ncd nanopore-geometry-experiment\n\n: # Run elaunch.py specifying certain files in the directory created above\nelaunch.py --nostamp -l 40 --input docker-example/cif_files.dat \\\n      --applicationDependencySource=\"nanopore-database=cif:copy\" \\\n      nanopore-geometry-experiment.package\n```\n\nThe experiment should take about 5 minutes to complete. The `-l 40` option keeps the log printouts to the bare minimum so don't worry if the command is silent for a few minutes. When the experiment completes expect to see something similar to the following text on your terminal:\n\n```\ncompleted-on=2024-03-15 14:39:29.909105\ncost=0\ncreated-on=2024-03-15 14:38:04.402969\ncurrent-stage=stage3\nexit-status=Success\nexperiment-state=finished\nstage-progress=1.0\nstage-state=finished\nstages=['stage0', 'stage1', 'stage2', 'stage3']\ntotal-progress=1.0\nupdated=2024-03-15 14:39:33.804727\nupdated-on=2024-03-15 14:39:33.804727\n```\n\nRunning an experiment creates a directory which contains the outputs. In this example we set the argument `--nostamp` which instructs ST4SD to not include a timestamp in the directory it creates for the experiment. As such it creates the directory `nanopore-geometry-experiment.instance`. If you run the same command a second time elaunch.py will complain that the experiment instance already exists. Either remove the `--nostamp` argument or delete the directory and retry `elaunch.py`. See Section [What is the output of my experiment?](#what-is-the-output-of-my-experiment) for more information.\n\n### Experiment project types\n\nExperiments can be packaged in two different ways. One way is the `standalone` project which is the example we show above. This type of experiments only support a single virtual experiment and are best suited for workflows with many artifacts or resources that are actively changing (i.e., they have multiple commits).\n\nStandalone projects contain:\n\n* a `conf` directory with the experiment definition files\n* (optional) a `data` directory with data files that the workflow steps can reference and the users may override at execution time\n* (optional) additional custom directories that the workflow developers include for the workflow steps to reference\n\nAnother type is the `standard` project. These are flexible, allowing for multiple virtual experiment definitions to be bundled together and share files, like scripts and restart hooks. They consist of\n\n* a YAML file that contains the experiment definition\n* (optional) manifest YAML file listing the directories that the virtual experiment needs and where they will be accessible from when it is running.\n\n### Providing input files\n\nExperiments typically require inputs to function properly. To view them, you can use the command `einputs.py <path to experiment>`. Refer to the documentation of the experiment you're trying to run to find out more about the necessary inputs. \n\nTo pass inputs to your experiment, you can use the `-i ${path to input file}` option in `elaunch.py`. In the above example we provide the input file `cif_files.dat` which is located in the directory `docker-example`.\n\nIf you want to use an input file whose name **is not the same** as the one the experiment expects, you must map them explicitly with `--input $local_path:$input_name`. For example, to use the contents of the file `/tmp/my-file.dat` as the input file `cif_files.dat` above you would specify `--input /tmp/my-file.dat:cif_files.dat`.\n\n### Setting configuration options\n\nExperiments may also come with configuration options that you can optionally override. We call these options `variables` and you can use `einputs.py` to get the list of variables (and their default) values for an experiment.\n\nFor example, here is the relevant section from the output of `einputs.py nanopore-geometry-experiment.package`:\n\n\n```yaml\noptional:\n  variables:\n    global:\n      numberOfNanopores: 1\n      probeRadius_A: 1.4\n      zeo_memory: 2Gi\n```\n\nTypically the experiment documentation explains what these variables control. To configure their values, put together a `variables` file using the format:\n\n```yaml\nglobal:\n  parameterName: value\n```\n\nUse this variables file with your experiment by specifying the `elaunch.py` argument `-a ${path to variables file}`. Take care when formatting the `variables.yaml` file, it should follow the indentation and syntax of YAML files.\n\n## Checking if your experiment worked\n\nIf the experiment works `elaunch.py` prints `exit-status=Success` before it terminates and then exits with return code 0. You can find more information about the status of your experiment under the file `${package_name}-${timestamp}.instance/output/status.txt`. For example, here's a `status.txt` for a successful run of an experiment:\n\n```\ncompleted-on=2024-03-15 14:39:29.909105\ncost=0\ncreated-on=2024-03-15 14:38:04.402969\ncurrent-stage=stage3\nexit-status=Success\nexperiment-state=finished\nstage-progress=1.0\nstage-state=finished\nstages=['stage0', 'stage1', 'stage2', 'stage3']\ntotal-progress=1.0\nupdated=2024-03-15 14:39:33.804727\nupdated-on=2024-03-15 14:39:33.804727\n```\n\nIf the experiment fails you will see the line `exit-status=Failed` in the logs of `elaunch.py` and it will exit with a return code other than `0`. If the experiment failed after the instance directory was created you will see this information in the `output/status.txt` file too. Common reasons for failures are invalid syntax, missing input files, or requesting a compute resource that is not available. For more information and dealing with these errors see our [Troubleshooting](#troubleshooting) section.\n\n## What is the output of my experiment ?\n\nAll outputs of the experiment are placed in the experiment instance directory. By default, this directory is`${package-name}-${timestamp}.instance` and you will find it under the directory you were in when you ran `elaunch.py`. If you specify the `--nostamp` argument then elaunch.py will not omit the `-${timestamp}` part.\n\nThe experiment instance directory contains several nested directories, of which the most noteworthy are `output` and `stages`. Here is the full list of directories and their description:\n\n* `stages`: contains one directory per stage of your experiment. Each stage directory contains one directory for each of the working directories of the components in that stage. Components store any files their produce, as well as text they print to the terminal under their working directory\n* `output`: contains the runtime logs and files with metadata about the outputs and status of your experiment\n* `inputs`: contains the input files you provided, including any variable files\n* `data`: (optional) contains files that the workflow definition bundles and the workflow steps can reference. Users may optionally override those files when they launch an experiment\n* `conf`: contains the experiment definition\n\n### The output directory\n\nIt contains the following files:\n\n* experiment.log: the logs of the `elaunch.py` process\n* status.txt: the final status of the experiment (see the status printout above for an example)\n* status_details.json: Similar to above but easier to consume programmatically\n* output.txt: contains metadata about key files that your experiments produce i.e. key outputs. This file gets updated when when the key named files that one of your tasks produced. It contains information such as their path relative to the root of the instance directory, modification time, etc.\n* output.json: Similar to above but easier to consume programmatically\n* properties.csv: (optional) If your experiment defines its [interface](/using-a-virtual-experiment-interface), then this file contains the measured properties of your experiment,\n* input-ids.json: (optional) If your experiment defines its [interface](/using-a-virtual-experiment-interface), then this file contains an array with the input ids that your experiment processed\n* additional_input_data.json: (optional) If your experiment defines its interface, then this file contains dictionary whose keys are input ids and values are additional input data (e.g. absolute paths) associated with the corresponding input id\n\n### The stages directory\n\nA virtual experiment is a computational workflow that executes tasks. Task outputs are organized under the `stages` directory like so: `stages/stage{$index}/${task-name}`. To find out the tasks that are in your experiment read the  experiment definition or look at the file structure of the `stages` directory.\n\nComponents specify which stage they belong to and by default they are all part of stage `0`. Generally, stages help you create logical groups of components. They do not really play a role in scheduling decisions, except for some special cases which are outside the scope of the information in this document\n\n### Understanding an experiment's execution requirements\n\nThe experiment documentation should explain what is required to execute it. For example, an experiment contains a set of tasks and elaunch.py submits those tasks to the backends that the tasks select. This means that if the machine on which you run elaunch.py does not support the backend that a task selects then elaunch.py cannot run that task.\n\n### How to run elaunch with LSF ?\n\nSome experiments can launch tasks on using the batch scheduler LSF (IBM Spectrum). If an experiment supports execution on LSF it should say so in its documentation and explain how to launch using it.\n\n<!--NOTE maybe something you can run that tells you which backend tasks need (and whether they're available???)-->\n\nTo launch an experiment that supports LSF you need to also install the official [`lsf-python-api`](https://github.com/IBMSpectrumComputing/lsf-python-api) python module:\n\n```bash\n. /path/to/profile.lsf\ngit clone https://github.com/IBMSpectrumComputing/lsf-python-api.git\ncd lsf-python-api\npython3 setup.py build\npython3 setup.py install\n```\n\nCheck the homepage of [`lsf-python-api`](https://github.com/IBMSpectrumComputing/lsf-python-api) for more information.\n\n### How to override experiment configuration data files\n\nExperiments may optionally bundle data files which you may override. The experiment documentation should explain what these files are and what your options are for overriding. Additionally `einputs.py` displays the names of the data files that an experiment references.\n\n\n### Store outputs to S3\n\nExperiments may optionally upload their [`key-outputs`](/tutorial#key-outputs) to S3 after termination. You can instruct `elaunch.py` to upload these files to S3 using the `--s3StoreToURI` parameter. When using this parameter, you must also specify exactly one of the parameters `--s3AuthWithEnvVars` or `--s3AuthBearer64`.\n\n### Example:\n\n\n```bash\nexport bucket=\"a-bucket\"\nexport path_in_bucket=\"optional/path\"\n\nexport S3_ACCESS_KEY_ID=\"s3 access key id\"\nexport S3_SECRET_ACCESS_KEY=\"s3 secret access key\"\nexport S3_END_POINT=\"s3 end point\"\n\nelaunch.py --s3StoreToURI s3://${bucket}/${path_in_bucket} \\\n  --s3AuthWithEnvVars path/to/experiment\n```\n\nWhen `--s3StoreToURI` is set, after the experiment terminates, `elaunch.py` will start uploading the `key-outputs` to the S3 bucket you provided under the specified `${path_in_bucket}`. `elaunch.py` replaces occurrences of the `%(instanceDir)s` literal in `--s3StoreToURI` with the name of the experiment instance. For example, you can use this to store the `key-outputs` of multiple workflow instances in the same bucket.\n\nAlternatively, you can base64-encode the JSON representation of the dictionary `{\"S3_ACCESS_KEY_ID\": \"val\", \"S3_SECRET_ACCESS_KEY\": \"val\", \"S3_END_POINT\": \"val\"}` and use the `--s3AuthBearer64` parameter instead:\n\n```bash\nexport bucket=\"a-bucket\"\nexport path_in_bucket=\"optional/path\"\nexport json=\"{\\\"S3_ACCESS_KEY_ID\\\": \\\"val\\\", \\\"S3_SECRET_ACCESS_KEY\\\": \\\"val\\\", \\\"S3_END_POINT\\\": \\\"val\\\"}\"\nexport s3_auth=`echo \"${json}\" | base64`\n\nelaunch.py --s3StoreToURI s3://${bucket}/${path_in_bucket} \\\n  --s3AuthBearer64 path/to/experiment\n```\n\n\n## What is the status of my experiment ?\n\nThe `elaunch.py` script will periodically store information about the status of your experiment instance under its `$instanceDir` directory. You can use `einspect.py` to see the current status of tasks in your experiment instance.\n\nHere is an example output of running `einspect.py` after a [`sum-numbers`](https://github.com/st4sd/sum-numbers) experiment terminates.\n\n\n```\ncd sum-numbers-2024-03-15T143804.402969.instance\neinspect.py -f all\n\n\nWARNING   MainThread                     root                          : <module>             2024-03-15 14:39:50,782: No instance given - checking if inside one\n\n========== STAGE 0 ==========\n\nComponents using engine-type: engine\nreference, state, backend, isWaitingOnOutput, engineExitReason, lastTaskRunTime, lastTaskRunState\nstage0.GenerateInput, finished, local, True, Success, 0:00:00.241827, finished\n\n========== STAGE 1 ==========\n\nComponents using engine-type: engine\nreference, state, backend, isWaitingOnOutput, engineExitReason, lastTaskRunTime, lastTaskRunState\nstage1.ExtractRow0, finished, local, True, Success, 0:01:00.197737, finished\nstage1.ExtractRow1, finished, local, True, Success, 0:01:00.227356, finished\nstage1.ExtractRow2, finished, local, True, Success, 0:01:00.208578, finished\nstage1.PartialSum0, finished, local, True, Success, 0:00:00.368524, finished\nstage1.PartialSum1, finished, local, True, Success, 0:00:00.385971, finished\nstage1.PartialSum2, finished, local, True, Success, 0:00:00.410082, finished\n\n========== STAGE 2 ==========\n\nComponents using engine-type: engine\nreference, state, backend, isWaitingOnOutput, engineExitReason, lastTaskRunTime, lastTaskRunState\nstage2.Sum, finished, local, True, Success, 0:00:00.061634, finished\n\n========== STAGE 3 ==========\n\nComponents using engine-type: engine\nreference, state, backend, isWaitingOnOutput, engineExitReason, lastTaskRunTime, lastTaskRunState\nstage3.Cat, finished, local, True, Success, 0:00:00.013691, finished\n```\n\nYou may also see a summary of your status in the `$instanceDir/output/status.txt` file:\n\n```\ncompleted-on=2024-03-15 14:39:29.909105\ncost=0\ncreated-on=2024-03-15 14:38:04.402969\ncurrent-stage=stage3\nexit-status=Success\nexperiment-state=finished\nstage-progress=1.0\nstage-state=finished\nstages=['stage0', 'stage1', 'stage2', 'stage3']\ntotal-progress=1.0\nupdated=2024-03-15 14:39:33.804727\nupdated-on=2024-03-15 14:39:33.804727\n```\n\nThe current status of your experiment is the value of `exit-status`.\n\n## Troubleshooting\n\nIf the `exit-status` of your experiment instance is `Failed` then this means that at least one of your components was unable to terminate successfully. You can find the name of the component that caused the experiment to fail in the `status` file and printout.\n\nHere is an example:\n\n```\ncompleted-on=2024-05-23 09:44:03.757679\ncost=0\ncreated-on=2024-05-23 09:42:19.223491\ncurrent-stage=stage1\nerror-description=Stage 1 failed. Reason:\\\\\\\\n3 jobs failed unexpectedly.\\\\\\\\nJob: stage1.PartialSum0. Returncode 1. Reason KnownIssue\\\\\\\\nJob: stage1.PartialSum2. Returncode 1. Reason KnownIssue\\\\\\\\nJob: stage1.PartialSum1. Returncode 1. Reason KnownIssue\\\\\\\\n\nexit-status=Failed\nexperiment-state=finished\nstage-progress=0.5\nstage-state=failed\nstages=['stage0', 'stage1', 'stage2', 'stage3']\ntotal-progress=0.875\nupdated=2024-05-23 09:44:08.793316\nupdated-on=2024-05-23 09:44:08.793316\n```\n\nThe error reports that multiple components failed: `stage1.PartialSum0`, `stage1.PartialSum1`, `stage1.PartialSum2`.\n\nYou may also get a full view of the state of the experiment by using the `einspect.py -f all` tool.\n\n```\n========== STAGE 0 ==========\n\nComponents using engine-type: engine\nreference, state, backend, isWaitingOnOutput, engineExitReason, lastTaskRunTime, lastTaskRunState\nstage0.GenerateInput, finished, local, True, Success, 0:00:00.358677, finished\n\n========== STAGE 1 ==========\n\nComponents using engine-type: engine\nreference, state, backend, isWaitingOnOutput, engineExitReason, lastTaskRunTime, lastTaskRunState\nstage1.ExtractRow0, finished, local, True, Success, 0:01:00.246955, finished\nstage1.ExtractRow1, finished, local, True, Success, 0:01:00.209115, finished\nstage1.ExtractRow2, finished, local, True, Success, 0:01:00.226946, finished\nstage1.PartialSum0, failed, local, True, KnownIssue, 0:00:00.323739, failed\nstage1.PartialSum1, failed, local, True, KnownIssue, 0:00:00.336899, failed\nstage1.PartialSum2, failed, local, True, KnownIssue, 0:00:00.360914, failed\n\n========== STAGE 2 ==========\n\nComponents using engine-type: engine\nreference, state, backend, isWaitingOnOutput, engineExitReason, lastTaskRunTime, lastTaskRunState\nstage2.Sum, component_shutdown, local, False, Killed, N/A, N/A\n\n========== STAGE 3 ==========\n\nComponents using engine-type: engine\nreference, state, backend, isWaitingOnOutput, engineExitReason, lastTaskRunTime, lastTaskRunState\nstage3.Cat, finished, local, True, Success, 0:00:00.011466, finished\n```\n\nAfter you spot a `Failed` component, try looking at the files it produced, including its stdout and stderr (for some backends both streams get fed into stdout). Recall that you can find these files under `$INSTANCE_DIR/stages/stage<stage index>/<component name>/`. Look for the `out.stdout` and `out.stderr` files.\n\nSometimes, a component fails because one of its predecessors (direct, or indirect) produced unexpected output. To find the predecessors of a component, look at the `$INSTANCE_DIR/conf/flowir_instance.yaml`, locate the component you are investigating and then follow its predecessors by looking at the `references` of the component. You can then investigate the output files and stdout/stderr of those components to see if you can spot why the downstream component failed.\n\n## How do I select an execution platform ?\n\nOften, workflows have support for multiple execution environments such as Cloud (e.g. Kubernetes/OpenShift), HPC, or even personal devices like laptops.\nST4SD uses the concept of execution platform to help workflow developers define how their workflows should execute under different execution environments.\nPlatforms are designed to assist in implementing generic components which are specialized for different purposes when specifying different platforms. This is particularly useful when working with packages that can utilize various kinds of HPC resources (e.g. a cluster fitted with LSF, a kubernetes installation, etc). For example, a component can be configured to utilize a certain amount of GPUs when it targets platform A but exclusively use CPUs on platform B. You can find more information about platforms [in our docs](/workflow-specification#platforms).\n\nUse `einputs.py <path to package>` to find a list of the platforms that an experiment supports. The experiment documentation should explain the requirements for executing the experiment with any of the platforms. To select a platform use the `--platform` commandline argument of `elaunch.py`. If you don't provide the commandline parameter `--platform` then `elaunch.py` will select the platform called `default` which is the default platform of experiments.\n\n## How to restart an experiment ?\n\nSometimes it's useful to restart a previously completed experiment instance instead of starting a brand new instance. For example, you can modify a script that a component in the instance used and then restart all components starting from a specific stage index and onwards.\n\nTo restart an existing instance from a given stage index use `elaunch.py --restart <stageindex> ... path/to/dir.instance`. All components from stage `<stageindex>` and onwards will be restarted. This means that `elaunch.py` will run the logic of their `restart hook` and may re-run a component depending on the output of the restart method. Use the `--noRestartHooks` option with `--restart` to skip the restart hook logic and re-run the components. Find out more information about `restart` hooks in [our docs](/restart).\n\n\n\n## Learn more\n\n<ExpressiveList\n    title=\"Write experiments\"\n    background=\"true\"\n    pictogram={<FileBackup />}>\n\nGet an introduction to [writing virtual experiment](/write-experiments) with ST4SD Core.\n\n  </ExpressiveList>\n\n<div className=\"expressive-content-list-group\">\n\n  <ExpressiveList\n    title=\"Exploring the Registry UI\"\n    background=\"true\"\n    pictogram={<CarbonForIbmDotcom />}>\n\nLearn about all the features of\n[our web interface](/using-the-virtual-experiments-registry-ui) for browsing and\nexamining virtual experiments packages and runs. You can visit the\n[ST4SD Global Registry](https://registry.st4sd.res.ibm.com/) for a first look.\n\n  </ExpressiveList>\n\n  <ExpressiveList\n    title=\"No Code, No Fuss creation of Experiments\"\n    background=\"true\"\n    pictogram={<ArtTools_01 />}>\n\nUse\n[an interactive Build Canvas and a Graph Library](/build-experiments-registry-ui)\nto create and modify experiments straight from your browser.\n\n  </ExpressiveList>\n\n</div>\n","fileAbsolutePath":"/home/travis/build/st4sd/overview/src/pages/direct-run.mdx"}}},"staticQueryHashes":["1364590287","137577622","2102389209","2456312558","2746626797","3018647132","3037994772","768070550"]}