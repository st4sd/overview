{"componentChunkName":"component---src-pages-direct-run-mdx","path":"/direct-run/","result":{"pageContext":{"frontmatter":{"title":"Directly running workflows"},"relativePagePath":"/direct-run.mdx","titleType":"page","MdxNode":{"id":"0aba6d26-675d-51c0-8255-3fdad0e256ec","children":[],"parent":"3de33297-e999-54f6-86e2-54c931356f35","internal":{"content":"---\ntitle: Directly running workflows\n---\n\n<!--\n\n  Copyright IBM Inc. All Rights Reserved.\n  SPDX-License-Identifier: Apache-2.0\n\n-->\n\n<PageDescription>\n\nThis page will teach you how to run a workflow directly using the `elaunch.py` command line utility.\nUsers comfortable with installing python modules and the [FlowIR](/workflow-specification) should be able to follow this content.\n\n</PageDescription>\n\n<AnchorLinks>\n  <AnchorLink>Prepare a virtual environment</AnchorLink>\n  <AnchorLink>Execute a workflow</AnchorLink>\n  <AnchorLink>Provide input files and override data files</AnchorLink>\n  <AnchorLink>Store outputs to S3</AnchorLink>\n</AnchorLinks>\n\n\n## Prepare a virtual environment\n\nWe recommend using a virtual environment with a modern version of python 3 (3.7+) to install [**ST4SD Core**](/st4sd-core-getting-started) like so:\n\n\n```bash\npython3 -m venv --copies st4sd\n. ./st4sd/bin/activate\npip install \"st4sd-runtime-core[develop]\"\n```\n\nIf you are installing ST4SD on a machine that can submit tasks to [IBM Spectrum LSF](https://www.ibm.com/products/hpc-workload-management)\nthen you should also install the official [`lsf-python-api`](https://github.com/IBMSpectrumComputing/lsf-python-api) python module.\n\n```\n. /path/to/profile.lsf\ngit clone https://github.com/IBMSpectrumComputing/lsf-python-api.git\ncd lsf-python-api\npython3 setup.py build\npython3 setup.py install\n```\n\nCheck the homepage of [`lsf-python-api`](https://github.com/IBMSpectrumComputing/lsf-python-api) for more information.\n\nAfter installing the `lsf-python-api` python module you can launch workflows which contain components that use the [`lsf`](/workflow-specification#description-of-basic-flowir-component-fields) backend.\n\n## Execute a workflow\n\nUse the `elaunch.py` command-line utility that is included by installing `st4sd-runtime-core` to run your workflows.\nFor example, you can run the toy workflow [`sum-numbers`](https://github.com/st4sd/sum-numbers) like so:\n\n```bash\ngit clone https://github.com/st4sd/sum-numbers.git\nelaunch.py --nostamp -l40 sum-numbers\n```\n\n## Provide input files and override data files\n\nST4SD workflows support 3 flavours of inputs:\n\n1. Input files - files user must provide when they execute the workflow\n2. Data files - configuration files that optionally can be overridden\n3. User variables - user provided values for workflow variables\n\nThe [tutorial](/tutorial#providing-inputs-to-workflows) contains more information about inputs.\n\n### Example\n\nHere's an example of a workflow that uses an `input` file, a `data` file, and a variable.\n\nFirst, prepare the workflow definition files by running the following on your terminal:\n\n```bash\ncat <<EOF >workflow.yaml\nvariables:\n  default:\n    global:\n      var: hello\n\ncomponents:\n- name: hello\n  command:\n    executable: sh\n    arguments: |\n      <<EOF\n      echo variable contains \"%(var)s\"\n      echo input contents are \"input/foo.txt:output\"\n      echo data contents are \"data/bar.txt:output\"\n      EOF\n    expandArguments: none\n  references:\n  - input/foo.txt:output\n  - data/bar.txt:output\nEOF\n\ncat <<EOF >manifest.yaml\ndata: data\nEOF\n\nmkdir -p shared_data\ncat <<EOF >shared_data/bar.txt\ndata-file-contents\nEOF\n\ncat <<EOF >foo.txt\ninput-file-contents\nEOF\n\ncat <<EOF >my_vars.yaml\nglobal:\n  var: hi\nEOF\n```\n\nThe above script creates the following file structure:\n\n```\nworkflow.yaml  # the workflow definition\nmanifest.yaml  # manifest that maps \"shared_data\" to \"data\"\nfoo.txt        # the input file\nmy_vars.yaml   # file containing user variables\nshared_data    # the directory containing \"data\" files\n└─ bar.txt\n```\n\nActivate the virtual environment that you used to install `st4sd-runtime-core` and then run:\n\n```bash\nelaunch.py -l40 --nostamp  \\\n      --failSafeDelays=no \\\n      --input foo.txt \\\n      --variables my_vars.yaml \\\n      --manifest manifest.yaml workflow.yaml\necho \"\\n\\nComponent stdout was:\"\ncat workflow.instance/stages/stage0/hello/out.stdout\n```\n\nIf you omit the `--variables` parameter then the `var` variable will receive the value that the `default` platform sets to it.\n\nYou can override the contents of the `data` file `bar.txt` by adding the argument: `--data path/to/a/different/bar.txt`.\nFinally, you can use the `--data` and `--input` parameters multiple times.\n\n## Store outputs to S3\n\nWorkflows may optionally define [`key-outputs`](/tutorial#key-outputs) which which `elaunch.py`\nmay upload to S3 after the experiment terminates.\n\nYou can instruct `elaunch.py` to upload `key-outputs` to S3 via the the `--s3StoreToURI` parameter.\nWhen setting the parameter `--s3StoreToURI` you must also use exactly one of the parameters `--s3AuthWithEnvVars` or `--s3AuthBearer64`.\n\n### Example:\n\n\n```bash\nexport bucket=\"a-bucket\"\nexport path_in_bucket=\"optional/path\"\n\nexport S3_ACCESS_KEY_ID=\"s3 access key id\"\nexport S3_SECRET_ACCESS_KEY=\"s3 secret access key\"\nexport S3_END_POINT=\"s3 end point\"\n\nelaunch.py --s3StoreToURI s3://${bucket}/${path_in_bucket} \\\n  --s3AuthWithEnvVars \\\n  path/to/workflow\n```\n\nWhen `--s3StoreToURI` is set, after the experiment terminates, `elaunch.py` will start uploading the `key-outputs` to the S3 bucket you provided under the specifeid `${path_in_bucket}`.\n`elaunch.py` replaces occurences of the `%(instanceDir)s` literal in `--s3StoreToURI` with the name of the experiment instance.\nFor example, you can use this to store the `key-outputs` of multiple workflow instances in the same bucket.\n\nAlternatively, you can base64-encode the JSON representation of the dictionary `{\"S3_ACCESS_KEY_ID\": \"val\", \"S3_SECRET_ACCESS_KEY\": \"val\", \"S3_END_POINT\": \"val\"}` and use the `--s3AuthBearer64` parameter instead:\n\n```bash\nexport bucket=\"a-bucket\"\nexport path_in_bucket=\"optional/path\"\nexport json=\"{\\\"S3_ACCESS_KEY_ID\\\": \\\"val\\\", \\\"S3_SECRET_ACCESS_KEY\\\": \\\"val\\\", \\\"S3_END_POINT\\\": \\\"val\\\"}\"\nexport s3_auth=`echo \"${json}\" | base64`\n\nelaunch.py --s3StoreToURI s3://${bucket}/${path_in_bucket} \\\n  --s3AuthBearer64 \\\n  path/to/workflow\n```","type":"Mdx","contentDigest":"575a345b425ba5a8f3e224b3b71d86d0","owner":"gatsby-plugin-mdx","counter":253},"frontmatter":{"title":"Directly running workflows"},"exports":{},"rawBody":"---\ntitle: Directly running workflows\n---\n\n<!--\n\n  Copyright IBM Inc. All Rights Reserved.\n  SPDX-License-Identifier: Apache-2.0\n\n-->\n\n<PageDescription>\n\nThis page will teach you how to run a workflow directly using the `elaunch.py` command line utility.\nUsers comfortable with installing python modules and the [FlowIR](/workflow-specification) should be able to follow this content.\n\n</PageDescription>\n\n<AnchorLinks>\n  <AnchorLink>Prepare a virtual environment</AnchorLink>\n  <AnchorLink>Execute a workflow</AnchorLink>\n  <AnchorLink>Provide input files and override data files</AnchorLink>\n  <AnchorLink>Store outputs to S3</AnchorLink>\n</AnchorLinks>\n\n\n## Prepare a virtual environment\n\nWe recommend using a virtual environment with a modern version of python 3 (3.7+) to install [**ST4SD Core**](/st4sd-core-getting-started) like so:\n\n\n```bash\npython3 -m venv --copies st4sd\n. ./st4sd/bin/activate\npip install \"st4sd-runtime-core[develop]\"\n```\n\nIf you are installing ST4SD on a machine that can submit tasks to [IBM Spectrum LSF](https://www.ibm.com/products/hpc-workload-management)\nthen you should also install the official [`lsf-python-api`](https://github.com/IBMSpectrumComputing/lsf-python-api) python module.\n\n```\n. /path/to/profile.lsf\ngit clone https://github.com/IBMSpectrumComputing/lsf-python-api.git\ncd lsf-python-api\npython3 setup.py build\npython3 setup.py install\n```\n\nCheck the homepage of [`lsf-python-api`](https://github.com/IBMSpectrumComputing/lsf-python-api) for more information.\n\nAfter installing the `lsf-python-api` python module you can launch workflows which contain components that use the [`lsf`](/workflow-specification#description-of-basic-flowir-component-fields) backend.\n\n## Execute a workflow\n\nUse the `elaunch.py` command-line utility that is included by installing `st4sd-runtime-core` to run your workflows.\nFor example, you can run the toy workflow [`sum-numbers`](https://github.com/st4sd/sum-numbers) like so:\n\n```bash\ngit clone https://github.com/st4sd/sum-numbers.git\nelaunch.py --nostamp -l40 sum-numbers\n```\n\n## Provide input files and override data files\n\nST4SD workflows support 3 flavours of inputs:\n\n1. Input files - files user must provide when they execute the workflow\n2. Data files - configuration files that optionally can be overridden\n3. User variables - user provided values for workflow variables\n\nThe [tutorial](/tutorial#providing-inputs-to-workflows) contains more information about inputs.\n\n### Example\n\nHere's an example of a workflow that uses an `input` file, a `data` file, and a variable.\n\nFirst, prepare the workflow definition files by running the following on your terminal:\n\n```bash\ncat <<EOF >workflow.yaml\nvariables:\n  default:\n    global:\n      var: hello\n\ncomponents:\n- name: hello\n  command:\n    executable: sh\n    arguments: |\n      <<EOF\n      echo variable contains \"%(var)s\"\n      echo input contents are \"input/foo.txt:output\"\n      echo data contents are \"data/bar.txt:output\"\n      EOF\n    expandArguments: none\n  references:\n  - input/foo.txt:output\n  - data/bar.txt:output\nEOF\n\ncat <<EOF >manifest.yaml\ndata: data\nEOF\n\nmkdir -p shared_data\ncat <<EOF >shared_data/bar.txt\ndata-file-contents\nEOF\n\ncat <<EOF >foo.txt\ninput-file-contents\nEOF\n\ncat <<EOF >my_vars.yaml\nglobal:\n  var: hi\nEOF\n```\n\nThe above script creates the following file structure:\n\n```\nworkflow.yaml  # the workflow definition\nmanifest.yaml  # manifest that maps \"shared_data\" to \"data\"\nfoo.txt        # the input file\nmy_vars.yaml   # file containing user variables\nshared_data    # the directory containing \"data\" files\n└─ bar.txt\n```\n\nActivate the virtual environment that you used to install `st4sd-runtime-core` and then run:\n\n```bash\nelaunch.py -l40 --nostamp  \\\n      --failSafeDelays=no \\\n      --input foo.txt \\\n      --variables my_vars.yaml \\\n      --manifest manifest.yaml workflow.yaml\necho \"\\n\\nComponent stdout was:\"\ncat workflow.instance/stages/stage0/hello/out.stdout\n```\n\nIf you omit the `--variables` parameter then the `var` variable will receive the value that the `default` platform sets to it.\n\nYou can override the contents of the `data` file `bar.txt` by adding the argument: `--data path/to/a/different/bar.txt`.\nFinally, you can use the `--data` and `--input` parameters multiple times.\n\n## Store outputs to S3\n\nWorkflows may optionally define [`key-outputs`](/tutorial#key-outputs) which which `elaunch.py`\nmay upload to S3 after the experiment terminates.\n\nYou can instruct `elaunch.py` to upload `key-outputs` to S3 via the the `--s3StoreToURI` parameter.\nWhen setting the parameter `--s3StoreToURI` you must also use exactly one of the parameters `--s3AuthWithEnvVars` or `--s3AuthBearer64`.\n\n### Example:\n\n\n```bash\nexport bucket=\"a-bucket\"\nexport path_in_bucket=\"optional/path\"\n\nexport S3_ACCESS_KEY_ID=\"s3 access key id\"\nexport S3_SECRET_ACCESS_KEY=\"s3 secret access key\"\nexport S3_END_POINT=\"s3 end point\"\n\nelaunch.py --s3StoreToURI s3://${bucket}/${path_in_bucket} \\\n  --s3AuthWithEnvVars \\\n  path/to/workflow\n```\n\nWhen `--s3StoreToURI` is set, after the experiment terminates, `elaunch.py` will start uploading the `key-outputs` to the S3 bucket you provided under the specifeid `${path_in_bucket}`.\n`elaunch.py` replaces occurences of the `%(instanceDir)s` literal in `--s3StoreToURI` with the name of the experiment instance.\nFor example, you can use this to store the `key-outputs` of multiple workflow instances in the same bucket.\n\nAlternatively, you can base64-encode the JSON representation of the dictionary `{\"S3_ACCESS_KEY_ID\": \"val\", \"S3_SECRET_ACCESS_KEY\": \"val\", \"S3_END_POINT\": \"val\"}` and use the `--s3AuthBearer64` parameter instead:\n\n```bash\nexport bucket=\"a-bucket\"\nexport path_in_bucket=\"optional/path\"\nexport json=\"{\\\"S3_ACCESS_KEY_ID\\\": \\\"val\\\", \\\"S3_SECRET_ACCESS_KEY\\\": \\\"val\\\", \\\"S3_END_POINT\\\": \\\"val\\\"}\"\nexport s3_auth=`echo \"${json}\" | base64`\n\nelaunch.py --s3StoreToURI s3://${bucket}/${path_in_bucket} \\\n  --s3AuthBearer64 \\\n  path/to/workflow\n```","fileAbsolutePath":"/Users/vassilis/projects/st4sd/overview/src/pages/direct-run.mdx"}}},"staticQueryHashes":["1364590287","137577622","2102389209","2456312558","2746626797","3018647132","3037994772","768070550"]}