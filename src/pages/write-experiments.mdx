---
title: Write experiments
---

import { CarbonForIbmDotcom } from "@carbon/pictograms-react";
import { ArtTools_01 } from "@carbon/pictograms-react";

<!--

  Copyright IBM Inc. All Rights Reserved.
  SPDX-License-Identifier: Apache-2.0

-->

<PageDescription>

This page assumes you are familiar with running experiments locally using the elaunch.py command line tool. If you need a refresher take a moment to read our [docs](/direct-run) before continuing any further.

</PageDescription>


<InlineNotification kind="info">

Here, we are using DSL 2.0, if you need to understand the previous syntax check out the [FlowIR docs](/workflow-specification) and [FlowIR tutorial](/tutorial).

</InlineNotification>

<AnchorLinks>

  <AnchorLink>Wrapping a python script for native execution</AnchorLink>
  <AnchorLink>Sharing your virtual experiments with others</AnchorLink>
  <AnchorLink>Your first Simulation experiment with GAMESS US</AnchorLink>

</AnchorLinks>



## Requirements

- An understanding of [how to run a virtual experiment locally](/direct-run).
- A python 3.9+ interpreter, [git](https://git-scm.com/downloads) to clone code from Git servers, and an understanding of the syntax & structure of [YAML](https://www.redhat.com/en/topics/automation/what-is-yaml)
- A virtual environment with the `st4sd-runtime-core` python module

    ```bash
    python -m venv venv
    . ./venv/bin/activate
    pip pip install "st4sd-runtime-core[develop]">=2.4.0"
    ```
- A Container Runtime system: install one of [docker](https://docs.docker.com/engine/install/), [podman](https://podman.io/docs/installation), or [Rancher Desktop](https://docs.rancherdesktop.io/getting-started/installation/)

<InlineNotification kind="warning">

Before you continue any further, please make sure you are comfortable with [running virtual experiments locally](/direct-run).

</InlineNotification>

## Wrapping a python script for native execution

For your first virtual experiment, we will start with a Python script and use a virtual environment where **st4sd-runtime-core** is installed. This is a great way to quickly prototype virtual experiments without worrying about making them shareable with others. Check out [Sharing your virtual experiments with others](#sharing-your-virtual-experiments-with-others) for an example.

Begin by creating a directory called `python-native.package`. In it, create 2 directories: `bin` and `conf`. Next, we'll define the definition of the virtual experiment.


ST4SD virtual experiments are structured as follows:


```yaml
entrypoint:
    # Instructions of the entry point to your experiment
components:
    # Templates for executing a single task
workflows:
    # Templates for executing multiple steps which can be
    # Workflows or Components
```


Developers build experiments by connecting **Components** and **Workflows**. Components represent individual tasks, while Workflows represent graphs of nested Workflows and Components. The example provided demonstrates the execution of a single task. A helpful way to understand virtual experiments is to think of them as **programs** written in a programming language. In this analogy, Workflows and Components serve as functions, and the **entrypoint** is similar to declaring a **main()** function, specifying which function to execute and what arguments to pass to it.

Place the following in the file `conf/dsl.yaml`:

```yaml
entrypoint:
  entry-instance: printer
  execute:
  - target: <entry-instance>
    args:
      message: Hello world

components:
- signature:
    name: printer
    parameters:
      - name: message
  command:
    executable: bin/printer.py
    arguments: "%(message)s"
```

In this example, the experiment's entrypoint, instantiates the printer component and sets its **message** parameter to "Hello world".

The **printer** component template has a single parameter, message, which does not have a default value. This component runs the executable "bin/printer.py", passing the message value as an argument.


Next, create the "bin/printer.py" file using this very simple python code:


```python
#!/usr/bin/env python

import sys
print(" ".join(sys.argv[1:]))
```


After creating the file, navigate to the **python-native.package** directory and run the command chmod +x bin/printer.py to make the file executable. This step is necessary to allow the file to be executed as a script.


<InlineNotification kind="warning">

At this point, double check that you have used the exact names as above, that your python file starts with the shebang line `#!/usr/bin/env python`, and that it's executable.

The expected file structure is the following:

```
python-native.package
├── bin
│   └── printer.py
└── conf
    └── dsl.yaml
```

If you want to produce similar tree outputs for your directories use the [tree](https://oldmanprogrammer.net/source.php?dir=projects/tree) commandline utility which is also available via [brew](https://formulae.brew.sh/formula/tree).


</InlineNotification>


Let's run the experiment using `elaunch.py`. After a few seconds you should see the following printout:

```
$ elaunch.py --nostamp -l40 python-native.package

completed-on=2025-03-22 12:39:13.806401
cost=0
created-on=2025-03-22 12:39:07.382235
current-stage=stage0
exit-status=Success
experiment-state=finished
stage-progress=1.0
stage-state=finished
stages=['stage0']
total-progress=1.0
updated=2025-03-22 12:39:17.417956
updated-on=2025-03-22 12:39:17.417956
```

The experiment will create the directory **python-native.instance** and store all files it generated in it.

```
python-native.instance
├── bin
│   └── printer.py
├── conf (For now focus on just the dsl.yaml file)
│   ├── dsl.yaml
│   ├── flowir_instance.yaml
│   ├── flowir_package.yaml
│   └── manifest.yaml
├── stages
│   └── stage0
│       └── entry-instance
│           ├── component_performance.csv
│           ├── out.stderr
│           └── out.stdout
├── output
│   ├── experiment.log
│   ├── output.json
│   ├── output.txt
│   ├── status.txt
│   └── status_details.json
├── input
├── python
├── elaunch.yaml
└── status.db

```

If you encountered any issues during the process, please refer to the [troubleshooting](/direct-run#troubleshooting) section of the documentation for guidance on launching experiments locally.

Now that you have ran the experiment, take a moment to **explore its outputs**. You can find the output files following directory:
`python-native.instance/stages/stage0/entry-instance`.


### Exercise


In ST4SD you can override the parameters of `entry-instance` that the `entrypoint` sets via the dictionary `entrypoint.execute[0].args`.

For example, place the following into a new file `my-variables.yaml`:

```yaml
global:
  message: my custom message
```

Then remove the `python-native.instance` directory and run the experiment again but this time use load the `my-variables.yaml` file:

```bash
rm -rf python-native.instance
elaunch.py -l40 --nostamp -a my-variables.yaml python-native.package
```

The **stdout** of the `hello` component can be found in the following file:
`0-hello-world.instance/stages/stage0/entry-instance/out.stdout`.

## Sharing your virtual experiments with others

To make experiments shareable, they must include the following key information:

1. All executables that they run, along with their software dependencies
2. How to map the executables to specific steps
3. How to connect inputs to these steps

In the [example](#wrapping-a-python-script-for-native-execution) above, we demonstrated how to wrap a single-step executable into a virtual experiment, covering the second and third requirements for a single-step experiment. In this example, we will utilize a container to share the software dependencies of the "printer.py" executable, addressing the first requirement.

### Containerize your python application

In a **requirements.txt** file place the python dependencies of your script.
The **printer.py** python script that we use here does not have any python requirements but we'll just include **numpy** to demonstrate the method:

The contents of the  `requirements.txt` file are:

```
numpy==2.2.4
```

Next, create a file called `Dockerfile` with the following contexts:

```docker
FROM python:3.11-slim

RUN    apt-get update \
    && apt-get upgrade -y \
    && apt-get clean -y \
    && rm -rf /var/lib/apt/lists/*

# Make sure that files under /app are part of $PATH
ENV PATH=/app:$PATH
WORKDIR /app

COPY requirements.txt /app/requirements.txt

RUN pip install -r requirements.txt

COPY printer.py /app/printer.py

# Make the printer.py file executable
RUN chmod +x /app/printer.py
```

Make sure you have the following files in the directory you are currently in:

```
.
├── Dockerfile
├── printer.py
└── requirements.txt
```

To build your container, run docker:


```
docker build --platform linux/amd64 -f Dockerfile -t my-printer:latest
```

<InlineNotification kind="info">

We recommend building images for the x86-64 CPU architecture using the `--platform linux/amd64` flag to ease the transition into executing your virtual experiments on the cloud. You can specify the platform when building your image using the following command-line argument `--platform linux/amd64`.

</InlineNotification>


### Making your container available to others

If you plan to share your experiment with others, you will need to push your containers to a remote container registry, such as [Docker Hub](https://hub.docker.com/). This allows others to easily access and pull your container images, making it simpler to share and reproduce your experiment.

To push your container to a remote registry, you can use the following steps:

1. **Tag your container image**: Use the `docker tag` command to assign a unique name to your image, including the registry URL and your username.
2. **Login to the registry**: Use the `docker login` command to authenticate with the registry.
3. **Push the image**: Use the `docker push` command to upload your image to the registry.

For example:

```bash
: # Tag the image
docker tag my-printer:latest <your-username>/my-printer:latest

: # Login to Docker Hub
docker login

: # Push the image
docker push <your-username>/my-printer:latest
```

<InlineNotification kind="info">

The remainder of this example will assume that you do not have access to a container registry. In this case, you can still share your experiment with others by providing them with the necessary files and instructions to build the container image themselves. This can be done by sharing the **Dockerfile**, **printer.py**, and **requirements.txt** files. The recipient can then build the image using the `docker build --platform linux/amd64 -f Dockerfile -t my-printer:latest` command and run your virtual experiment locally.

</InlineNotification>

### Create a virtual experiment that uses the container

Create a new directory called `python-docker.package`, in it create a directory called `conf`.

Next, create the file **cpython-docker.package/onf/dsl.yaml** using the following contents:

```yaml
entrypoint:
  entry-instance: printer
  execute:
  - target: <entry-instance>
    args:
      message: Hello world

components:
- signature:
    name: printer
    parameters:
      - name: message
  command:
    executable: printer.py
    arguments: "%(message)s"
    environment:
        PATH: /usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/app
  resourceManager:
    config:
        backend: docker
    docker:
        image: my-printer:latest
        imagePullPolicy: IfNotPresent
```


The differences between this experiment and **python-native.package** are all about the **printer** `component`:

1. The **bin/printer.py** file is no longer used.
    - We set `command.executable` to `printer.py`
    - The runtime system will search for this executable in the `$PATH` environment variable of the component
1. We set `command.environment.PATH` to include the path to the `printer.py` script
    - By default, components receive the virtual environment of the runtime process which is not guaranteed to be compatible with the environment variables that enable the execution of commands inside the container
1. Configure the docker backend for this component
   - Set `resourceManager.config.backend` to `docker`
   - Set `resourceManager.docker.image` to `my-printer:latest`
   - Set `resourceManager.docker.imagePullPolicy` to `IfNotPresent`
      - This setting instructs the runtime to only attempt to pull the image if it's not already present on the local machine

The resulting file tree of your experiment should look like this:

```
python-docker.package
└── conf
    └── dsl.yaml
```

<InlineNotification kind="info">

ST4SD supports multiple different backends for your components however these features are beyond the focus of this example. You can find more information in the [advanced experiments](/add-interface-to-experiments) as well as the [DSL documentation](/workflow-specification-dsl) page.

</InlineNotification>

### Exercise

Run your virtual experiment using **elaunch.py**:

```
elaunch.py --nostamp -l40 python-docker.package
```

If you encountered any issues during the process, please refer to the [troubleshooting](/direct-run#troubleshooting) section of the documentation for guidance on launching experiments locally.

Now that you have ran the experiment, take a moment to **explore its outputs**. You can find the output files following directory:
`python-native.instance/stages/stage0/entry-instance`.


## Your first Simulation experiment with GAMESS US

In this example, we will create a virtual experiment that performs the Parameterized Model 3 (PM3) method in GAMESS US. PM3 is a semi-empirical quantum chemistry method. Scientists use it to calculate the molecular properties and energies when computational efficiency is a priority as an alternative to high accuracy but slow to run high-level quantum methods like Hartree-Fock or Density Functional Theory (DFT).

Start by creating a new directory called `gamess-us-pm3.package` containing 3 directories: `bin`, `conf`, and `hooks` like so:

```
gamess-us-pm3.package
├── bin
├── conf
└── hooks
```

Create the file `bin/run-gamess.sh` using the following:

```
#!/usr/bin/env sh

molecule=$1
cpus=$2

# The restart hook expects the filename to exist in the working directory
# of GAMESS US
molecule_name=$(basename "${molecule}")
cp ${molecule} ${molecule_name}

PATH_RUNGMS_WRAPPER=${PATH_RUNGMS:-/usr/local/bin/rungms}
PATH_GAMESS=${PATH_GAMESS:-/usr/local/bin/gamess}

PATH_MY_GAMESS=${PATH_MY_GAMESS:-/tmp/gamess}
GAMESS_SCRATCH_DIR=${GAMESS_SCRATCH_DIR:-${PATH_MY_GAMESS}/scratch}

here=`pwd`
mkdir -p "${PATH_MY_GAMESS}"
mkdir -p "${GAMESS_SCRATCH_DIR}"

sed -e "s#set USERSCR=/workspace/restart#set USERSCR=${here}#g" \
    -e "s#set currentdir=\`pwd\`#set currentdir=${PATH_GAMESS}#g" \
    -e "s#set SCR=\`pwd\`/scratch#set SCR=${GAMESS_SCRATCH_DIR}#g" \
    -e "s#TARGET=mpi#TARGET=ga#g" \
    "${PATH_GAMESS}/rungms" >"${PATH_MY_GAMESS}/run-gamess.sh"

cp /usr/local/bin/gamess/install.info "${PATH_GAMESS}/install.info"

# The NVidia Image Features version 00 ONLY and target=GA ONLY
version=00

chmod +x ${PATH_MY_GAMESS}/run-gamess.sh

"${PATH_MY_GAMESS}"/run-gamess.sh "${molecule_name}" "${version}" "${cpus}"


```

Then download the [extract_gmsout.py script](https://github.com/st4sd/band-gap-gamess/blob/main/component-scripts/extract_gmsout.py) and store it in the `bin` directory.

Next, make the both the **run-gamess.sh** and **extract_gmsout.py** scripts executable by running `chmod +x bin/*` inside the `gamess-us-pm3.package` directory.

Download the [RestartHook example](https://github.com/st4sd/band-gap-gamess/blob/main/hooks/semi_empirical_restart.py) and save it in the **hooks** directory. This script checks if the PM3 method in GAMESS US has converged. If not, it triggers a task restart.

Next, prepare the definition of the experiment by pasting the following into the **conf/dsl.yaml** file:

```yaml
entrypoint:
  entry-instance: gamess-us-pm3
  execute:
  - target: <entry-instance>
    args:
      input.molecule.inp: input/molecule.inp
      gamess-number-processors: 1
      gamess-memory: "4096Mi"
      # gamess-gpus is only relevant for executon on Kubernetes
      gamess-gpus: 0
      backend: docker

workflows:
- signature:
    name: gamess-us-pm3
    parameters:
    - name: input.molecule.inp
    - name: gamess-number-processors
    - name: gamess-memory
    - name: gamess-gpus
    - name: backend
  steps:
    optimise: geometry-optimisation
    parse-gamess: extract-energies
  execute:
    - target: <optimise>
      args:
        molecule: "%(input.molecule.inp)s:ref"
        gamess-number-processors: "%(gamess-number-processors)s"
        gamess-memory: "%(gamess-memory)s"
        gamess-gpus: "%(gamess-gpus)s"
        backend: "%(backend)s"
    - target: <parse-gamess>
      args:
        gamess-working-directory: "<optimise>:ref"
        backend: "%(backend)s"

components:
- signature:
    name: geometry-optimisation
    parameters:
    - name: molecule
    - name: gamess-number-processors
      default: 1
    - name: gamess-memory
      default: "4096Mi"
    - name: backend
      default: docker
    - name: gamess-image
      default: nvcr.io/hpc/gamess:17.09-r2-libcchem
    - name: docker-platform
      default: "linux/amd64"
    - name: gamess-gpus
      default: 0
  command:
    arguments: "%(molecule)s %(gamess-number-processors)s"
    environment:
      PATH: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
    executable: bin/run-gamess.sh
  workflowAttributes:
    restartHookFile: semi_empirical_restart.py
    restartHookOn:
    - KnownIssue
    - Success
    - ResourceExhausted
    shutdownOn:
    - KnownIssue
    - ResourceExhausted
  resourceManager:
    config:
      backend: '%(backend)s'
      # in minutes - only applies to Kubernetes runs
      walltime: 600
    docker:
      image: "%(gamess-image)s"
      platform: "%(docker-platform)s"
    kubernetes:
      image: "%(gamess-image)s"
  resourceRequest:
    memory: '%(gamess-memory)s'
    numberThreads: '%(gamess-number-processors)s'
    threadsPerCore: 1
    gpus: '%(gamess-gpus)s'

- signature:
    name: extract-energies
    parameters:
    - name: gamess-working-directory
    - name: backend
    - name: docker-platform
      default: "linux/amd64"
  command:
    arguments: "%(gamess-working-directory)s"
    executable: bin/extract_gmsout.py
  resourceManager:
    config:
      backend: '%(backend)s'
    kubernetes:
      image: quay.io/st4sd/community-applications/rdkit-st4sd:2019.09.1
    docker:
      image: quay.io/st4sd/community-applications/rdkit-st4sd:2019.09.1
      platform: "%(docker-platform)s"
```

You should now have the following file structure:

```
gamess-us-pm3.package
├── bin
│   ├── extract_gmsout.py
│   └── run-gamess.sh
├── conf
│   └── dsl.yaml
└── hooks
    └── semi_empirical_restart.py
```

### Exercise

Try starting your experiment now using a container runtime.

In the parent directory of **gamess-us-pm3.package** create your input **molecule.inp** file.

You can use this:

```
 $CONTRL COORD=UNIQUE SCFTYP=RHF RUNTYP=OPTIMIZE MULT=1
 ISPHER=1 ICHARG=0 MAXIT=100 $END
 $SYSTEM MWORDS=100 TIMLIM=600 $END
 $BASIS GBASIS=PM3 $END
 $GUESS GUESS=HUCKEL $END
 $SCF DIRSCF=.t. FDIFF=.f. DIIS=.t. $END
 $STATPT NSTEP=500 PROJCT=.f. IHREP=20 HSSEND=.t. $END
 $DATA
CH4 C CH4
 C1
 C 6.0 0.0 0.0 0.0
 H 1.0 0.1895 0.9552 -0.4946
 H 1.0 0.9509 -0.4809 0.2396
 H 1.0 -0.5631 0.1717 0.92
 H 1.0 -0.5773 -0.6461 -0.665
 $END
```

<InlineNotification kind="info">

To use your own GAMESS US input file, carefully review the configuration options preceding the `$DATA` section and include them in your file. Additionally, ensure that your input is saved in a file named **molecule.inp**.

</InlineNotification>


You should now have this structure:

```
.
├── molecule.inp
└── gamess-us-pm3.package
    ├── bin
    │   └── run-gamess.sh
    │   └── extract_gmsout.py
    ├── conf
    │   └── dsl.yaml
    └── hooks
        └── semi_empirical_restart.py
```

The command to launch the experiment is:

```
$ elaunch.py -l40 --nostamp -i molecule.inp gamess-us-pm3.package
```

After a couple of minutes you should see:

```
completed-on=2025-03-24 15:44:58.212321
cost=0
created-on=2025-03-24 15:44:34.036279
current-stage=stage0
exit-status=Success
experiment-state=finished
stage-progress=1.0
stage-state=finished
stages=['stage0']
total-progress=1.0
updated=2025-03-24 15:45:00.865704
updated-on=2025-03-24 15:45:00.865704
```

The **gamess-us-pm3.instance** directory will have the following structure:

```
gamess-us-pm3.instance
├── bin
│   ├── extract_gmsout.py
│   └── run-gamess.sh
├── conf
│   ├── dsl.yaml
│   ├── flowir_instance.yaml
│   ├── flowir_package.yaml
│   └── manifest.yaml
├── elaunch.yaml
├── hooks
│   ├── __pycache__
│   │   └── semi_empirical_restart.cpython-310.pyc
│   └── semi_empirical_restart.py
├── input
│   └── molecule.inp
├── output
│   ├── experiment.log
│   ├── output.json
│   ├── output.txt
│   ├── status.txt
│   └── status_details.json
├── python
├── stages
│   └── stage0
│       ├── optimise
│       │   ├── Run1
│       │   │   ├── component_performance.csv
│       │   │   ├── molecule.dat
│       │   │   ├── molecule.inp
│       │   │   ├── molecule.rst
│       │   │   ├── out.stderr
│       │   │   └── out.stdout
│       │   ├── component_performance.csv
│       │   ├── molecule.dat
│       │   ├── molecule.inp
│       │   ├── molecule.rst
│       │   ├── out.stderr
│       │   └── out.stdout
│       └── parse-gamess
│           ├── component_performance.csv
│           ├── csv2inp.log
│           ├── energies.csv
│           ├── out.stderr
│           └── out.stdout
└── status.db

```


Examine the files under **stages/stage0/optimise** and **stages/stage0/parse-gamess**.

These were the contents of **stages/stage0/parse-gamess/energies.csv** for the experiment we ran on our laptop:

```
label,completed,total-energy,homo,lumo,gap,electric-moments,total-time,total-time-per-core
molecule,OK,-180.53313527498008,-13.641,4.245,17.886,0.000050,0.1,0.10
```



## What's next?

- Learn more about writing experiments, including more advanced features and best practice [here](/write-more-experiments)
- Learn how to add [key-outputs and interfaces](/add-interface-to-experiments) to your experiments
- More information on running experiments directly, i.e. via `elaunch.py` [here](/direct-run)
- More information on the DSL of ST4SD i.e. how to write experiments [here](/workflow-specification-dsl)
- More information on how to structure and test your experiments [here](/packaging-workflows/)

