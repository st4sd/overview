---
title: Write experiments
---

import { CarbonForIbmDotcom } from "@carbon/pictograms-react";
import { ArtTools_01 } from "@carbon/pictograms-react";

<!--

  Copyright IBM Inc. All Rights Reserved.
  SPDX-License-Identifier: Apache-2.0

-->

<PageDescription>

This page assumes you are familiar with running experiments locally using the elaunch.py command line tool. If you need a refresher take a moment to read our [docs](/direct-run) before continuing any further.

</PageDescription>


<InlineNotification kind="info">

Here, we are using DSL 2.0, if you need to understand the previous syntax check out the [FlowIR docs](/workflow-specification) and [FlowIR tutorial](/tutorial).

</InlineNotification>

<AnchorLinks>

  <AnchorLink>Wrapping a python script for native execution</AnchorLink>
  <AnchorLink>Packaging your virtual experiment</AnchorLink>
  <AnchorLink>Using containers for shareable virtual experiments</AnchorLink>
  <AnchorLink>Your first Simulation experiment with GAMESS US</AnchorLink>

</AnchorLinks>



## Requirements

- An understanding of [how to run a virtual experiment locally](/direct-run).
- A python 3.9+ interpreter, [git](https://git-scm.com/downloads) to clone code from Git servers, and an understanding of the syntax & structure of [YAML](https://www.redhat.com/en/topics/automation/what-is-yaml)
- A virtual environment with the `st4sd-runtime-core` python module

    ```bash
    python -m venv venv
    . ./venv/bin/activate
    pip install "st4sd-runtime-core[develop]>=2.5.1"
    ```
- A Container Runtime system: install one of [docker](https://docs.docker.com/engine/install/), [podman](https://podman.io/docs/installation), or [Rancher Desktop](https://docs.rancherdesktop.io/getting-started/installation/)

<InlineNotification kind="warning">

Before you continue any further, please make sure you are comfortable with [running virtual experiments locally](/direct-run).

</InlineNotification>

## Wrapping a python script for native execution

For your first virtual experiment, we will start with a Python script and use a virtual environment where **st4sd-runtime-core** is installed. This is a great way to quickly prototype virtual experiments without worrying about making them shareable with others. Check out [Using containers for shareable virtual experiments](#using-containers-for-shareable-virtual-experiments) for an example.

Begin by creating the directory `/tmp/hello-world` and navigating into it. We are going to store all files relevant to the virtual experiment of this example in this directory.

<InlineNotification kind="info">

Double check you have created a directory with the path `/tmp/hello-world` - this examples assumes you are using this exact path.

</InlineNotification>


A good way to become familiar with ST4SD is to wrap a simple python script in a virtual experiment and execute it. So, let's build a **hello world** experiment using python. Create the file **printer.py** with the following contents:

```python
import sys
print(" ".join(sys.argv[1:]))
```

This Python script is straightforward and does not rely on any external Python packages. If it did, you would need to install the required dependencies using `pip install` within the virtual environment where **st4sd-runtime-core** is installed. Since this script has no external dependencies, no additional Python modules need to be installed.


To create a ST4SD virtual experiment based on this script, we will define it using a YAML file. The structure of a ST4SD virtual experiment definition is as follows:


```yaml
entrypoint:
    # Instructions of the entry point to your experiment
components:
    # Templates each of which execute a single task
workflows:
    # Templates each of which pipelines of tasks which themselves
    # are either instances of Workflows or Components templates
```


Developers build experiments by connecting together **Components** and **Workflows** and identifying the entry point of the virtual experiment.

Components represent individual tasks, while Workflows represent pipelines (i.e. graphs) of Workflows and Components. The example provided demonstrates the execution of a single task. A helpful way to understand virtual experiments is to think of them as **programs** written in a programming language. In this analogy, Workflows and Components serve as functions, and the **entrypoint** is similar to declaring a **main()** function, specifying which function to execute and what arguments to pass to it.

Let's put together a simple experiment consisting of a single step that prints a message to its standard output.

Place the following in the file `hello-world.yaml`:

```yaml
entrypoint:
  entry-instance: printer
  execute:
  - target: <entry-instance>
    args:
      message: Hello world

components:
- signature:
    name: printer
    parameters:
      - name: message
  command:
    executable: python
    arguments: /tmp/hello-world/printer.py "%(message)s"
```

In this example, the **entrypoint** to the experiment is an instance of the **printer** component which sets its message parameter to the value **"Hello world"**.

Moving on to the **Components** template section we observe that there is a single template called **printer**. The **printer** component template has a single parameter called **message**, which does not have a default value. Instances of this template, run the executable **python**, passing the absolute path to the **printer.py** script and the **message** value as arguments. In the next example we will show you a better way to package experiments alleviating the need to use absolute paths for your scripts.


<InlineNotification kind="warning">

At this point, double check that you have used the exact names as above.

The expected file structure in **/tmp/hello-world** is as follows:

```
/tmp/hello-world
├── hello-world.yaml
└── printer.py
```

</InlineNotification>


Let's run the experiment using `elaunch.py`. Run the following command from inside the **/tmp/hello-world** directory

```
elaunch.py --nostamp hello-world.yaml
```

After a few seconds you should see:

```
completed-on=2025-03-22 12:39:13.806401
cost=0
created-on=2025-03-22 12:39:07.382235
current-stage=stage0
exit-status=Success
experiment-state=finished
stage-progress=1.0
stage-state=finished
stages=['stage0']
total-progress=1.0
updated=2025-03-22 12:39:17.417956
updated-on=2025-03-22 12:39:17.417956
```

The experiment will create the directory **hello-world.instance** and store all files it generated in it.

```
hello-world.instance
├── conf
│   ├── dsl.yaml               # your virtual experiment definition
│   ├── flowir_instance.yaml   # ignore this file
│   ├── flowir_package.yaml    # ignore this file
│   └── manifest.yaml          # ignore this file
├── elaunch.yaml
├── input
├── output
│   ├── experiment.log
│   ├── output.json
│   ├── output.txt
│   ├── status.txt
│   └── status_details.json
├── python
├── stages
│   └── stage0
│       └── entry-instance
│           ├── component_performance.csv
│           ├── out.stderr
│           └── out.stdout
└── status.db

```

If you encountered any issues during the process, please refer to the [troubleshooting](/direct-run#troubleshooting) section of the documentation for guidance on launching experiments locally.

Now that you have ran the experiment, take a moment to **explore its outputs**. You can find the output files following directory:
`hello-world.instance/stages/stage0/entry-instance`.


### Exercise


In ST4SD you can override the parameters of `entry-instance` that the `entrypoint` sets via the dictionary `entrypoint.execute[0].args`.

For example, place the following into a new file `my-variables.yaml`:

```yaml
global:
  message: my custom message
```

Then remove the `hello-world.instance` directory and run the experiment again but this time use load the `my-variables.yaml` file:

```bash
rm -rf hello-world.instance
elaunch.py --nostamp -a my-variables.yaml hello-world.yaml
```

The **stdout** of the `hello` component can be found in the following file:
`hello-world.instance/stages/stage0/entry-instance/out.stdout`.


## Packaging your virtual experiment

When automating simulation codes with custom bash scripts, you may have experienced difficulties with absolute paths when relocating your codes to different directories or execution environments. ST4SD provides a solution to this issue. It offers two methods for [packaging multiple files](/packaging-workflows), with the most convenient approach being the use of the [Standard](/packaging-workflows#standard-project) project structure. This structure consists of a virtual experiment definition, defined in a YAML file, and an optional **manifest** file, which specifies additional directories to be included with the virtual experiment. The **manifest** file has the following format:



The **manifest** file has the following format:

```yaml
destinationDirectoryName: sourceDirectory
```

This instructs the runtime system to create a directory called **destinationDirectoryName** using the files from the path **sourceDirectory**. If the **sourceDirectory** is not an absolute path then it is considered relative to the location of the experiment definition YAML file.


To convert the above example to use the **Standard** project structure, first create the directory `/tmp/hello-world/bin` and move the **printer.py** script into it. Next, create the file **manifest.yaml** under the directory `/tmp/hello-world/` with the following content:

```yaml
bin: bin
```

Now, let's update the virtual definition to use the **bin/printer.py** file. You just need to change the last line in the **components** section of your **hello-world.yaml** file:
```yaml
...
components:
- signature:
    name: printer
    parameters:
      - name: message
  command:
    executable: python
    arguments: /tmp/hello-world/printer.py "%(message)s" # HERE
```

Replace the absolute path **/tmp/hello-world/printer.py** with **bin/printer.py:ref**. The **:ref** suffix indicates that this is a reference to a file, rather than a direct path. At runtime, the system will use the **manifest.yaml** file to resolve this reference, enabling you to include additional files with your virtual experiment definition in a flexible and portable way.

Your updated **hello-world.yaml** file should now look like this:

```yaml
entrypoint:
  entry-instance: printer
  execute:
  - target: <entry-instance>
    args:
      message: Hello world

components:
- signature:
    name: printer
    parameters:
      - name: message
  command:
    executable: python
    arguments: bin/printer.py:ref "%(message)s"
```


You should end up with the following files:

```
/tmp/hello-world
├── bin
│   └── printer.py
├── hello-world.yaml
└── manifest.yaml
```

Finally, let's run this experiment:

```
elaunch.py --nostamp --manifest manifest.yaml hello-world.yaml
```

<InlineNotification kind="info">

The **--manifest manifest.yaml** argument is used to specify the **manifest.yaml** file as the source of manifest information for your virtual experiment, allowing the runtime to access the necessary configuration details.


</InlineNotification>


After a few seconds you should see this output on your terminal:

```
completed-on=2025-03-31 10:19:45.271792
cost=0
created-on=2025-03-31 10:19:39.121168
current-stage=stage0
exit-status=Success
experiment-state=finished
stage-progress=1.0
stage-state=finished
stages=['stage0']
total-progress=1.0
updated=2025-03-31 10:19:49.192388
updated-on=2025-03-31 10:19:49.192388
```

Congratulations! You have successfully packaged your virtual experiment!

### Exercise

Modify your **printer.py** script to import a Python package, such as **transformers**. To ensure successful execution, make sure to install **transformers** within the same virtual environment where **st4sd-runtime-core** is installed.

Next, run it elaunch.py:

```
rm -rf hello-world.instance
elaunch.py --nostamp --manifest manifest.yaml hello-world.yaml
```

Double check that it runs to completion.

## Using containers for shareable virtual experiments

To make experiments truly shareable, they must include the following key information:

1. All executables that they run, along with their software dependencies
2. How to map the executables to specific steps
3. How to connect inputs to these steps

In the [above example](#packaging-your-virtual-experiment) we wrapped a single-step executable into a virtual experiment, covering the second and third requirements for a single-step experiment. In this example, we will utilize a container to share the software dependencies of the **printer.py** python script, addressing the first requirement.

Create a new directory in `/tmp/docker-package` and cd into it, we will use it for the files of this virtual experiment.

### Containerize your python application

In a **requirements.txt** file place the python dependencies of your script.
The **printer.py** python script that we use here does not have any python requirements but we'll just install **transformers** in the container we use to execute the script just to demonstrate the method:

The contents of the  `requirements.txt` file are:

```
transformers==4.50.3
```

Next, create a file called `Dockerfile` with the following contexts:

```docker
FROM python:3.11-slim

RUN    apt-get update \
    && apt-get upgrade -y \
    && apt-get clean -y \
    && rm -rf /var/lib/apt/lists/*

# Make sure that files under /app are part of $PATH
ENV PATH=/app:$PATH
WORKDIR /app

COPY requirements.txt /app/requirements.txt

RUN pip install -r requirements.txt

# Place the printer.py file inside the container
COPY printer.py /app/printer.py
```

Make sure you have the following files in the directory you are currently in:

```
/tmp/docker-package
├── Dockerfile
├── printer.py
└── requirements.txt
```

To build your container, run **docker build**:


```
docker build --platform linux/amd64 -f Dockerfile -t my-printer:latest .
```

<InlineNotification kind="info">

We recommend building images for the x86-64 CPU architecture using the `--platform linux/amd64` flag to ease the transition into executing your virtual experiments on the cloud. You can specify the platform when building your image using the following command-line argument `--platform linux/amd64`.

</InlineNotification>


### Making your container available to others

If you plan to share your experiment with others, you will need to push your containers to a remote container registry, such as [Docker Hub](https://hub.docker.com/). This allows others to easily access and pull your container images, making it simpler to share and reproduce your experiment.

To push your container to a remote registry, you can use the following steps:

1. **Tag your container image**: Use the `docker tag` command to assign a unique name to your image, including the registry URL and your username.
2. **Login to the registry**: Use the `docker login` command to authenticate with the registry.
3. **Push the image**: Use the `docker push` command to upload your image to the registry.

For example:

```bash
: # Tag the image
docker tag my-printer:latest <your-username>/my-printer:latest

: # Login to Docker Hub
docker login

: # Push the image
docker push <your-username>/my-printer:latest
```

<InlineNotification kind="info">

The remainder of this example will assume that you do not have access to a container registry. In this case, you can still share your experiment with others by providing them with the necessary files and instructions to build the container image themselves. This can be done by sharing the **Dockerfile**, **printer.py**, and **requirements.txt** files. The recipient can then build the image using the `docker build --platform linux/amd64 -f Dockerfile -t my-printer:latest` command and run your virtual experiment locally.

</InlineNotification>

### Create a virtual experiment that uses the container

Create the file **docker-package.yaml** in the `/tmp/docker-package` directory with the following contents:

```yaml
entrypoint:
  entry-instance: printer
  execute:
  - target: <entry-instance>
    args:
      message: Hello world

components:
- signature:
    name: printer
    parameters:
      - name: message
  command:
    executable: python
    arguments: /app/printer.py "%(message)s"
    environment:
        PATH: /usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/app
  resourceManager:
    config:
        backend: docker
    docker:
        image: my-printer:latest
        imagePullPolicy: IfNotPresent
```


The differences between this experiment and above **hello-world.yaml** experiment are all about the **printer** `component`:

1. The **bin/printer.py:ref** Reference is replaced by the direct path `/app/printer.py`
    - The file is now located inside the container so using a direct path is perfectly fine
    - The runtime system will search for this executable in the `$PATH` environment variable of the component
1. We set `command.environment.PATH` to include the path to the `printer.py` script
    - By default, components receive the virtual environment of the runtime process which is not guaranteed to be compatible with the environment variables that enable the execution of commands inside the container
1. Configure the docker backend for this component
   - Set `resourceManager.config.backend` to `docker`
   - Set `resourceManager.docker.image` to `my-printer:latest`
   - Set `resourceManager.docker.imagePullPolicy` to `IfNotPresent`
      - This setting instructs the runtime to only attempt to pull the image if it's not already present on the local machine

The resulting file tree in `/tmp/docker-package` should be:

```
/tmp/docker-package
├── Dockerfile            # To build image
├── requirements.txt      # To build image
├── printer.py            # To build image
└── docker-package.yaml   # To execute experiment
```

<InlineNotification kind="info">

ST4SD supports multiple different backends for your components however these features are beyond the focus of this example. You can find more information in the [advanced experiments](/add-interface-to-experiments) as well as the [DSL documentation](/workflow-specification-dsl) page.

</InlineNotification>

### Exercise

Run your virtual experiment using **elaunch.py**:

```
elaunch.py --nostamp docker-package.yaml
```

If you encountered any issues during the process, please refer to the [troubleshooting](/direct-run#troubleshooting) section of the documentation for guidance on launching experiments locally.

Now that you have ran the experiment, take a moment to **explore its outputs**. You can find the output files following directory:
`docker-package.instance/stages/stage0/entry-instance`.


## Your first Simulation experiment with GAMESS US

In this example, we will create a virtual experiment that performs the Parameterized Model 3 (PM3) method in GAMESS US. PM3 is a semi-empirical quantum chemistry method. Scientists use it to calculate the molecular properties and energies when computational efficiency is a priority as an alternative to high accuracy but slow to run high-level quantum methods like Hartree-Fock or Density Functional Theory (DFT).

Start by creating a new directory in `/tmp/gamess-us-pm3` containing 2 directories: `bin` and `hooks` like so:

```
/tmp/gamess-us-pm3
├── bin
└── hooks
```

Create the file `bin/run-gamess.sh` using the following:

```
#!/usr/bin/env sh

molecule=$1
cpus=$2

# The restart hook expects the filename to exist in the working directory
# of GAMESS US
molecule_name=$(basename "${molecule}")
cp ${molecule} ${molecule_name}

PATH_RUNGMS_WRAPPER=${PATH_RUNGMS:-/usr/local/bin/rungms}
PATH_GAMESS=${PATH_GAMESS:-/usr/local/bin/gamess}

PATH_MY_GAMESS=${PATH_MY_GAMESS:-/tmp/gamess}
GAMESS_SCRATCH_DIR=${GAMESS_SCRATCH_DIR:-${PATH_MY_GAMESS}/scratch}

here=`pwd`
mkdir -p "${PATH_MY_GAMESS}"
mkdir -p "${GAMESS_SCRATCH_DIR}"

sed -e "s#set USERSCR=/workspace/restart#set USERSCR=${here}#g" \
    -e "s#set currentdir=\`pwd\`#set currentdir=${PATH_GAMESS}#g" \
    -e "s#set SCR=\`pwd\`/scratch#set SCR=${GAMESS_SCRATCH_DIR}#g" \
    -e "s#TARGET=mpi#TARGET=ga#g" \
    "${PATH_GAMESS}/rungms" >"${PATH_MY_GAMESS}/run-gamess.sh"

cp /usr/local/bin/gamess/install.info "${PATH_GAMESS}/install.info"

# The NVidia Image Features version 00 ONLY and target=GA ONLY
version=00

chmod +x ${PATH_MY_GAMESS}/run-gamess.sh

"${PATH_MY_GAMESS}"/run-gamess.sh "${molecule_name}" "${version}" "${cpus}"


```

Then download the [extract_gmsout.py script](https://github.com/st4sd/band-gap-gamess/blob/main/component-scripts/extract_gmsout.py) and store it in the `bin` directory.

Next, make the **run-gamess.sh** script executable by running `chmod +x bin/run-gamess.sh` from inside the `/tmp/gamess-us-pm3` directory.

Download the [RestartHook example](https://github.com/st4sd/band-gap-gamess/blob/main/hooks/semi_empirical_restart.py) and save it under **hooks/semi_empirical_restart.py**. This script checks if the PM3 method in GAMESS US has converged. If not, it triggers a task restart. You can find more information on **RestartHooks** in our documentation about [restarting tasks](/restart).

Next, prepare the definition of the experiment by pasting the following into the **gamess-us-pm3.yaml** file:

```yaml
entrypoint:
  entry-instance: gamess-us-pm3
  execute:
  - target: <entry-instance>
    args:
      input.molecule.inp: input/molecule.inp
      gamess-number-processors: 1
      gamess-memory: "4096Mi"
      # gamess-gpus is only relevant for execution on Kubernetes
      gamess-gpus: 0
      backend: docker

workflows:
- signature:
    name: gamess-us-pm3
    parameters:
    - name: input.molecule.inp
    - name: gamess-number-processors
    - name: gamess-memory
    - name: gamess-gpus
    - name: backend
  steps:
    optimise: geometry-optimisation
    parse-gamess: extract-energies
  execute:
    - target: <optimise>
      args:
        molecule: "%(input.molecule.inp)s:ref"
        gamess-number-processors: "%(gamess-number-processors)s"
        gamess-memory: "%(gamess-memory)s"
        gamess-gpus: "%(gamess-gpus)s"
        backend: "%(backend)s"
    - target: <parse-gamess>
      args:
        gamess-working-directory: "<optimise>:ref"
        backend: "%(backend)s"

components:
- signature:
    name: geometry-optimisation
    parameters:
    - name: molecule
    - name: gamess-number-processors
      default: 1
    - name: gamess-memory
      default: "4096Mi"
    - name: backend
      default: docker
    - name: gamess-image
      default: nvcr.io/hpc/gamess:17.09-r2-libcchem
    - name: docker-platform
      default: "linux/amd64"
    - name: gamess-gpus
      default: 0
  command:
    arguments: "%(molecule)s %(gamess-number-processors)s"
    environment:
      PATH: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
    executable: bin/run-gamess.sh  # the runtime resolves relative paths in the executable field
                                   # using the manifest file
  workflowAttributes:
    restartHookFile: semi_empirical_restart.py
    restartHookOn:
    - KnownIssue
    - Success
    - ResourceExhausted
    shutdownOn:
    - KnownIssue
    - ResourceExhausted
  resourceManager:
    config:
      backend: '%(backend)s'
      # in minutes - only applies to Kubernetes runs
      walltime: 600
    docker:
      image: "%(gamess-image)s"
      platform: "%(docker-platform)s"
    kubernetes:
      image: "%(gamess-image)s"
  resourceRequest:
    memory: '%(gamess-memory)s'
    numberThreads: '%(gamess-number-processors)s'
    threadsPerCore: 1
    gpus: '%(gamess-gpus)s'

- signature:
    name: extract-energies
    parameters:
    - name: gamess-working-directory
    - name: backend
    - name: docker-platform
      default: "linux/amd64"
  command:
    executable: python
    arguments: "bin/extract_gmsout.py:ref %(gamess-working-directory)s"
  resourceManager:
    config:
      backend: '%(backend)s'
    kubernetes:
      image: quay.io/st4sd/community-applications/rdkit-st4sd:2019.09.1
    docker:
      image: quay.io/st4sd/community-applications/rdkit-st4sd:2019.09.1
      platform: "%(docker-platform)s"
```

Finally create your **manifest.yaml** file:

```yaml
bin: bin
hooks: hooks
```

You should now have the following file structure:

```
/tmp/gamess-us-pm3
├── bin
│   ├── extract_gmsout.py
│   └── run-gamess.sh
├── hooks
│   └── semi_empirical_restart.pyf
├── manifest.yaml
└── gamess-us-pm3.yaml
```

### Exercise

Try starting your experiment now using a container runtime.

Create your GAMESS-US input **molecule.inp** file in the directory `/tmp/gamess-us-pm3`.  You can use this example input file:

```
 $CONTRL COORD=UNIQUE SCFTYP=RHF RUNTYP=OPTIMIZE MULT=1
 ISPHER=1 ICHARG=0 MAXIT=100 $END
 $SYSTEM MWORDS=100 TIMLIM=600 $END
 $BASIS GBASIS=PM3 $END
 $GUESS GUESS=HUCKEL $END
 $SCF DIRSCF=.t. FDIFF=.f. DIIS=.t. $END
 $STATPT NSTEP=500 PROJCT=.f. IHREP=20 HSSEND=.t. $END
 $DATA
CH4 C CH4
 C1
 C 6.0 0.0 0.0 0.0
 H 1.0 0.1895 0.9552 -0.4946
 H 1.0 0.9509 -0.4809 0.2396
 H 1.0 -0.5631 0.1717 0.92
 H 1.0 -0.5773 -0.6461 -0.665
 $END
```

<InlineNotification kind="info">

To use your own GAMESS US input file, carefully review the configuration options preceding the `$DATA` section and include them in your file. Additionally, ensure that your input is saved in a file named **molecule.inp**.

</InlineNotification>


You should now have this structure:

```
/tmp/gamess-us-pm3
├── bin
│   ├── extract_gmsout.py
│   └── run-gamess.sh
├── gamess-us-pm3.yaml
├── hooks
│   └── semi_empirical_restart.py
├── manifest.yaml
└── molecule.inp
```

Launch your experiment providing the input file **molecule.inp** and the manifest file **manifest.yaml**.

```
elaunch.py --nostamp --manifest manifest.yaml -i molecule.inp gamess-us-pm3.yaml
```

After a couple of minutes you should see:

```
completed-on=2025-03-31 11:37:49.301663
cost=0
created-on=2025-03-31 11:35:46.527619
current-stage=stage0
exit-status=Success
experiment-state=finished
stage-progress=1.0
stage-state=finished
stages=['stage0']
total-progress=1.0
updated=2025-03-31 11:37:51.780640
updated-on=2025-03-31 11:37:51.780640
```

The **gamess-us-pm3.instance** directory will have the following structure:

```
gamess-us-pm3.instance
├── bin
│     ├── extract_gmsout.py
│     └── run-gamess.sh
├── conf
│     ├── dsl.yaml
│     ├── flowir_instance.yaml
│     ├── flowir_package.yaml
│     └── manifest.yaml
├── elaunch.yaml
├── hooks
│     ├── __pycache__
│     │     └── semi_empirical_restart.cpython-310.pyc
│     └── semi_empirical_restart.py
├── input
│     └── molecule.inp
├── output
│     ├── experiment.log
│     ├── output.json
│     ├── output.txt
│     ├── status.txt
│     └── status_details.json
├── python
├── stages
│     └── stage0
│         ├── optimise
│         │     ├── Run1
│         │     │     ├── component_performance.csv
│         │     │     ├── molecule.dat
│         │     │     ├── molecule.inp
│         │     │     ├── molecule.rst
│         │     │     ├── out.stderr
│         │     │     └── out.stdout
│         │     ├── component_performance.csv
│         │     ├── molecule.dat
│         │     ├── molecule.inp
│         │     ├── molecule.rst
│         │     ├── out.stderr
│         │     └── out.stdout
│         └── parse-gamess
│             ├── component_performance.csv
│             ├── csv2inp.log
│             ├── energies.csv
│             ├── out.stderr
│             └── out.stdout
└── status.db
```


Examine the files under **stages/stage0/optimise** and **stages/stage0/parse-gamess**.

These were the contents of **stages/stage0/parse-gamess/energies.csv** for the experiment we ran on our laptop:

```
label,completed,total-energy,homo,lumo,gap,electric-moments,total-time,total-time-per-core
molecule,OK,-180.53313527498008,-13.641,4.245,17.886,0.000050,0.1,0.10
```

## What's next?

- Learn more about writing experiments, including more advanced features and best practice [here](/write-more-experiments)
- Learn how to add [key-outputs and interfaces](/add-interface-to-experiments) to your experiments
- More information on running experiments directly, i.e. via `elaunch.py` [here](/direct-run)
- More information on the DSL of ST4SD i.e. how to write experiments [here](/workflow-specification-dsl)
- More information on how to structure and test your experiments [here](/packaging-workflows/)

